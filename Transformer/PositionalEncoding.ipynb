{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "# from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    '''\n",
    "    Implement the PE function.\n",
    "    d_model: 输入序列中每个词嵌入的维度（与词向量的维度一致）\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # compute the positional encoding once in log space.\n",
    "        # 初始化一个大小为 [max_len, d_model] 的全零张量 pe，用于存储位置编码的值。\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # torch.arange 是 PyTorch 中用于生成等间隔数值序列的一种函数。其功能类似于 Python 中的 range 函数，但它生成的是张量（tensor）而不是普通的数字序列。\n",
    "        # 生成一个 position 张量，它的值是从 0 到 max_len-1 的序列（即每个位置的索引），形状为 [max_len, 1]。unsqueeze(1) 表示在第一维度增加一个维度。\n",
    "        position = torch.arange(0, max_len).unsqueeze(1) # [max_len, 1]\n",
    "        '''\n",
    "        计算位置编码中的分母项 div_term, 使用上述公式, 保存为 div_term = 1/分母。\n",
    "        torch.arange(0, d_model, 2) 生成从 0 开始到 d_model-1 步长为 2 的序列，这部分用来处理偶数位置的维度。\n",
    "        \n",
    "        为什么是公式: torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model) ?\n",
    "\n",
    "        '''\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # (0, 2, 4, ...)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # (1, 3, 5, ...)\n",
    "        \n",
    "        # 扩展维度并注册为 buffer\n",
    "        # 使用 unsqueeze(0) 为位置编码增加一个批次维度, 这样可以适配不同批次的数据。\n",
    "        pe = pe.unsqueeze(0) # [max_len, d_model] -> [1, max_len, d_model]\n",
    "        \n",
    "        '''\n",
    "        将 pe 作为 buffer 注册到模型中。Buffer 是模型的持久状态，但不会作为模型参数参与优化，也不会随着梯度更新而改变。\n",
    "        通常情况下，模型中的状态分为两类：\n",
    "            可训练参数：这些参数会在模型的前向传播和反向传播过程中参与梯度计算，并在优化器中更新。这些参数可以通过 nn.Parameter 来注册，它们通常是权重或偏置。通过优化器或手动调整更新\n",
    "            非可训练的持久状态：这些是模型需要的值，但它们不需要参与优化过程。只能通过代码逻辑手动修改。例如： \n",
    "                用于存储固定的常量，比如用于正则化的参数、批次统计数据（如 BatchNorm 中的均值和方差）。\n",
    "                用于模型中不需要梯度更新但要随模型一起保存的变量。\n",
    "        Buffer 就属于第二类状态。它的关键点是：\n",
    "            不会参与梯度计算，不会随着训练过程中的优化步骤被更新。\n",
    "            会随模型保存和加载，即使这些变量不会更新，仍然希望它们作为模型的一部分存储和恢复。\n",
    "        持久存储: pe 是一个位置编码矩阵，代表了每个位置的编码。这个矩阵是预先计算的，并且在整个模型中保持不变，因此它不需要被优化器更新。\n",
    "                但在保存模型时，我们希望位置编码 pe 和其他可训练的参数一样，被保存下来，并在加载模型时恢复。\n",
    "        不会更新：由于位置编码的矩阵 pe 是基于固定的公式计算的，它不需要参与反向传播和梯度更新。\n",
    "                因此，将其作为 buffer 来注册，这样在训练过程中它不会被优化器错误地修改。\n",
    "        '''\n",
    "         \n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, sequence_length, d_model]\n",
    "        # 这一步是将位置编码加到词嵌入上，使得模型能够感知位置信息。\n",
    "        # requires_grad_(False) 是为了防止位置编码参与梯度更新，因为它们是固定的。\n",
    "        x =  x + self.pe[:, x.size(1)].requires_grad_(False)\n",
    "        \n",
    "        '''\n",
    "        为什么对位置编码应用 Dropout ? \n",
    "            尽管 Dropout 最初是在全连接层中用于“随机失活”一部分神经元的，但它的应用范围实际上更广，既可以用于神经网络的各层、或输出层，也可以用于特征向量，如输入的词嵌入或位置编码。\n",
    "            增加随机性: 通过对位置编码的部分值进行随机“失活”，可以引入一些随机性，使得模型不会过度依赖特定的位置信息。这可以让模型在训练时更具鲁棒性，减少过拟合的可能。\n",
    "            正则化输入: Dropout 可以防止模型对特定的输入位置编码产生过度拟合。因为位置编码是添加到输入中的一部分，\n",
    "                      对其进行 dropout 可以有效地打破模型对绝对位置信息的依赖性，迫使模型更关注整体上下文，而不仅仅是某些特定位置的依赖。\n",
    "        '''\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "<img src=\"./images/Position.png\" alt=\"示例图片\" width=\"700\">\n",
    "\n",
    "<img src=\"./images/Position2.png\" alt=\"示例图片\" width=\"300\">\n",
    "\n",
    "$\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad PE_{(pos, 2i)} = \\sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$\n",
    "\n",
    "$\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad PE_{(pos, 2i+1)} = \\cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$\n",
    "\n",
    "$\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad$ 其中 2i 和 2i+1 是特征的维度第几维，分为第**奇**数维和第**偶**数维\n",
    "\n",
    "<img src=\"./images/Position3.png\" alt=\"示例图片\" width=\"700\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绝对位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0100,  1.0000],\n",
       "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
       "        [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
       "        [-0.7568, -0.6536,  0.0400,  0.9992],\n",
       "        [-0.9589,  0.2837,  0.0500,  0.9988],\n",
       "        [-0.2794,  0.9602,  0.0600,  0.9982],\n",
       "        [ 0.6570,  0.7539,  0.0699,  0.9976],\n",
       "        [ 0.9894, -0.1455,  0.0799,  0.9968],\n",
       "        [ 0.4121, -0.9111,  0.0899,  0.9960]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SinPositionEncoding(nn.Module):\n",
    "    def __init__(self, max_sequence_length, d_model, base=10000):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "        self.base = base\n",
    "        \n",
    "    def forward(self):\n",
    "        # 初始化一个大小为 [max_len, d_model] 的全零张量 pe，用于存储位置编码的值。\n",
    "        pe = torch.zeros(self.max_sequence_length, self.d_model, dtype=torch.float)\n",
    "        \n",
    "        exp_1 = torch.arange(self.d_model//2, dtype=torch.float) # 初始化一半维度，sin位置编码的维度被分成了两部分\n",
    "        \n",
    "        exp_value = 2 * exp_1 / self.d_model\n",
    "        \n",
    "        alpha = 1 / (self.base ** exp_value) # size(d_model / 2)\n",
    "        \n",
    "        out = torch.arange(self.max_sequence_length, dtype=torch.float)[:, None] @ alpha[None, :] \n",
    "        # size(max_sequence_length, d_model / 2)\n",
    "        # [:, None] 给张量新添加了一个维度 (max_sequence_length, 1)\n",
    "        # [None, :] (1, d_model / 2)\n",
    "        # @: 矩阵乘法\n",
    "        \n",
    "        embedding_sin = torch.sin(out)\n",
    "        embedding_cos = torch.cos(out)\n",
    "        \n",
    "        pe[:, 0::2] = embedding_sin # 将 embedding_sin 的内容赋值到 pe 的偶数列上\n",
    "        # 行数 (max_sequence_length)：embedding_sin 的行数与 pe 相同，每一行对应一个位置。\n",
    "        # embedding_sin 的列数是 pe 的一半，因为正弦部分只填充到 pe 的偶数列 (0::2)。\n",
    "        pe[:, 1::2] = embedding_cos\n",
    "        \n",
    "        return pe\n",
    "    \n",
    "SinPositionEncoding(d_model=4, max_sequence_length=10, base=10000).forward()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可学习位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mn/8fw7vgd551394dxtykrf78200000gn/T/ipykernel_36378/3846723043.py:10: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(pe.weight, 0.) # 将 pe.weight 的所有元素初始化为 0。\n"
     ]
    }
   ],
   "source": [
    "class TrainablePositionEncoding(nn.Module):\n",
    "    def __init__(self, max_sequence_length, d_model):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self):\n",
    "        pe = nn.Embedding(self.max_sequence_length, self.d_model)\n",
    "        # pe 是一个 torch.nn.Embedding 或类似的层，它的 weight 属性是一个可以学习的权重矩阵。\n",
    "        nn.init.constant(pe.weight, 0.) # 将 pe.weight 的所有元素初始化为 0。\n",
    "        \n",
    "        return pe\n",
    "pe = TrainablePositionEncoding(max_sequence_length=10, d_model=4).forward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相对位置编码\n",
    "\n",
    "<img src=\"./images/xiangdui.png\" alt=\"示例图片\" width=\"1100\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePosition(nn.Module):\n",
    "    '''\n",
    "    根据查询和键的长度，生成相对位置的差距矩阵。\n",
    "    将差距裁剪到指定范围并映射为合法索引。\n",
    "    根据索引从嵌入表中查找相应的嵌入，输出相对位置编码。\n",
    "    '''\n",
    "    def __init__(self, num_units, max_relative_position, device = 'cpu'):\n",
    "        super().__init__()\n",
    "        self.num_units = num_units # 每个相对位置的嵌入维度\n",
    "        self.max_relative_position = max_relative_position # 最大相对距离\n",
    "        self.embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units))\n",
    "        # self.embeddings_table 是一个 learnable 参数矩阵, 形状为 (max_relative_position * 2 + 1, num_units)\n",
    "        # 如果 self.embeddings_table 的形状是 (5, num_units)，\n",
    "        # 而 final_mat 是一个形状为 (length_q, length_k) 的张量，其值是 [0, 1, 2, 3, 4] 之间的索引。\n",
    "        # 那么 self.embeddings_table[final_mat] 会返回形状为 (length_q, length_k, num_units) 的张量，对应每个相对位置的嵌入。\n",
    "\n",
    "        nn.init.xavier_uniform_(self.embeddings_table)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, length_q, length_k):\n",
    "        range_vec_q = torch.arange(length_q)\n",
    "        range_vec_k = torch.arange(length_k)\n",
    "        # 一般 length_q == length_k\n",
    "        # 通过广播机制,计算相对位置差矩阵, (length_q, length_k)\n",
    "        distance_mat = range_vec_q[None, :] - range_vec_k[:, None]\n",
    "        \n",
    "        # 将超出范围的差距裁剪到 [-max_relative_position, max_relative_position]\n",
    "        distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)\n",
    "        \n",
    "        # 将 [-2, -1, 0, 1, 2] 映射为 [0, 1, 2, 3, 4]\n",
    "        # final_mat 是一个整型索引张量，用于从 self.embeddings_table 中查找对应的嵌入\n",
    "        final_mat = distance_mat_clipped + self.max_relative_position\n",
    "        final_mat = torch.LongTensor(final_mat).to(self.device)\n",
    "        \n",
    "        embeddings = self.embeddings_table[final_mat].to(self.device)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention\n",
    "\n",
    "<img src=\"./images/MultiHeadAttention.png\" alt=\"示例图片\" width=\"700\">\n",
    "\n",
    "$MultiHead(Q, K, V) = Concat(head_1, head_2, ... , head_h)W^O\\text{, where }head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$\n",
    "\n",
    "$W_i^Q \\in \\mathbb{R}^{d_{model} \\times d_k}, \\quad W_i^K \\in \\mathbb{R}^{d_{model} \\times d_K}, \\quad W_i^V \\in \\mathbb{R}^{d_{model} \\times d_V}, \\quad W_i^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$\n",
    "\n",
    "$$\n",
    "Attention(q, k, v) = [softmax(\\frac{q \\dot (k + pos_k)}{\\sqrt{d_k}})](v + pos_v)\n",
    "$$\n",
    "\n",
    "其中 $pos_k$ 是注入键（$k$）的相对位置编码。只需要为 $k$ 注入相对位置信息，因为 𝑞⋅𝑘 已能体现注意力权重的相对顺序。\n",
    "\n",
    "相对位置编码的核心目标是调整键（k）和值（v）的权重。\n",
    "\n",
    "注入到 k 中的相对位置编码即可覆盖所有信息，q 不需要重复注入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, num):\n",
    "        \"\"\"返回指定数量的 module 深拷贝\"\"\"\n",
    "        return nn.ModuleList([deepcopy(module) for _ in range(num)])\n",
    "\n",
    "class RelativeMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1, batch_size=6, device='cpu'):\n",
    "        \"Take in model size and number of heads. \"\n",
    "        super(RelativeMultiHeadAttention, self).__init__()\n",
    "        self.device = device\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        assert d_model % n_heads == 0\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.linears = _get_clones(nn.Linear(d_model, d_model), 4)\n",
    "        # linears: W_i^Q, W_i^K, W_i^V, W_i^O\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relative_position_k = RelativePosition(self.head_dim, max_relative_position=16)\n",
    "        self.relative_position_v = RelativePosition(self.head_dim, max_relative_position=16)\n",
    "        # 返回形状为 (length_q, length_k, head_dim) 的张量，对应每个相对位置的嵌入\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        # embedding\n",
    "        # query, key, value = [batch_size, len, hid_dim]\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        '''\n",
    "        这段代码的作用是通过线性变换，将 query、key 和 value 映射到多头注意力机制的特征空间。\n",
    "        .view(self.batch_size, -1, self.d_model):\n",
    "            .view(self.batch_size, -1, self.d_model)：将线性变换的结果重塑为 (self.batch_size, -1, self.d_model) 的形状，\n",
    "            目的是分配给每个头。\n",
    "            view 函数用于改变张量的形状。这里的目的是将每个线性层的输出重塑为形状为 [self.batch_size, -1, self.d_model] 的张量。\n",
    "            batch_size 是批次大小，表示每个批次中的样本数。\n",
    "            -1 是自动推导的维度，它会根据原始张量的总元素数量和指定的其他维度自动计算,即 len。\n",
    "            self.d_model 是特征维度。\n",
    "        zip 函数将 self.linears 和 (query, key, value) 这三个张量打包在一起，使得在循环中可以同时迭代线性层和输入张量。\n",
    "            三个线性层, 分别以query, key, value作为参数跑一遍\n",
    "        '''\n",
    "        query, key, value = [l(x).view(self.batch_size, -1, self.d_model) for l,x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        len_k = query.shape[1]\n",
    "        len_q = query.shape[1]\n",
    "        len_v = value.shape[1]\n",
    "        \n",
    "        # Self-Attention\n",
    "        # r_q1, r_k1 = [batch_size, len, n_heads, head_dim]\n",
    "        # -> [batch_size, n_heads, len, head_dim]\n",
    "        r_q1 = query.view(self.batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        r_k1 = key.view(self.batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # -> [batch_size, n_heads, head_dim, len]\n",
    "        # 计算 q*k\n",
    "        attn1 = torch.matmul(r_q1, r_k1.permute(0, 1, 3, 2))\n",
    "        # attn1: [batch_size, n_heads, head_dim, len(_q), len(_k)]\n",
    "        \n",
    "        # [batch_size, len, d_model] -> [len, batch_size, d_model] (-> [len, batch_size, n_heads, head_dim])\n",
    "        # -> [len, batch_size * n_heads, head_dim]\n",
    "        # batch_size * n_heads 合并批量大小和头的数量，适配批量化并行处理\n",
    "        r_q2 = query.permute(1, 0, 2).contiguous().view(len_q, self.batch_size * self.n_heads, self.head_dim)\n",
    "        # contiguous的作用是 保证张量在内存中是连续存储的。\n",
    "        # 在某些张量操作（比如 permute 或 view）之后，张量可能不再是连续的，\n",
    "        # 这时需要调用 contiguous() 来创建一个连续的张量副本。\n",
    "        # permute 作用: 改变张量的维度顺序以适配逻辑需求。\n",
    "        # view 作用: 根据新的需求调整张量的形状。\n",
    "        # 为什么要先 permute 再 view？\n",
    "        # 因为 view 要求张量是连续的，而 permute 不改变内存布局，因此必须先 permute，再用 contiguous() 确保内存连续，最后用 view 调整形状。\n",
    "        r_k2 = self.relative_position_k(len_q, len_k)\n",
    "        # r_k2: [len_q, len_k, head_dim]\n",
    "        \n",
    "        # 计算 q*pos_k\n",
    "        # [len, batch_size * n_heads, head_dim], [len_q, len_k, head_dim]->[len_q, head_dim, len_k]\n",
    "        # attn2: [len_q, batch_size * n_heads, len_k] -> [batch_size * n_heads, len_q, len_k]\n",
    "        attn2 = torch.matmul(r_q2, r_k2.transpose(1, 2)).transpose(0, 1)\n",
    "        attn2 = attn2.contiguous().view(self.batch_size, self.n_heads, len_q, len_k)\n",
    "        # attn2: [batch_size, n_heads, len_q, len_k]\n",
    "        \n",
    "        attn = (attn1 + attn2) / self.scale\n",
    "        attn = self.dropout(torch.softmax(attn, dim=-1))\n",
    "        # attn: [batch_size, n_heads, len(_q), len(_k)]\n",
    "        \n",
    "        # 同上\n",
    "        r_v1 = value.view(self.batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # [batch_size, n_heads, head_dim, len]\n",
    "        \n",
    "        # 计算 attn*v\n",
    "        # [batch_size, n_heads, len(_q), len(_k)], [batch_size, n_heads, head_dim, len]\n",
    "        weight1 = torch.matmul(attn, r_v1)\n",
    "        # weight1: [batch_size, n_heads, len, len]\n",
    "        \n",
    "        r_v2 = self.relative_position_v(len_q, len_v)\n",
    "        # r_v2: [len_q, len_v, head_dim]\n",
    "        \n",
    "        # [batch_size, n_heads, len(_q), len(_k)] -> [len(_q), batch_size, n_heads, len(_k)]\n",
    "        # weight2 -> [len(_q), batch_size * n_heads, len(_k)]\n",
    "        weight2 = attn.permute(2, 0, 1, 3).contiguous().view(len_q, self.batch_size * self.n_heads, len_k) # len_k == len_v\n",
    "        # [len(_q), batch_size * n_heads, len(_k)], [len_q, len_v, head_dim]\n",
    "        # weight2 -> [len(_q), batch_size * n_heads, head_dim]\n",
    "        weight2 = torch.matmul(weight2, r_v2)\n",
    "        \n",
    "        # [len(_q), batch_size * n_heads, head_dim] (-> [batch_size * n_heads, len(_1), head_dim])\n",
    "        # weight2 -> [batch_size, n_heads, len(_q), head_dim]\n",
    "        weight2 = weight2.transpose(0, 1).contiguous().view(self.batch_size, self.n_heads, len_q, self.head_dim)\n",
    "        \n",
    "        # x: [batch_size, n_heads, len(_q), head_dim]\n",
    "        x = weight1 + weight2\n",
    "        \n",
    "        # x: [batch_size, len(_q), n_heads, head_dim]\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        # x: [batch_size * len(_q), n_heads, head_dim] -> [batch_size * len(_q), d_model]\n",
    "        x = x.view(self.batch_size * len_q, self.d_model)\n",
    "        \n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([10, 16])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    d_model = 16\n",
    "    n_heads = 4\n",
    "    device = 'cpu'\n",
    "\n",
    "    # Dummy inputs\n",
    "    query = torch.rand(batch_size, seq_len, d_model)\n",
    "    key = torch.rand(batch_size, seq_len, d_model)\n",
    "    value = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = RelativeMultiHeadAttention(d_model=d_model, n_heads=n_heads, batch_size=batch_size, device=device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(query, key, value)\n",
    "    print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_position_embedding(batch_size, nums_head, max_len, output_dim, device):\n",
    "    # (max_len, 1)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(-1)\n",
    "    # torch.arange 生成一个张量（tensor），该张量包含指定范围内的连续值\n",
    "    # (output_dim // 2)\n",
    "    ids = torch.arange(0, output_dim // 2, dtype=torch.float) # 即公式里的 i, i的范围是[0, d/2]\n",
    "    theta = torch.pow(10000, -2 * ids / output_dim)\n",
    "    # print(position.shape) # torch.Size([max_len, 1])\n",
    "    # print(theta.shape) # torch.Size([output_dim // 2])\n",
    "    \n",
    "    # (max_len, output_dim//2)\n",
    "    embeddings = position * theta # 即公式里的: pos / (10000^(2i/d))\n",
    "    # print(embedding.shape) # torch.Size([max_len, output_dim//2])\n",
    "    \n",
    "    # torch.stack 将一组张量沿着新的维度进行连接的函数。它将多个张量堆叠在一起，返回一个新的张量，新的维度通常是增加了一个维度的张量。\n",
    "    # (max_len, output_dim//2, 2)\n",
    "    embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "    \n",
    "    # torch.repeat(*sizes) sizes：一个整数元组，指定每个维度的重复次数。这个元组的长度应该与原始张量的维度数相同。\n",
    "    # e.g. t.repeat(2, 3) 使得原始张量在第0维上重复了 2 次，在第1维上重复了 3 次\n",
    "    # (bs, head, max_len, output_dim//2, 2)\n",
    "    embeddings = embeddings.repeat(batch_size, nums_head, *([1] * len(embeddings.shape)))\n",
    "    # 在第一维(bs)重复batch_size次，在第二维重复nums_head次，其他维度重复一次(不重复)\n",
    "    # *([1] * len(embeddings.shape)) 创建一个列表，列表的长度等于 embeddings 的维度数(len(t.shape))，每个元素都是 1\n",
    "    \n",
    "    # (hs, head, max_len, output_dim)\n",
    "    # reshape后就是偶数sin，奇数cos了（是(output_dim//2, 2)，第一列是奇，第二列是偶，所以按行拼接就是奇偶交叉）\n",
    "    # 如果是(2, output_dim//2)则变成了所有sin在前，所有cos在后\n",
    "    '''\n",
    "    t = torch.tensor([[1, 3, 5], [2, 4, 6]])\n",
    "    flattened_tensor = t.reshape(-1) # tensor([1, 3, 5, 2, 4, 6])\n",
    "    flattened_tensor = t.T.reshape(-1) # tensor([1, 2, 3, 4, 5, 6])\n",
    "    t = torch.tensor([[[1, 2], [3, 4],[5,6]]])\n",
    "    flattened_tensor = t.reshape(-1) # tensor([1, 2, 3, 4, 5, 6])\n",
    "    flattened_tensor = t.T.reshape(-1) # tensor([1, 3, 5, 2, 4, 6])\n",
    "    '''\n",
    "    embeddings = torch.reshape(embeddings, (batch_size, nums_head, max_len, output_dim))\n",
    "    # print(embeddings.shape) # torch.Size([batch_size, nums_head, max_len, output_dim])\n",
    "    \n",
    "    embeddings = embeddings.to(device)\n",
    "    return embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/RoPE.png\" alt=\"示例图片\" width=\"1100\">\n",
    "\n",
    "$$\n",
    "f_q(x_m, m) = \n",
    "\\begin{pmatrix}\n",
    "\\cos m\\theta & -\\sin m\\theta \\\\\n",
    "\\sin m\\theta & \\cos m\\theta\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "W_q^{(1,1)} & W_q^{(1,2)} \\\\\n",
    "W_q^{(2,1)} & W_q^{(2,2)}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_m^{(1)} \\\\\n",
    "x_m^{(2)}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\cos m\\theta & -\\sin m\\theta \\\\\n",
    "\\sin m\\theta & \\cos m\\theta\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "q_m^{(1)} \\\\\n",
    "q_m^{(2)}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "For n-dim:\n",
    "$$\n",
    "f_q(x_m, m) = \n",
    "\\begin{pmatrix}\n",
    "\\cos m\\theta_0 & -\\sin m\\theta_0 & 0 & 0 & \\cdots & 0 & 0 \\\\\n",
    "\\sin m\\theta_0 & \\cos m\\theta_0 & 0 & 0 & \\cdots & 0 & 0 \\\\\n",
    "0 & 0 & \\cos m\\theta_1 & -\\sin m\\theta_1 & \\cdots & 0 & 0 \\\\\n",
    "0 & 0 & \\sin m\\theta_1 & \\cos m\\theta_1 & \\cdots & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & 0 & 0 & \\cdots & \\cos m\\theta_{d/2-1} & -\\sin m\\theta_{d/2-1} \\\\\n",
    "0 & 0 & 0 & 0 & \\cdots & \\sin m\\theta_{d/2-1} & \\cos m\\theta_{d/2-1}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "q_m^{(1)} \\\\\n",
    "q_m^{(2)} \\\\\n",
    "q_m^{(3)} \\\\\n",
    "q_m^{(4)} \\\\\n",
    "\\vdots\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "i.e.\n",
    "\n",
    "$$\n",
    "R_{\\Theta, m}^d \\mathbf{x} =\n",
    "\\begin{pmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d-2} \\\\\n",
    "x_{d-1}\n",
    "\\end{pmatrix}\n",
    "\\otimes\n",
    "\\begin{pmatrix}\n",
    "\\cos m\\theta_0 \\\\\n",
    "\\cos m\\theta_0 \\\\\n",
    "\\cos m\\theta_1 \\\\\n",
    "\\cos m\\theta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\cos m\\theta_{d/2-1} \\\\\n",
    "\\cos m\\theta_{d/2-1}\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "-x_1 \\\\\n",
    "x_0 \\\\\n",
    "-x_3 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "-x_{d-1} \\\\\n",
    "x_{d-2}\n",
    "\\end{pmatrix}\n",
    "\\otimes\n",
    "\\begin{pmatrix}\n",
    "\\sin m\\theta_0 \\\\\n",
    "\\sin m\\theta_0 \\\\\n",
    "\\sin m\\theta_1 \\\\\n",
    "\\sin m\\theta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\sin m\\theta_{d/2-1} \\\\\n",
    "\\sin m\\theta_{d/2-1}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoPE(q, k):\n",
    "    assert(q.shape == k.shape)\n",
    "    \n",
    "    # q, k: (bs, head, max_len, output_dim)\n",
    "    batch_size = q.shape[0]\n",
    "    nums_head = q.shape[1]\n",
    "    max_len = q.shape[2]\n",
    "    output_dim = q.shape[-1]\n",
    "    \n",
    "    # (bs, head, max_len, output_dim)\n",
    "    pos_emb = sinusoidal_position_embedding(batch_size, nums_head, max_len, output_dim, q.device)\n",
    "    \n",
    "    # cos_pos, sin_pos: (bs, head, max_len, output_dim)\n",
    "    # 看rope公式可知，相邻一组(如1-2，3-4，5-6，...)之间的 \\theta 是相同的，所以cos(m\\theta)，sin(m\\theta)也是同一个。\n",
    "    # 只需要复制一遍sin，cos的向量即可,如(1, 2, 3)变成(1, 1, 2, 2, 3, 3)\n",
    "    # interleave 交错\n",
    "    # torch.repeat_interleave(input, repeats, dim=None) 每个元素重复 repeats 次\n",
    "    # pos_emb中先sin后cos交错排列\n",
    "    sin_pos = pos_emb[..., 0::2].repeat_interleave(2, dim=-1)\n",
    "    cos_pos = pos_emb[..., 1::2].repeat_interleave(2, dim=-1)\n",
    "    \n",
    "    # q,k: (bs, head, max_len, output_dim)\n",
    "    q2 = torch.stack([-q[..., 1::2], q[..., 0::2]], dim=-1)\n",
    "    # q2: (bs, head, max_len, output_dim//2, 2)\n",
    "    q2 = q2.reshape(q.shape) # reshape 后就是正负交替了，先负后正，先奇后偶，奇负偶正\n",
    "    \n",
    "    # 更新qw, *对应位置想乘，见RoPE公式\n",
    "    q = q * cos_pos + q2 * sin_pos\n",
    "    \n",
    "    # k 同理\n",
    "    k2 = torch.stack([-k[..., 1::2], k[..., 0::2]], dim=-1)\n",
    "    k2 = k2.reshape(k.shape)\n",
    "    k = k * cos_pos + k2 * sin_pos\n",
    "    \n",
    "    return q, k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "<img src=\"./images/attention.png\" alt=\"示例图片\" width=\"700\">\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = softmax(\\frac{QK^\\top}{\\sqrt{d_k}})V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, mask=None, dropout=None, use_RoPE=True):\n",
    "    # q.shape: (bs, head, seq_len, dk)\n",
    "    # k.shape: (bs, head, seq_len, dk)\n",
    "    # v.shape: (bs, head, seq_len, dk)\n",
    "    \n",
    "    if use_RoPE:\n",
    "        q, k = RoPE(q, k)\n",
    "    \n",
    "    d_k = k.size()[-1]\n",
    "    \n",
    "    att_logits = torch.matmul(q, k.transpose(-2, -1)) # (bs, head, seq_len, seq_len)\n",
    "    att_logits /= math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        att_logits = att_logits.masked_fill(mask == 0, -1e9) # mask掉mask矩阵为0的部分，设为负无穷大\n",
    "    \n",
    "    att_scores = F.softmax(att_logits, dim=-1) # (bs, head, seq_ken, seq_len)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        att_scores = dropout(att_scores)\n",
    "        \n",
    "    # (bs, head, seq_ken, seq_len) * (bs, head, seq_ken, dk) = (bs, head, seq_ken, seq_len)\n",
    "    \n",
    "    return torch.matmul(att_scores, v), att_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 12, 10, 32]) torch.Size([8, 12, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # (bs, head, seq_ken, dk)\n",
    "    q = torch.randn((8, 12, 10, 32))\n",
    "    k = torch.randn((8, 12, 10, 32))\n",
    "    v = torch.randn((8, 12, 10, 32))\n",
    "    \n",
    "    res, att_scores = attention(q, k, v, mask=None, dropout=None, use_RoPE=True)\n",
    "    \n",
    "    # (bs, head, seq_len, seq_len), (bs, head, seq_len, seq_len)\n",
    "    print(res.shape, att_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 4.7924e-01, -8.1184e-02,  5.0471e-01,  ...,  3.3904e-02,\n",
      "           -5.6309e-01, -6.9180e-01],\n",
      "          [ 2.8582e-02, -6.0121e-01,  2.2196e-01,  ...,  5.8244e-01,\n",
      "            4.9220e-02, -9.2528e-01],\n",
      "          [-5.4878e-01,  1.3189e-01, -8.0629e-01,  ...,  5.6292e-01,\n",
      "            4.6413e-01, -9.8989e-02],\n",
      "          ...,\n",
      "          [-9.2218e-02,  6.4448e-01, -8.5428e-02,  ..., -1.6412e-01,\n",
      "            1.2261e+00,  2.9948e-02],\n",
      "          [-3.7048e-01, -5.1464e-01, -3.4825e-02,  ...,  4.9220e-01,\n",
      "            2.7366e-01, -6.0428e-01],\n",
      "          [ 1.2036e-01, -3.5371e-01,  2.0824e-02,  ...,  8.7174e-01,\n",
      "           -4.2703e-01, -1.4617e-01]],\n",
      "\n",
      "         [[ 4.4240e-01,  2.7135e-01,  5.3776e-02,  ..., -4.9925e-01,\n",
      "           -5.0704e-01, -2.3714e-01],\n",
      "          [ 6.4617e-01, -1.4401e-03,  2.9460e-01,  ...,  4.6954e-01,\n",
      "            2.9533e-02,  3.1978e-01],\n",
      "          [ 1.3858e-01,  2.2991e-01, -3.6092e-02,  ...,  9.3772e-02,\n",
      "           -4.0733e-01,  1.0657e-01],\n",
      "          ...,\n",
      "          [ 3.4989e-02,  2.6000e-01,  1.4576e-01,  ...,  7.6645e-02,\n",
      "           -5.4252e-01, -3.2226e-01],\n",
      "          [ 8.8788e-02, -3.3115e-01,  4.2124e-01,  ...,  1.9108e-01,\n",
      "           -2.6256e-01,  1.8804e-01],\n",
      "          [-2.2684e-01, -3.4425e-02,  2.4380e-01,  ..., -2.5996e-02,\n",
      "           -4.0494e-01,  1.5631e-01]],\n",
      "\n",
      "         [[-2.1639e-01, -3.0787e-01,  2.8946e-01,  ...,  4.0077e-01,\n",
      "           -2.9228e-01,  1.3379e-01],\n",
      "          [-2.9024e-01, -2.3434e-01,  8.4954e-01,  ..., -1.5155e-01,\n",
      "           -5.4104e-02,  7.7297e-01],\n",
      "          [ 3.4037e-02, -9.1880e-01,  3.5744e-01,  ...,  6.9139e-01,\n",
      "           -5.7824e-01, -5.3555e-01],\n",
      "          ...,\n",
      "          [-5.1589e-02, -8.1027e-01,  6.0797e-01,  ...,  5.7539e-01,\n",
      "           -7.5569e-01, -4.5427e-01],\n",
      "          [-2.0580e-01, -7.4785e-02, -1.3232e-01,  ...,  2.2612e-01,\n",
      "           -3.0013e-01,  5.3096e-01],\n",
      "          [-2.7965e-01,  2.8699e-01,  4.1593e-03,  ..., -3.1512e-01,\n",
      "           -2.7426e-01,  1.8116e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0195e-01, -2.3557e-01, -2.9131e-02,  ..., -5.4168e-01,\n",
      "            6.2999e-03, -1.0059e-01],\n",
      "          [ 2.8077e-01, -3.4883e-01,  4.2808e-01,  ...,  1.4548e-01,\n",
      "            1.0152e+00, -8.0355e-01],\n",
      "          [ 7.9329e-02,  6.6116e-01, -2.8568e-01,  ..., -1.6833e-01,\n",
      "            5.7640e-01,  3.5408e-01],\n",
      "          ...,\n",
      "          [ 4.3780e-01,  3.3121e-02,  1.6845e-01,  ..., -4.7117e-01,\n",
      "            2.4850e-01, -1.4400e-01],\n",
      "          [ 2.7887e-01,  3.1533e-01, -1.7673e-01,  ..., -4.8047e-01,\n",
      "            4.0018e-01,  2.9982e-02],\n",
      "          [ 1.2810e-01, -1.6127e-01,  1.0703e-01,  ..., -6.3268e-01,\n",
      "            3.6958e-01, -1.5425e-01]],\n",
      "\n",
      "         [[-4.7543e-01,  1.9812e-01,  5.6307e-01,  ...,  2.6627e-01,\n",
      "            4.0405e-01, -2.8978e-01],\n",
      "          [-2.4350e-01,  2.0318e-01,  2.7093e-01,  ...,  1.9905e-01,\n",
      "            2.2477e-01, -1.5909e-01],\n",
      "          [-1.6419e-01,  8.4131e-01,  1.8118e-02,  ..., -2.8050e-01,\n",
      "            2.2074e-01, -4.1729e-02],\n",
      "          ...,\n",
      "          [-1.7347e-01, -1.9314e-01,  5.1593e-01,  ...,  9.2244e-01,\n",
      "            4.7507e-01, -1.0857e+00],\n",
      "          [-3.5580e-01, -1.7366e-01,  8.5489e-01,  ...,  8.3374e-02,\n",
      "            1.0673e+00, -6.1902e-01],\n",
      "          [-5.0193e-01, -1.7296e-01,  5.5527e-01,  ..., -8.7066e-02,\n",
      "           -8.0513e-02,  3.3487e-01]],\n",
      "\n",
      "         [[-4.5692e-01, -6.4109e-01,  4.4119e-01,  ...,  2.5906e-01,\n",
      "           -5.8484e-01,  4.4600e-01],\n",
      "          [-2.8324e-01, -8.1902e-01,  7.3462e-01,  ...,  1.3290e-01,\n",
      "           -5.7838e-02, -7.8076e-02],\n",
      "          [ 1.2106e-01, -1.0992e+00,  6.4203e-01,  ..., -4.2581e-01,\n",
      "           -1.1977e-01,  1.1300e-01],\n",
      "          ...,\n",
      "          [-2.7591e-01, -8.1999e-01,  4.2743e-01,  ...,  2.0346e-01,\n",
      "           -4.7837e-01,  3.6752e-01],\n",
      "          [-3.2803e-01, -4.4243e-01,  2.4399e-01,  ...,  1.6217e-01,\n",
      "           -1.0442e-02,  5.1314e-02],\n",
      "          [ 6.6155e-03, -8.9396e-01,  3.4566e-01,  ..., -3.1655e-01,\n",
      "            4.0352e-02,  6.4501e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.7029e-01,  1.7544e-01, -8.1281e-04,  ..., -2.6750e-02,\n",
      "            2.2090e-01, -2.5004e-01],\n",
      "          [ 5.4162e-01,  9.7657e-01,  4.3832e-01,  ..., -8.4969e-01,\n",
      "            3.1121e-01,  3.7905e-01],\n",
      "          [-3.9632e-01,  2.7898e-01, -3.2684e-01,  ...,  5.6299e-01,\n",
      "            1.2080e-01, -4.8805e-01],\n",
      "          ...,\n",
      "          [-2.7699e-01,  8.4262e-02,  6.2049e-02,  ...,  2.8001e-01,\n",
      "            6.7081e-02,  8.2519e-03],\n",
      "          [-4.4701e-02,  4.6291e-01, -3.8571e-02,  ...,  2.4390e-01,\n",
      "            2.0752e-02, -1.6943e-02],\n",
      "          [ 1.3064e-01,  3.8652e-01, -1.7360e-01,  ...,  1.1950e-01,\n",
      "           -4.7680e-01,  2.3712e-01]],\n",
      "\n",
      "         [[ 5.8091e-01, -5.3749e-01, -2.0376e-01,  ..., -9.8394e-01,\n",
      "           -3.8998e-01, -5.2257e-01],\n",
      "          [ 6.0920e-01, -1.8986e-01,  9.0336e-01,  ..., -5.9936e-01,\n",
      "            2.9605e-01, -2.7062e-01],\n",
      "          [ 3.1753e-01, -2.7817e-01,  2.1399e-01,  ..., -1.3096e+00,\n",
      "            1.1912e-02, -3.8203e-01],\n",
      "          ...,\n",
      "          [-2.3240e-01, -6.0973e-01, -1.7615e-01,  ..., -1.8782e+00,\n",
      "           -4.0190e-01, -8.0506e-01],\n",
      "          [ 4.7885e-01,  1.9562e-02, -6.3807e-02,  ..., -1.0138e+00,\n",
      "            3.8124e-01, -1.4419e-01],\n",
      "          [ 8.5594e-01, -1.5613e-01, -1.8509e-01,  ..., -2.5674e-01,\n",
      "            3.6647e-01, -2.9472e-01]],\n",
      "\n",
      "         [[ 9.2285e-02,  1.5553e-01,  1.0490e-01,  ...,  5.6417e-01,\n",
      "            4.8751e-01, -1.3544e-01],\n",
      "          [ 3.2645e-01, -4.1026e-02, -9.7962e-02,  ...,  3.8279e-01,\n",
      "            1.8368e-01, -1.1838e-01],\n",
      "          [ 5.7203e-01,  4.8235e-01,  1.9791e-01,  ..., -3.6352e-02,\n",
      "            5.8256e-02, -4.0499e-01],\n",
      "          ...,\n",
      "          [ 3.9179e-01,  3.0344e-01,  1.4551e-01,  ...,  3.6985e-01,\n",
      "           -1.9181e-01, -4.0508e-01],\n",
      "          [ 8.6369e-02,  7.5198e-01,  1.3808e-01,  ...,  5.1065e-01,\n",
      "           -8.2306e-02, -2.9161e-01],\n",
      "          [ 4.0088e-01, -1.6916e-01,  8.7512e-02,  ...,  6.3094e-01,\n",
      "            6.3393e-01, -4.5854e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.6879e-02, -1.3068e-01,  8.2993e-01,  ..., -2.3318e-01,\n",
      "           -8.1337e-03,  5.9286e-01],\n",
      "          [ 4.1407e-02, -2.1275e-01,  8.5471e-01,  ...,  8.2760e-02,\n",
      "            1.9359e-01,  6.2116e-01],\n",
      "          [-1.8419e-01, -9.3145e-02,  5.8710e-01,  ..., -2.1494e-01,\n",
      "           -1.7035e-02,  4.5434e-01],\n",
      "          ...,\n",
      "          [-6.4694e-01, -4.9237e-01,  5.0846e-01,  ..., -4.3289e-01,\n",
      "            6.9534e-01,  1.0738e-01],\n",
      "          [-2.6657e-01, -2.6750e-01,  7.0937e-01,  ..., -3.1482e-01,\n",
      "            1.7169e-01,  5.8019e-01],\n",
      "          [-1.7186e-01,  6.1341e-02,  5.5129e-01,  ..., -4.6578e-01,\n",
      "           -2.2503e-01,  3.4128e-01]],\n",
      "\n",
      "         [[ 9.7067e-01,  6.6213e-03, -2.3060e-01,  ...,  4.1603e-01,\n",
      "            3.8044e-01, -1.0995e-01],\n",
      "          [ 1.0522e+00,  4.9822e-01, -3.6363e-01,  ...,  2.3937e-01,\n",
      "            1.9121e-01,  7.1644e-02],\n",
      "          [ 1.7395e-01,  4.5694e-01,  2.0278e-01,  ...,  5.9748e-01,\n",
      "            1.3978e-01,  5.8743e-02],\n",
      "          ...,\n",
      "          [ 6.3406e-01,  2.8487e-01, -3.4488e-01,  ..., -2.6943e-01,\n",
      "            1.8932e-01, -3.0720e-01],\n",
      "          [ 3.5947e-01,  4.4615e-01,  3.7505e-01,  ...,  3.5900e-01,\n",
      "            1.1405e-01,  3.6689e-01],\n",
      "          [ 4.7173e-01,  6.4534e-01, -7.9822e-02,  ...,  5.0651e-02,\n",
      "            3.7713e-01, -3.6410e-01]],\n",
      "\n",
      "         [[-4.3475e-02,  1.1449e-01,  2.4829e-01,  ...,  5.5120e-01,\n",
      "           -4.5483e-01,  2.9874e-01],\n",
      "          [ 2.6935e-01,  3.3060e-01,  1.5657e-01,  ..., -2.5308e-01,\n",
      "           -5.6907e-01, -4.6733e-01],\n",
      "          [-2.1354e-02,  4.8800e-01, -7.4036e-01,  ...,  6.2789e-01,\n",
      "           -6.1856e-01, -2.2941e-01],\n",
      "          ...,\n",
      "          [ 2.1814e-02,  4.1347e-01, -3.6688e-01,  ...,  2.7987e-01,\n",
      "           -4.0039e-01, -3.0427e-01],\n",
      "          [ 7.8844e-02, -1.7686e-01,  4.2960e-01,  ...,  9.4490e-01,\n",
      "           -3.9176e-01,  8.3883e-02],\n",
      "          [ 1.1996e-01,  1.3163e-01, -2.0352e-01,  ..., -7.3182e-02,\n",
      "           -1.8716e-01, -4.9470e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3243e+00, -1.9121e-01,  1.7316e-01,  ...,  7.7150e-01,\n",
      "           -1.7175e-01,  2.6716e-01],\n",
      "          [ 5.4124e-01,  9.5806e-02,  1.6171e-01,  ...,  9.1571e-01,\n",
      "           -1.1631e-01,  3.6933e-01],\n",
      "          [ 3.7376e-01, -3.0432e-01,  2.4725e-01,  ...,  9.2877e-01,\n",
      "            5.7641e-02, -8.4606e-02],\n",
      "          ...,\n",
      "          [ 1.0740e+00,  2.0798e-01,  8.3930e-01,  ...,  8.4779e-01,\n",
      "           -4.2792e-01, -1.4105e-02],\n",
      "          [ 7.6515e-01,  1.3741e-01,  1.2855e-01,  ...,  3.9787e-01,\n",
      "           -1.7103e-01, -1.0558e-01],\n",
      "          [ 6.2873e-01,  3.7529e-01,  4.9916e-01,  ...,  8.6743e-01,\n",
      "           -1.1591e-01, -1.5042e-01]],\n",
      "\n",
      "         [[ 2.2594e-01,  4.9842e-01,  3.0417e-02,  ...,  1.8655e-02,\n",
      "           -1.1960e-01,  2.9537e-01],\n",
      "          [ 5.5913e-02,  5.9855e-01,  4.2994e-01,  ..., -1.9249e-02,\n",
      "           -1.3079e-01, -3.2250e-02],\n",
      "          [-6.9349e-02,  4.1468e-01,  7.3211e-01,  ..., -1.1942e-01,\n",
      "           -1.5448e-01, -5.6684e-02],\n",
      "          ...,\n",
      "          [-1.9243e-01,  7.6134e-01, -4.1016e-02,  ..., -1.9849e-01,\n",
      "           -8.0432e-01, -3.1233e-01],\n",
      "          [-1.1551e-01,  4.3944e-01,  1.3607e-01,  ..., -6.2944e-02,\n",
      "           -2.5979e-01, -2.8764e-02],\n",
      "          [-2.1726e-01,  5.1984e-01, -7.4972e-03,  ..., -1.0121e-01,\n",
      "           -5.6147e-01, -1.8461e-01]],\n",
      "\n",
      "         [[ 4.5489e-01, -3.4948e-01,  5.3839e-01,  ...,  1.1812e-01,\n",
      "            8.2117e-02,  1.7124e-01],\n",
      "          [ 8.3414e-01, -3.3398e-01,  7.3370e-02,  ..., -3.9293e-01,\n",
      "           -2.5024e-01,  4.6656e-02],\n",
      "          [-2.7639e-02,  1.4252e-01, -8.5912e-02,  ..., -1.9518e-02,\n",
      "           -2.3589e-02,  2.8576e-01],\n",
      "          ...,\n",
      "          [ 5.5886e-01,  5.0275e-01, -9.9409e-02,  ..., -1.4107e-01,\n",
      "           -1.3549e-01,  1.3512e-01],\n",
      "          [ 3.6913e-01, -4.2027e-01,  7.3338e-01,  ...,  8.2184e-02,\n",
      "            3.0860e-01,  1.9023e-01],\n",
      "          [ 7.9675e-01, -2.7875e-01,  4.3588e-01,  ..., -1.0544e-01,\n",
      "           -1.7917e-02, -1.9584e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.9295e-02,  2.0494e-01, -4.7975e-03,  ..., -4.9044e-01,\n",
      "           -2.7083e-01,  2.4271e-02],\n",
      "          [ 1.2941e-02, -2.6700e-01, -1.5023e-01,  ...,  2.3695e-01,\n",
      "           -1.4488e-01,  4.4903e-01],\n",
      "          [-3.0651e-01,  1.8228e-01, -6.0365e-02,  ..., -4.4616e-01,\n",
      "            3.2392e-01,  7.6669e-02],\n",
      "          ...,\n",
      "          [-2.2396e-01,  1.0517e-01, -2.2325e-01,  ..., -3.3939e-02,\n",
      "            2.4434e-02,  3.0712e-01],\n",
      "          [-9.3771e-01,  8.5500e-01, -7.2919e-01,  ..., -6.4170e-01,\n",
      "            9.1899e-01,  4.2282e-01],\n",
      "          [ 4.6549e-01, -8.4375e-02,  2.3884e-01,  ..., -1.0444e-01,\n",
      "           -7.9816e-01,  5.8314e-01]],\n",
      "\n",
      "         [[-1.3256e-01,  5.9601e-01, -1.3535e-01,  ...,  1.0708e+00,\n",
      "           -3.4130e-01,  5.8796e-01],\n",
      "          [-4.0031e-01,  2.2137e-01,  2.7294e-01,  ..., -8.3012e-02,\n",
      "           -3.0582e-01,  8.4462e-01],\n",
      "          [-2.9746e-01,  1.9875e-02,  3.2362e-01,  ...,  5.8164e-01,\n",
      "           -3.8398e-01,  1.2948e+00],\n",
      "          ...,\n",
      "          [-6.8934e-01,  4.0069e-01,  5.2332e-01,  ...,  4.7779e-01,\n",
      "           -8.9600e-01,  4.7081e-01],\n",
      "          [-5.7948e-01,  3.0864e-01,  7.0143e-01,  ...,  2.0274e-01,\n",
      "           -6.4357e-01,  4.4949e-01],\n",
      "          [-6.4063e-01,  2.7403e-01,  3.7135e-01,  ..., -6.0648e-03,\n",
      "           -5.4945e-01,  6.9397e-01]],\n",
      "\n",
      "         [[ 8.1531e-02, -8.0417e-02,  6.1975e-02,  ..., -3.5764e-01,\n",
      "            5.6226e-01,  3.6915e-01],\n",
      "          [-6.1186e-01,  1.9519e-01,  4.0820e-01,  ..., -6.5346e-01,\n",
      "            4.1650e-01, -5.0919e-01],\n",
      "          [-6.0355e-01, -6.4829e-02,  2.6066e-01,  ..., -6.0753e-01,\n",
      "            4.1690e-01, -1.0619e-01],\n",
      "          ...,\n",
      "          [-5.7858e-01, -1.0667e-01,  3.5084e-01,  ..., -6.5580e-01,\n",
      "            5.3641e-01, -2.2045e-01],\n",
      "          [-4.4036e-01,  2.0049e-01, -5.2761e-01,  ..., -4.6803e-01,\n",
      "            2.9812e-01, -4.4919e-03],\n",
      "          [ 9.1632e-02, -1.5355e-01,  2.4623e-01,  ..., -5.8234e-01,\n",
      "            6.4249e-01,  2.1948e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.4793e-01, -2.9164e-01,  1.1798e-01,  ...,  2.3959e-01,\n",
      "            8.7365e-02, -1.8304e-01],\n",
      "          [ 7.1551e-02, -4.0548e-01,  2.0729e-01,  ..., -4.6791e-01,\n",
      "            3.5786e-01,  1.8609e-01],\n",
      "          [-2.3844e-01, -2.2511e-01,  4.1091e-01,  ...,  4.5862e-01,\n",
      "            1.8404e-01,  7.5979e-02],\n",
      "          ...,\n",
      "          [-1.1000e-01, -5.7740e-01,  3.3437e-01,  ..., -5.4834e-01,\n",
      "            5.0374e-01, -5.7071e-01],\n",
      "          [-2.5750e-02, -5.2174e-01,  2.5303e-01,  ..., -2.3786e-01,\n",
      "            4.7181e-01, -5.1656e-01],\n",
      "          [ 3.9212e-01, -2.2885e-01,  1.5712e-01,  ...,  3.0947e-01,\n",
      "            3.4574e-01, -5.2711e-02]],\n",
      "\n",
      "         [[ 4.2538e-01,  4.5353e-01,  1.2700e-01,  ..., -3.8070e-01,\n",
      "           -1.0039e-01,  1.3760e-02],\n",
      "          [ 7.3353e-02,  1.6065e-01,  3.8837e-02,  ..., -1.6612e-01,\n",
      "            3.2539e-01, -2.6115e-01],\n",
      "          [ 1.3307e-01,  3.6230e-01,  3.5560e-01,  ..., -2.0223e-01,\n",
      "           -5.4144e-02, -5.9702e-01],\n",
      "          ...,\n",
      "          [ 3.3271e-01,  2.7384e-01,  9.1058e-01,  ..., -6.8907e-01,\n",
      "            1.9202e-01, -1.9739e-01],\n",
      "          [ 6.7893e-02,  3.8260e-01,  5.2463e-01,  ..., -1.6778e-01,\n",
      "            3.1047e-01, -1.6127e-01],\n",
      "          [ 2.0472e-01,  4.2271e-01, -3.6884e-02,  ...,  1.7339e-01,\n",
      "            6.1886e-01, -2.5198e-01]],\n",
      "\n",
      "         [[-2.6201e-02,  4.2892e-02, -1.2884e-01,  ..., -4.8532e-01,\n",
      "            3.8394e-01, -7.0707e-01],\n",
      "          [ 4.9076e-01,  8.4691e-01,  2.6701e-01,  ..., -8.8749e-01,\n",
      "            6.8527e-01, -3.2109e-02],\n",
      "          [ 1.7296e-01,  7.4011e-01, -4.3412e-01,  ..., -5.6196e-01,\n",
      "           -1.5765e-01,  2.9952e-01],\n",
      "          ...,\n",
      "          [ 1.7058e-01, -6.1463e-01, -4.6562e-01,  ...,  3.5287e-01,\n",
      "           -6.6381e-01, -1.7147e-01],\n",
      "          [-7.7515e-01,  2.2416e-01, -1.2662e-02,  ...,  6.4468e-01,\n",
      "           -2.5655e-01,  2.2389e-01],\n",
      "          [ 2.8420e-01,  3.1885e-01,  1.7346e-01,  ..., -4.2993e-01,\n",
      "            2.9269e-01, -2.4433e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.4071e-01, -2.7825e-01,  1.3341e-01,  ...,  2.9251e-01,\n",
      "            7.1916e-01,  3.1884e-02],\n",
      "          [ 2.4261e-01,  5.5601e-01, -3.7778e-01,  ..., -4.3500e-03,\n",
      "            1.0315e+00, -2.3222e-01],\n",
      "          [-3.3044e-01, -4.0753e-01,  2.9208e-01,  ...,  5.2939e-03,\n",
      "            1.6210e+00,  3.9556e-01],\n",
      "          ...,\n",
      "          [-7.2248e-02,  3.9507e-01, -2.2531e-01,  ..., -7.6233e-02,\n",
      "           -5.4306e-02,  2.9579e-01],\n",
      "          [ 4.5611e-01, -3.0219e-01,  3.3346e-01,  ...,  7.2762e-01,\n",
      "            5.7999e-01,  2.8652e-01],\n",
      "          [-9.9513e-01, -4.1706e-01,  1.6258e-01,  ...,  2.6462e-01,\n",
      "            5.4178e-01,  1.0018e-01]],\n",
      "\n",
      "         [[ 3.9263e-01, -8.4098e-01, -1.8766e-01,  ...,  3.2654e-01,\n",
      "            2.3071e-01,  3.6736e-01],\n",
      "          [-3.6515e-01, -6.6667e-01, -2.0525e-01,  ..., -8.2563e-02,\n",
      "            2.0923e-01,  1.3242e-01],\n",
      "          [ 2.6366e-01, -7.3263e-01, -6.0818e-04,  ...,  1.3993e-01,\n",
      "            4.3431e-01,  4.8702e-01],\n",
      "          ...,\n",
      "          [ 3.7649e-01, -8.9890e-01, -6.5742e-02,  ..., -9.9849e-01,\n",
      "            1.2734e-01,  3.3203e-01],\n",
      "          [ 2.9385e-01, -8.0958e-01,  1.0681e-01,  ...,  2.4611e-01,\n",
      "            1.5585e-01,  4.7888e-01],\n",
      "          [-1.9156e-01, -4.8420e-01, -5.3325e-01,  ..., -1.3070e+00,\n",
      "            2.4056e-01,  3.3837e-01]],\n",
      "\n",
      "         [[ 8.9472e-01,  7.3788e-01,  5.4692e-01,  ...,  2.0574e-01,\n",
      "           -1.8933e-02,  2.0512e-01],\n",
      "          [ 4.2799e-01, -2.4217e-01, -2.6678e-01,  ...,  3.7725e-01,\n",
      "           -1.9129e-01,  7.7682e-01],\n",
      "          [ 5.1894e-03,  2.7766e-01, -1.8488e-01,  ...,  3.3199e-01,\n",
      "            3.0357e-01,  7.1835e-01],\n",
      "          ...,\n",
      "          [ 5.7355e-02, -4.6527e-01, -3.3178e-01,  ...,  4.0403e-01,\n",
      "           -4.8888e-01,  6.8698e-01],\n",
      "          [ 7.7599e-01,  3.1980e-01,  3.0325e-01,  ...,  2.6604e-01,\n",
      "           -2.4436e-01,  4.6815e-01],\n",
      "          [ 8.2133e-01, -1.7685e-02, -3.6779e-01,  ...,  1.0833e-01,\n",
      "           -9.4827e-02,  7.4480e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6109e-01,  3.8316e-01, -3.5317e-01,  ...,  1.3212e-01,\n",
      "           -2.0413e-01, -2.4374e-01],\n",
      "          [-8.9549e-02,  5.0399e-01, -6.5639e-01,  ...,  5.2028e-01,\n",
      "           -4.8563e-01, -2.1895e-01],\n",
      "          [ 1.6616e-01, -4.1562e-01, -4.0586e-01,  ...,  2.8994e-01,\n",
      "            2.1557e-01,  1.2789e-01],\n",
      "          ...,\n",
      "          [-4.1460e-01, -8.5589e-01, -4.6663e-01,  ...,  3.0978e-01,\n",
      "            3.9882e-01,  2.2555e-01],\n",
      "          [ 2.9411e-01,  1.7158e-01, -5.0183e-02,  ..., -1.3152e-01,\n",
      "           -6.4166e-03, -1.8002e-01],\n",
      "          [ 3.0146e-02, -6.4981e-01, -6.4923e-01,  ...,  4.3632e-01,\n",
      "            1.6933e-01,  7.3136e-02]],\n",
      "\n",
      "         [[-1.5327e-01, -2.7182e-01,  3.1793e-01,  ...,  3.7502e-01,\n",
      "            3.9386e-01,  6.2427e-01],\n",
      "          [ 1.1513e-01, -5.6229e-01,  1.7839e-01,  ...,  5.2734e-01,\n",
      "            2.5943e-01, -9.9544e-02],\n",
      "          [-1.0602e-01, -2.3308e-01,  2.2594e-01,  ...,  4.7585e-01,\n",
      "            4.0451e-01, -8.0822e-02],\n",
      "          ...,\n",
      "          [ 2.0647e-01, -6.0084e-01,  3.4130e-01,  ...,  5.8837e-01,\n",
      "            2.8919e-01, -2.9388e-01],\n",
      "          [ 3.5841e-01, -1.1047e+00,  1.6670e-01,  ...,  5.3994e-01,\n",
      "            1.8940e-01,  3.8007e-01],\n",
      "          [-4.5000e-01, -2.4313e-01,  1.6802e-01,  ...,  2.9690e-01,\n",
      "            7.6262e-01,  5.7498e-01]],\n",
      "\n",
      "         [[ 2.4948e-01,  1.7732e-01,  5.6860e-01,  ..., -3.4419e-01,\n",
      "            2.3950e-01, -1.0063e+00],\n",
      "          [ 9.1258e-01, -2.6015e-01,  4.5902e-01,  ...,  2.9973e-01,\n",
      "            4.4914e-01, -9.3596e-01],\n",
      "          [-9.8624e-02,  7.6542e-01,  5.7637e-01,  ..., -2.7896e-02,\n",
      "            1.6406e-01, -1.3386e+00],\n",
      "          ...,\n",
      "          [ 6.6470e-02,  5.6028e-01,  5.6362e-01,  ..., -2.5880e-01,\n",
      "           -4.5017e-01, -1.0596e+00],\n",
      "          [ 3.0611e-01, -1.2370e-02,  5.5587e-01,  ..., -4.1938e-01,\n",
      "           -2.0176e-01, -7.5091e-01],\n",
      "          [ 1.7629e-01,  8.2877e-02,  3.5683e-01,  ..., -5.4968e-02,\n",
      "            4.3594e-01, -9.3648e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.1328e-01, -7.7880e-03, -5.7301e-01,  ..., -2.3938e-01,\n",
      "           -1.1993e-01,  2.2179e-01],\n",
      "          [-1.0713e+00, -1.8561e-01, -1.4299e-01,  ...,  3.4184e-01,\n",
      "           -7.9613e-01,  5.1186e-01],\n",
      "          [-4.8969e-01, -2.1426e-01,  7.6214e-01,  ...,  1.0503e-01,\n",
      "           -3.2875e-01,  1.4851e-01],\n",
      "          ...,\n",
      "          [-9.5968e-01,  3.7474e-01, -3.7995e-01,  ..., -5.3994e-01,\n",
      "           -1.3506e-01, -8.4683e-02],\n",
      "          [-7.7048e-01,  4.0488e-01, -1.5944e-01,  ...,  4.8248e-02,\n",
      "            6.4608e-02,  7.3195e-02],\n",
      "          [-5.6155e-01,  6.2963e-02, -2.0233e-01,  ...,  6.3242e-01,\n",
      "           -2.9964e-01,  4.8028e-01]],\n",
      "\n",
      "         [[-3.3242e-01,  3.8360e-01, -2.6460e-01,  ..., -1.3284e-01,\n",
      "            1.0011e-01,  1.5237e-01],\n",
      "          [ 2.5161e-01, -7.9590e-02, -4.1826e-01,  ...,  2.9021e-01,\n",
      "            6.4693e-01, -3.6847e-03],\n",
      "          [-7.2333e-01, -4.0529e-01,  6.0342e-01,  ...,  2.8749e-01,\n",
      "           -7.1828e-02, -2.0649e-01],\n",
      "          ...,\n",
      "          [-8.9132e-02,  4.2637e-01, -4.8257e-01,  ..., -8.7476e-01,\n",
      "            1.5025e-02,  1.2020e-01],\n",
      "          [-3.5065e-01,  4.3955e-01, -3.3772e-01,  ..., -7.2944e-01,\n",
      "            1.7876e-01, -8.7165e-03],\n",
      "          [-6.5447e-01,  5.7953e-01, -5.9845e-01,  ..., -2.2527e-01,\n",
      "            7.2166e-02,  2.2649e-01]],\n",
      "\n",
      "         [[-1.1765e+00, -1.0791e+00, -6.3752e-01,  ..., -6.2139e-01,\n",
      "           -7.1285e-01,  6.5751e-02],\n",
      "          [-4.5631e-03, -2.9865e-01, -2.2268e-01,  ..., -1.3656e-01,\n",
      "           -6.0378e-01,  1.1800e-01],\n",
      "          [-1.1140e-01, -1.1892e-02,  7.7987e-03,  ..., -3.0117e-01,\n",
      "            3.0194e-01, -2.5939e-01],\n",
      "          ...,\n",
      "          [ 1.9670e-01,  1.8955e-01, -6.7149e-01,  ..., -6.3535e-01,\n",
      "           -4.7198e-01,  2.4423e-01],\n",
      "          [ 2.9435e-01, -4.8777e-01, -4.1071e-01,  ..., -2.2581e-01,\n",
      "            5.3428e-02,  5.3353e-01],\n",
      "          [ 1.9321e-01,  2.6070e-02, -5.9207e-01,  ..., -5.7333e-01,\n",
      "           -5.1081e-01, -5.8947e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.8602e-02, -3.2072e-01, -3.6146e-01,  ..., -9.5316e-02,\n",
      "           -2.4882e-01,  5.1134e-01],\n",
      "          [ 4.1033e-02, -1.3715e-01,  6.2516e-02,  ..., -8.0728e-02,\n",
      "           -8.1578e-02,  5.5949e-01],\n",
      "          [ 4.4600e-01, -1.3292e-02, -2.8428e-01,  ..., -3.4495e-01,\n",
      "           -2.4854e-01,  4.2393e-01],\n",
      "          ...,\n",
      "          [-1.1417e-01, -7.7129e-01, -2.9200e-01,  ...,  3.5723e-01,\n",
      "           -2.2075e-02,  6.8474e-01],\n",
      "          [ 5.3302e-02, -5.0379e-01, -4.4527e-01,  ..., -8.5963e-03,\n",
      "           -2.3234e-01,  6.0333e-01],\n",
      "          [-4.1968e-01, -1.3486e+00, -1.1934e+00,  ...,  1.2290e+00,\n",
      "            5.7471e-01,  5.0396e-01]],\n",
      "\n",
      "         [[ 3.2046e-01,  4.9326e-01,  6.2410e-01,  ..., -3.5459e-01,\n",
      "           -5.5964e-01,  4.6966e-01],\n",
      "          [ 1.6407e-01,  6.8157e-01,  4.8084e-01,  ..., -4.6754e-01,\n",
      "            1.0563e-01,  6.6984e-02],\n",
      "          [-3.3670e-03,  7.3377e-01,  4.9035e-02,  ..., -6.1428e-01,\n",
      "           -6.7607e-03, -1.9141e-01],\n",
      "          ...,\n",
      "          [ 1.1692e-01,  9.5868e-01,  3.8383e-01,  ..., -4.8771e-01,\n",
      "           -1.1058e-01,  1.0258e-01],\n",
      "          [ 3.5611e-01,  4.3385e-01,  8.4536e-01,  ..., -3.7420e-01,\n",
      "           -6.4432e-01,  9.0045e-02],\n",
      "          [ 1.4333e-01,  3.2013e-01,  8.2390e-02,  ..., -4.9031e-01,\n",
      "           -1.5368e-01,  5.5332e-01]],\n",
      "\n",
      "         [[-1.7177e-01,  4.9464e-03, -9.9688e-02,  ...,  2.8670e-01,\n",
      "           -2.2025e-01,  1.9029e-01],\n",
      "          [-1.9806e-01,  2.3635e-01,  1.8135e-01,  ...,  2.5551e-01,\n",
      "            1.4106e-01,  2.4534e-01],\n",
      "          [-1.3516e-01,  3.4474e-01, -1.0897e-01,  ..., -6.8928e-01,\n",
      "           -1.2399e-02,  3.7540e-01],\n",
      "          ...,\n",
      "          [-3.0228e-01,  1.9857e-02,  2.1353e-01,  ...,  3.5883e-01,\n",
      "           -1.3377e-01,  5.1109e-01],\n",
      "          [-3.0204e-01,  2.9528e-01,  3.4658e-01,  ..., -1.8417e-01,\n",
      "            5.0822e-02,  2.1732e-01],\n",
      "          [-3.8690e-01,  3.8115e-02, -3.9720e-02,  ...,  1.8124e-01,\n",
      "           -1.0100e-01,  2.8935e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.4670e-02, -5.5875e-03,  1.9029e-01,  ...,  1.3918e-01,\n",
      "            1.7480e-01,  7.3661e-01],\n",
      "          [ 6.7880e-02,  5.7915e-02, -4.3781e-01,  ...,  1.1685e-01,\n",
      "           -8.1669e-02,  3.3072e-01],\n",
      "          [-1.7683e+00,  2.0060e-01, -7.1248e-01,  ...,  1.1680e-01,\n",
      "           -2.8363e-01,  2.1862e+00],\n",
      "          ...,\n",
      "          [ 2.0741e-01, -9.2974e-02,  2.8286e-01,  ...,  3.2948e-02,\n",
      "            1.1527e-01,  8.4983e-01],\n",
      "          [ 5.8870e-01, -3.1358e-01, -6.7451e-01,  ...,  3.6658e-01,\n",
      "           -9.6220e-02, -1.4572e-01],\n",
      "          [ 4.4004e-02, -1.4830e-01,  3.5024e-01,  ..., -1.6517e-01,\n",
      "            9.7271e-02,  5.4324e-01]],\n",
      "\n",
      "         [[ 5.4462e-01, -5.7459e-01,  3.1119e-02,  ..., -2.6727e-01,\n",
      "           -5.1736e-01,  3.9662e-01],\n",
      "          [-3.6090e-01, -6.6757e-01, -1.9034e-02,  ..., -4.9054e-01,\n",
      "            4.4066e-01, -4.4940e-01],\n",
      "          [ 5.6171e-01, -5.9143e-01, -2.3828e-01,  ..., -3.3758e-01,\n",
      "           -3.4995e-01,  2.8452e-01],\n",
      "          ...,\n",
      "          [ 1.5188e-01, -4.5765e-01, -2.2215e-02,  ..., -3.5744e-01,\n",
      "           -8.1350e-01, -1.7563e-01],\n",
      "          [-1.7202e-01, -1.0399e+00, -3.2499e-01,  ..., -1.3183e-01,\n",
      "            4.3848e-01, -4.2475e-01],\n",
      "          [ 3.1674e-01, -9.7147e-01, -7.0646e-01,  ...,  1.2359e-01,\n",
      "           -6.0220e-01,  1.5796e-01]],\n",
      "\n",
      "         [[-8.6757e-03,  4.0604e-01, -4.0565e-01,  ..., -1.8928e-01,\n",
      "           -2.8094e-01, -2.6138e-01],\n",
      "          [-2.6716e-01,  6.6344e-01, -5.6559e-01,  ..., -8.6945e-02,\n",
      "           -3.0200e-02,  3.9371e-02],\n",
      "          [-1.8804e-01,  2.9957e-01, -1.0868e+00,  ..., -3.6184e-01,\n",
      "           -3.5120e-01, -4.2509e-02],\n",
      "          ...,\n",
      "          [-4.1498e-01,  5.6118e-01, -5.5429e-01,  ..., -2.2918e-01,\n",
      "            1.3820e-01, -1.2322e-01],\n",
      "          [ 1.1048e-01,  4.6936e-01, -5.0433e-01,  ..., -1.1360e-01,\n",
      "           -3.3876e-01, -3.8663e-01],\n",
      "          [ 1.1824e-01,  3.6798e-01, -5.5920e-01,  ...,  1.7623e-01,\n",
      "           -4.6912e-01, -1.9337e-01]]]])\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0180, 0.0601, 0.0260,  ..., 0.3758, 0.1334, 0.0133],\n",
      "          [0.3908, 0.0807, 0.0795,  ..., 0.0656, 0.0289, 0.0188],\n",
      "          [0.3409, 0.0319, 0.1047,  ..., 0.0195, 0.0596, 0.1622],\n",
      "          ...,\n",
      "          [0.0573, 0.1623, 0.0544,  ..., 0.1582, 0.0978, 0.0624],\n",
      "          [0.0164, 0.0631, 0.0293,  ..., 0.0773, 0.0669, 0.1870],\n",
      "          [0.2110, 0.0840, 0.0599,  ..., 0.1923, 0.0307, 0.0273]],\n",
      "\n",
      "         [[0.0153, 0.0116, 0.0195,  ..., 0.1708, 0.1476, 0.1923],\n",
      "          [0.0374, 0.0141, 0.1279,  ..., 0.0389, 0.0975, 0.0265],\n",
      "          [0.0181, 0.0448, 0.0415,  ..., 0.1504, 0.0865, 0.0359],\n",
      "          ...,\n",
      "          [0.0938, 0.0161, 0.0439,  ..., 0.1646, 0.2964, 0.0866],\n",
      "          [0.0301, 0.0191, 0.0489,  ..., 0.1560, 0.4166, 0.1474],\n",
      "          [0.0130, 0.0220, 0.0455,  ..., 0.1164, 0.0349, 0.0251]],\n",
      "\n",
      "         [[0.1585, 0.1547, 0.0475,  ..., 0.0116, 0.1992, 0.1282],\n",
      "          [0.0722, 0.1171, 0.1464,  ..., 0.1562, 0.1852, 0.0427],\n",
      "          [0.3656, 0.1009, 0.0870,  ..., 0.1443, 0.0039, 0.1745],\n",
      "          ...,\n",
      "          [0.1781, 0.1176, 0.1237,  ..., 0.0436, 0.0457, 0.0461],\n",
      "          [0.0293, 0.0324, 0.0378,  ..., 0.0127, 0.1528, 0.5671],\n",
      "          [0.0205, 0.0271, 0.0066,  ..., 0.0541, 0.0829, 0.0748]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0674, 0.1447, 0.0614,  ..., 0.1408, 0.0622, 0.0418],\n",
      "          [0.0850, 0.0058, 0.0168,  ..., 0.2866, 0.0744, 0.0831],\n",
      "          [0.0403, 0.1013, 0.0745,  ..., 0.1138, 0.0481, 0.1796],\n",
      "          ...,\n",
      "          [0.1781, 0.1893, 0.0434,  ..., 0.0277, 0.2002, 0.0143],\n",
      "          [0.0249, 0.0547, 0.0369,  ..., 0.0607, 0.0684, 0.6659],\n",
      "          [0.0308, 0.0531, 0.0994,  ..., 0.0224, 0.0300, 0.0020]],\n",
      "\n",
      "         [[0.3551, 0.0537, 0.0336,  ..., 0.0255, 0.0558, 0.1335],\n",
      "          [0.0427, 0.2354, 0.0892,  ..., 0.0779, 0.0131, 0.0739],\n",
      "          [0.0313, 0.0454, 0.0679,  ..., 0.0916, 0.2596, 0.0181],\n",
      "          ...,\n",
      "          [0.0840, 0.2114, 0.0367,  ..., 0.0728, 0.0230, 0.1244],\n",
      "          [0.0323, 0.0954, 0.0564,  ..., 0.0589, 0.0339, 0.2172],\n",
      "          [0.0316, 0.0761, 0.0244,  ..., 0.1058, 0.1554, 0.2181]],\n",
      "\n",
      "         [[0.0213, 0.1196, 0.0624,  ..., 0.0917, 0.0746, 0.1551],\n",
      "          [0.2394, 0.0159, 0.2464,  ..., 0.1768, 0.0379, 0.0787],\n",
      "          [0.0857, 0.0233, 0.0303,  ..., 0.0066, 0.0233, 0.0529],\n",
      "          ...,\n",
      "          [0.1262, 0.0561, 0.0810,  ..., 0.0955, 0.1901, 0.1117],\n",
      "          [0.0380, 0.1666, 0.0137,  ..., 0.0412, 0.2690, 0.0999],\n",
      "          [0.6630, 0.0101, 0.0018,  ..., 0.0385, 0.0547, 0.0115]]],\n",
      "\n",
      "\n",
      "        [[[0.0492, 0.0418, 0.0296,  ..., 0.0240, 0.0345, 0.0299],\n",
      "          [0.0464, 0.0090, 0.0404,  ..., 0.1005, 0.0238, 0.3177],\n",
      "          [0.0121, 0.0779, 0.0240,  ..., 0.1205, 0.4320, 0.1175],\n",
      "          ...,\n",
      "          [0.0582, 0.1117, 0.0664,  ..., 0.0463, 0.0673, 0.0677],\n",
      "          [0.0298, 0.0735, 0.1760,  ..., 0.0270, 0.1170, 0.0251],\n",
      "          [0.0535, 0.1503, 0.1054,  ..., 0.0477, 0.0644, 0.0805]],\n",
      "\n",
      "         [[0.1101, 0.0441, 0.1039,  ..., 0.2537, 0.1368, 0.0854],\n",
      "          [0.0652, 0.1048, 0.0271,  ..., 0.0625, 0.0135, 0.5693],\n",
      "          [0.1378, 0.0135, 0.0246,  ..., 0.0358, 0.0322, 0.0763],\n",
      "          ...,\n",
      "          [0.0503, 0.2760, 0.0131,  ..., 0.0266, 0.0415, 0.0342],\n",
      "          [0.0850, 0.0809, 0.2621,  ..., 0.0647, 0.0179, 0.0441],\n",
      "          [0.0275, 0.0159, 0.1662,  ..., 0.2522, 0.2622, 0.0319]],\n",
      "\n",
      "         [[0.2243, 0.0564, 0.0249,  ..., 0.1848, 0.0348, 0.1166],\n",
      "          [0.0338, 0.0155, 0.0413,  ..., 0.3711, 0.1281, 0.2133],\n",
      "          [0.1205, 0.0650, 0.0410,  ..., 0.0655, 0.1849, 0.0958],\n",
      "          ...,\n",
      "          [0.1007, 0.0283, 0.1754,  ..., 0.0354, 0.1039, 0.0322],\n",
      "          [0.0273, 0.0364, 0.1514,  ..., 0.2430, 0.0056, 0.0186],\n",
      "          [0.1307, 0.1461, 0.0267,  ..., 0.0139, 0.0644, 0.0883]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0497, 0.0976, 0.1995,  ..., 0.0924, 0.0468, 0.0143],\n",
      "          [0.1058, 0.0734, 0.0328,  ..., 0.3537, 0.0966, 0.1342],\n",
      "          [0.0465, 0.3291, 0.1484,  ..., 0.0105, 0.0909, 0.1019],\n",
      "          ...,\n",
      "          [0.0622, 0.0290, 0.0676,  ..., 0.5618, 0.0686, 0.0370],\n",
      "          [0.1176, 0.0201, 0.1617,  ..., 0.1218, 0.0230, 0.0305],\n",
      "          [0.1329, 0.0734, 0.0970,  ..., 0.1289, 0.1481, 0.0493]],\n",
      "\n",
      "         [[0.0694, 0.0815, 0.0277,  ..., 0.0331, 0.2785, 0.0607],\n",
      "          [0.2392, 0.0503, 0.0588,  ..., 0.0837, 0.0995, 0.0608],\n",
      "          [0.1551, 0.1197, 0.0440,  ..., 0.0129, 0.1034, 0.0118],\n",
      "          ...,\n",
      "          [0.1332, 0.3311, 0.0648,  ..., 0.0341, 0.0507, 0.0159],\n",
      "          [0.1498, 0.1783, 0.0788,  ..., 0.1320, 0.0320, 0.0474],\n",
      "          [0.1833, 0.1201, 0.0216,  ..., 0.1769, 0.0809, 0.0548]],\n",
      "\n",
      "         [[0.0282, 0.0888, 0.0065,  ..., 0.2429, 0.0484, 0.0712],\n",
      "          [0.0718, 0.1492, 0.0117,  ..., 0.0873, 0.0175, 0.0662],\n",
      "          [0.0170, 0.0772, 0.4386,  ..., 0.0190, 0.0504, 0.0615],\n",
      "          ...,\n",
      "          [0.0622, 0.0347, 0.0758,  ..., 0.0895, 0.0079, 0.0603],\n",
      "          [0.0539, 0.0129, 0.1043,  ..., 0.0361, 0.0424, 0.3410],\n",
      "          [0.0750, 0.0918, 0.0465,  ..., 0.1471, 0.0887, 0.0808]]],\n",
      "\n",
      "\n",
      "        [[[0.1098, 0.1078, 0.1187,  ..., 0.1016, 0.1485, 0.0410],\n",
      "          [0.0216, 0.1040, 0.0177,  ..., 0.0599, 0.0227, 0.0190],\n",
      "          [0.2685, 0.0948, 0.4609,  ..., 0.0120, 0.0194, 0.0385],\n",
      "          ...,\n",
      "          [0.0793, 0.1351, 0.0214,  ..., 0.0145, 0.0885, 0.4300],\n",
      "          [0.1165, 0.1183, 0.1306,  ..., 0.2862, 0.0371, 0.1530],\n",
      "          [0.0235, 0.0288, 0.2751,  ..., 0.3076, 0.0412, 0.0159]],\n",
      "\n",
      "         [[0.0572, 0.0828, 0.0553,  ..., 0.0592, 0.0812, 0.0886],\n",
      "          [0.1107, 0.0793, 0.0911,  ..., 0.0350, 0.2710, 0.1198],\n",
      "          [0.0934, 0.1300, 0.2142,  ..., 0.0775, 0.0667, 0.0677],\n",
      "          ...,\n",
      "          [0.0425, 0.0577, 0.2739,  ..., 0.0307, 0.0169, 0.1236],\n",
      "          [0.0651, 0.0851, 0.1711,  ..., 0.5491, 0.0163, 0.0108],\n",
      "          [0.1495, 0.0238, 0.0349,  ..., 0.0966, 0.0363, 0.1772]],\n",
      "\n",
      "         [[0.0063, 0.3199, 0.0706,  ..., 0.2376, 0.0750, 0.0300],\n",
      "          [0.2314, 0.1314, 0.2063,  ..., 0.1823, 0.0302, 0.0179],\n",
      "          [0.0626, 0.0314, 0.2659,  ..., 0.0424, 0.1785, 0.0457],\n",
      "          ...,\n",
      "          [0.0129, 0.0688, 0.0094,  ..., 0.0402, 0.6924, 0.0766],\n",
      "          [0.4068, 0.0766, 0.0504,  ..., 0.0572, 0.0832, 0.0551],\n",
      "          [0.3015, 0.0925, 0.0768,  ..., 0.1096, 0.0310, 0.0906]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0882, 0.1289, 0.0735,  ..., 0.0402, 0.0089, 0.1673],\n",
      "          [0.1255, 0.0517, 0.0563,  ..., 0.0393, 0.1492, 0.0382],\n",
      "          [0.0258, 0.3486, 0.1235,  ..., 0.0597, 0.0092, 0.1368],\n",
      "          ...,\n",
      "          [0.1521, 0.1698, 0.0352,  ..., 0.0628, 0.1210, 0.1627],\n",
      "          [0.1328, 0.2829, 0.0270,  ..., 0.0635, 0.1892, 0.1300],\n",
      "          [0.0392, 0.0727, 0.1726,  ..., 0.1099, 0.0835, 0.0176]],\n",
      "\n",
      "         [[0.0818, 0.1122, 0.0910,  ..., 0.1233, 0.0253, 0.1720],\n",
      "          [0.2198, 0.1006, 0.0270,  ..., 0.0296, 0.0276, 0.4803],\n",
      "          [0.1236, 0.1021, 0.0835,  ..., 0.1357, 0.1487, 0.1172],\n",
      "          ...,\n",
      "          [0.1016, 0.1540, 0.0581,  ..., 0.0180, 0.1110, 0.0229],\n",
      "          [0.1517, 0.0828, 0.1610,  ..., 0.0976, 0.2230, 0.0190],\n",
      "          [0.1258, 0.0153, 0.0249,  ..., 0.1598, 0.1343, 0.1787]],\n",
      "\n",
      "         [[0.0700, 0.0685, 0.0725,  ..., 0.1497, 0.1041, 0.1879],\n",
      "          [0.0309, 0.0537, 0.0315,  ..., 0.1449, 0.0454, 0.1012],\n",
      "          [0.0725, 0.0352, 0.6416,  ..., 0.0484, 0.0724, 0.0163],\n",
      "          ...,\n",
      "          [0.2265, 0.0093, 0.1341,  ..., 0.0832, 0.1150, 0.1188],\n",
      "          [0.0417, 0.2668, 0.0731,  ..., 0.0419, 0.0115, 0.0537],\n",
      "          [0.1269, 0.0756, 0.0211,  ..., 0.0625, 0.1818, 0.0652]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.1426, 0.1541, 0.0219,  ..., 0.0704, 0.0088, 0.0358],\n",
      "          [0.0350, 0.1001, 0.2124,  ..., 0.0463, 0.1100, 0.0318],\n",
      "          [0.0283, 0.0241, 0.1580,  ..., 0.0373, 0.0367, 0.0980],\n",
      "          ...,\n",
      "          [0.0610, 0.0847, 0.0924,  ..., 0.0948, 0.1316, 0.2517],\n",
      "          [0.0513, 0.0954, 0.0725,  ..., 0.1627, 0.2312, 0.0473],\n",
      "          [0.2978, 0.0421, 0.0434,  ..., 0.0726, 0.1953, 0.0087]],\n",
      "\n",
      "         [[0.4307, 0.0096, 0.0683,  ..., 0.0575, 0.0624, 0.0324],\n",
      "          [0.0844, 0.0340, 0.0455,  ..., 0.1076, 0.0288, 0.0797],\n",
      "          [0.1846, 0.1171, 0.1856,  ..., 0.1629, 0.0439, 0.0595],\n",
      "          ...,\n",
      "          [0.5137, 0.0302, 0.0059,  ..., 0.0165, 0.0345, 0.1555],\n",
      "          [0.3279, 0.0781, 0.0561,  ..., 0.0400, 0.0301, 0.2533],\n",
      "          [0.0917, 0.1608, 0.1750,  ..., 0.0507, 0.0735, 0.1368]],\n",
      "\n",
      "         [[0.3877, 0.3131, 0.0325,  ..., 0.0017, 0.0329, 0.0335],\n",
      "          [0.2029, 0.0373, 0.1005,  ..., 0.1536, 0.1372, 0.0331],\n",
      "          [0.1069, 0.0369, 0.0478,  ..., 0.0729, 0.0162, 0.0180],\n",
      "          ...,\n",
      "          [0.0500, 0.1099, 0.0827,  ..., 0.2372, 0.0582, 0.1307],\n",
      "          [0.0708, 0.0638, 0.0849,  ..., 0.1010, 0.0375, 0.1568],\n",
      "          [0.0210, 0.0018, 0.0376,  ..., 0.2671, 0.3680, 0.1841]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0470, 0.1617, 0.0274,  ..., 0.0074, 0.0390, 0.2495],\n",
      "          [0.0418, 0.0636, 0.0386,  ..., 0.1869, 0.0105, 0.0430],\n",
      "          [0.2790, 0.0862, 0.0533,  ..., 0.0693, 0.0979, 0.0919],\n",
      "          ...,\n",
      "          [0.0555, 0.1660, 0.0169,  ..., 0.0203, 0.0397, 0.0446],\n",
      "          [0.0820, 0.0227, 0.0179,  ..., 0.0339, 0.2495, 0.0801],\n",
      "          [0.0477, 0.1988, 0.0364,  ..., 0.0321, 0.3356, 0.0179]],\n",
      "\n",
      "         [[0.1260, 0.1032, 0.0214,  ..., 0.0323, 0.1294, 0.0242],\n",
      "          [0.0520, 0.4773, 0.0891,  ..., 0.0501, 0.0632, 0.0762],\n",
      "          [0.1367, 0.1475, 0.1298,  ..., 0.1286, 0.0748, 0.0280],\n",
      "          ...,\n",
      "          [0.2629, 0.0451, 0.2190,  ..., 0.1020, 0.0257, 0.1475],\n",
      "          [0.1329, 0.0401, 0.0385,  ..., 0.1030, 0.1104, 0.0187],\n",
      "          [0.0886, 0.0966, 0.0275,  ..., 0.1190, 0.1123, 0.0552]],\n",
      "\n",
      "         [[0.0221, 0.2848, 0.0391,  ..., 0.0508, 0.3227, 0.0132],\n",
      "          [0.1446, 0.1239, 0.0521,  ..., 0.0469, 0.0460, 0.0873],\n",
      "          [0.1124, 0.1146, 0.0277,  ..., 0.0636, 0.1012, 0.0756],\n",
      "          ...,\n",
      "          [0.0369, 0.0396, 0.0247,  ..., 0.1494, 0.2262, 0.2411],\n",
      "          [0.0421, 0.0286, 0.0475,  ..., 0.1932, 0.0288, 0.0467],\n",
      "          [0.0154, 0.2641, 0.0218,  ..., 0.3280, 0.2374, 0.0116]]],\n",
      "\n",
      "\n",
      "        [[[0.0271, 0.0269, 0.1728,  ..., 0.0386, 0.0527, 0.0935],\n",
      "          [0.1299, 0.0404, 0.0244,  ..., 0.2647, 0.1125, 0.0556],\n",
      "          [0.0953, 0.0442, 0.0722,  ..., 0.0570, 0.0212, 0.0934],\n",
      "          ...,\n",
      "          [0.0841, 0.3293, 0.0960,  ..., 0.0253, 0.1281, 0.0444],\n",
      "          [0.2942, 0.1744, 0.0093,  ..., 0.1262, 0.0707, 0.0319],\n",
      "          [0.0154, 0.1283, 0.0654,  ..., 0.0952, 0.2904, 0.0866]],\n",
      "\n",
      "         [[0.0194, 0.0765, 0.1028,  ..., 0.2603, 0.0193, 0.1557],\n",
      "          [0.0187, 0.0366, 0.1536,  ..., 0.0611, 0.0108, 0.1527],\n",
      "          [0.0736, 0.1503, 0.1226,  ..., 0.0513, 0.2090, 0.0320],\n",
      "          ...,\n",
      "          [0.0184, 0.0566, 0.1694,  ..., 0.1474, 0.2732, 0.0171],\n",
      "          [0.3070, 0.0260, 0.0455,  ..., 0.0148, 0.3124, 0.0637],\n",
      "          [0.1568, 0.0712, 0.0431,  ..., 0.0408, 0.0760, 0.0348]],\n",
      "\n",
      "         [[0.0475, 0.0670, 0.0779,  ..., 0.0232, 0.3876, 0.0567],\n",
      "          [0.1764, 0.0632, 0.0043,  ..., 0.2662, 0.2292, 0.0115],\n",
      "          [0.0184, 0.0915, 0.0644,  ..., 0.1425, 0.0496, 0.0530],\n",
      "          ...,\n",
      "          [0.0184, 0.1122, 0.0541,  ..., 0.0276, 0.1092, 0.0349],\n",
      "          [0.2243, 0.0341, 0.0262,  ..., 0.0623, 0.1884, 0.0962],\n",
      "          [0.1000, 0.0753, 0.0393,  ..., 0.1729, 0.0510, 0.0537]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0222, 0.1101, 0.0793,  ..., 0.0699, 0.2203, 0.0798],\n",
      "          [0.0334, 0.1026, 0.2944,  ..., 0.0079, 0.0469, 0.0017],\n",
      "          [0.0275, 0.0225, 0.0332,  ..., 0.0323, 0.0583, 0.4778],\n",
      "          ...,\n",
      "          [0.0943, 0.0313, 0.1447,  ..., 0.2799, 0.0706, 0.0465],\n",
      "          [0.0948, 0.0492, 0.0660,  ..., 0.0383, 0.0723, 0.0797],\n",
      "          [0.0495, 0.1453, 0.1667,  ..., 0.0189, 0.0214, 0.0170]],\n",
      "\n",
      "         [[0.0830, 0.1049, 0.0401,  ..., 0.2221, 0.0445, 0.0345],\n",
      "          [0.0539, 0.3502, 0.0889,  ..., 0.0507, 0.2001, 0.0324],\n",
      "          [0.1873, 0.0052, 0.0082,  ..., 0.0161, 0.5662, 0.0433],\n",
      "          ...,\n",
      "          [0.0896, 0.1446, 0.0079,  ..., 0.0649, 0.2553, 0.0357],\n",
      "          [0.0970, 0.0576, 0.0540,  ..., 0.0309, 0.1652, 0.1105],\n",
      "          [0.0771, 0.0357, 0.0902,  ..., 0.0206, 0.0029, 0.5580]],\n",
      "\n",
      "         [[0.2576, 0.0526, 0.0671,  ..., 0.0716, 0.0395, 0.1167],\n",
      "          [0.0283, 0.0634, 0.0748,  ..., 0.0148, 0.0681, 0.0397],\n",
      "          [0.2895, 0.0944, 0.0212,  ..., 0.0422, 0.0138, 0.0302],\n",
      "          ...,\n",
      "          [0.0748, 0.0287, 0.1717,  ..., 0.0379, 0.1582, 0.0562],\n",
      "          [0.0238, 0.0707, 0.0108,  ..., 0.0285, 0.3062, 0.2417],\n",
      "          [0.1744, 0.0389, 0.0391,  ..., 0.1119, 0.0966, 0.0307]]],\n",
      "\n",
      "\n",
      "        [[[0.2146, 0.0614, 0.0335,  ..., 0.0343, 0.2316, 0.0136],\n",
      "          [0.0067, 0.0115, 0.0237,  ..., 0.2094, 0.0015, 0.0213],\n",
      "          [0.4561, 0.0843, 0.0585,  ..., 0.0277, 0.0679, 0.0789],\n",
      "          ...,\n",
      "          [0.1972, 0.0735, 0.1557,  ..., 0.0404, 0.0178, 0.1443],\n",
      "          [0.0245, 0.0956, 0.0688,  ..., 0.2179, 0.0818, 0.0668],\n",
      "          [0.2538, 0.1715, 0.0993,  ..., 0.0310, 0.0643, 0.1142]],\n",
      "\n",
      "         [[0.2071, 0.0065, 0.0670,  ..., 0.0496, 0.1428, 0.0777],\n",
      "          [0.0135, 0.0527, 0.0302,  ..., 0.0067, 0.0944, 0.0361],\n",
      "          [0.0454, 0.2344, 0.2695,  ..., 0.1186, 0.1153, 0.0420],\n",
      "          ...,\n",
      "          [0.1289, 0.0806, 0.1160,  ..., 0.0589, 0.2998, 0.0559],\n",
      "          [0.0509, 0.1310, 0.0383,  ..., 0.0858, 0.1748, 0.0630],\n",
      "          [0.0710, 0.0631, 0.0502,  ..., 0.0578, 0.2408, 0.1101]],\n",
      "\n",
      "         [[0.0486, 0.0282, 0.0469,  ..., 0.5666, 0.0293, 0.0477],\n",
      "          [0.0110, 0.1612, 0.0695,  ..., 0.0653, 0.0852, 0.0313],\n",
      "          [0.0743, 0.2208, 0.0201,  ..., 0.0620, 0.0380, 0.0264],\n",
      "          ...,\n",
      "          [0.1328, 0.0604, 0.0786,  ..., 0.0519, 0.2531, 0.1054],\n",
      "          [0.0298, 0.0292, 0.0476,  ..., 0.1435, 0.0448, 0.0508],\n",
      "          [0.0958, 0.0589, 0.0706,  ..., 0.4174, 0.0674, 0.1023]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.2508, 0.0261, 0.0617,  ..., 0.0770, 0.0702, 0.1751],\n",
      "          [0.0562, 0.1205, 0.0139,  ..., 0.0287, 0.0833, 0.0682],\n",
      "          [0.1043, 0.1349, 0.0783,  ..., 0.1463, 0.1027, 0.1722],\n",
      "          ...,\n",
      "          [0.0636, 0.1558, 0.1989,  ..., 0.2701, 0.0265, 0.0815],\n",
      "          [0.0263, 0.2056, 0.0327,  ..., 0.1342, 0.0308, 0.0325],\n",
      "          [0.0786, 0.2393, 0.0189,  ..., 0.0939, 0.0560, 0.1235]],\n",
      "\n",
      "         [[0.1218, 0.0235, 0.0535,  ..., 0.0915, 0.2435, 0.0913],\n",
      "          [0.0602, 0.1750, 0.0981,  ..., 0.2112, 0.0276, 0.0402],\n",
      "          [0.4086, 0.0642, 0.0288,  ..., 0.0201, 0.2383, 0.0632],\n",
      "          ...,\n",
      "          [0.0525, 0.0224, 0.3096,  ..., 0.1063, 0.0307, 0.0366],\n",
      "          [0.0387, 0.4206, 0.1334,  ..., 0.0469, 0.0168, 0.0332],\n",
      "          [0.0882, 0.1884, 0.0972,  ..., 0.0406, 0.0784, 0.1992]],\n",
      "\n",
      "         [[0.0324, 0.0560, 0.0963,  ..., 0.0431, 0.0267, 0.0443],\n",
      "          [0.0893, 0.0777, 0.0088,  ..., 0.0454, 0.1362, 0.1075],\n",
      "          [0.1275, 0.1861, 0.2640,  ..., 0.0833, 0.0962, 0.0415],\n",
      "          ...,\n",
      "          [0.0547, 0.2598, 0.2708,  ..., 0.0376, 0.0249, 0.1221],\n",
      "          [0.3385, 0.1098, 0.0345,  ..., 0.0092, 0.0901, 0.0020],\n",
      "          [0.0540, 0.0405, 0.0620,  ..., 0.1908, 0.2293, 0.1006]]]])\n"
     ]
    }
   ],
   "source": [
    "print(att_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
