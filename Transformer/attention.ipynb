{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from typing import Optional, Tuple, Union \n",
    "import math\n",
    "import warnings\n",
    "from einops import rearrange"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单头注意力机制 (ScaledDotProductAttention)\n",
    "\n",
    "<img src=\"./images/attention.png\" alt=\"示例图片\" width=\"700\">\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = [softmax(\\frac{QK^{\\top}}{\\sqrt{d_k}})] V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    '''\n",
    "    Compute 'Scaled Dot Product Attention'.\n",
    "    The input consists of queries and keys of dimension d_k, and values of dimension d_v.\n",
    "    K = W_k * x\n",
    "    x: [batch_size, ..., features]\n",
    "    Q, K: [batch_size, ... , features]\n",
    "    '''\n",
    "    \n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    alpha_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        alpha_attn = dropout(alpha_attn)\n",
    "    return torch.matmul(alpha_attn, value), alpha_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\" Scaled Dot-Product Attention \"\"\"\n",
    "    def __init__(self, scale):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v: [b, l, d]\n",
    "        # u: [b, l_q, l_k]\n",
    "        u = torch.bmm(q, k.transpose(1, 2)) # 1. Matmul\n",
    "        # torch.bmm() 是一个用于批量矩阵乘法（Batch Matrix Multiplication）的函数。它的全名是 batch matrix multiplication\n",
    "        # torch.bmm(input, mat2, out=None) -> Tensor\n",
    "        # input：一个 3D 张量，形状为 (b, n, m)，表示一个批次中 b 个矩阵，每个矩阵的维度为 (n, m)。\n",
    "        # mat2：一个 3D 张量，形状为 (b, m, p)，表示另一个批次中 b 个矩阵，每个矩阵的维度为 (m, p)\n",
    "        # 返回一个 3D 张量，形状为 (b, n, p)，即每个批次中的矩阵相乘后的结果。\n",
    "        u = u / self.scale # 2. Sclae\n",
    "        \n",
    "        if mask is not None:\n",
    "            u = u.masked_fill(mask, -np.inf) # 3. Mask\n",
    "            \n",
    "        attn = self.softmax(u) # 4. Softmax\n",
    "        # attn: [b, l_q, l_k]\n",
    "        output = torch.bmm(attn, v) # Output\n",
    "        # output: [b, l_q, d_v]\n",
    "        \n",
    "        return attn, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 4])\n",
      "tensor([[[0.1412, 0.5310, 0.1467, 0.1812],\n",
      "         [0.1449, 0.0711, 0.6258, 0.1582]],\n",
      "\n",
      "        [[0.2215, 0.2577, 0.3899, 0.1310],\n",
      "         [0.5084, 0.2397, 0.0741, 0.1778]],\n",
      "\n",
      "        [[0.3534, 0.2216, 0.0810, 0.3440],\n",
      "         [0.3795, 0.3106, 0.1051, 0.2048]],\n",
      "\n",
      "        [[0.2082, 0.0021, 0.7299, 0.0598],\n",
      "         [0.1500, 0.2821, 0.0438, 0.5241]]])\n",
      "torch.Size([4, 2, 64])\n",
      "tensor([[[-3.9275e-01,  2.6675e-01,  1.2438e+00,  1.0520e+00,  1.1428e-01,\n",
      "           3.7404e-01, -7.1114e-01, -5.8611e-01, -2.2759e-01,  2.0379e-01,\n",
      "           4.8823e-01, -3.5633e-01, -6.4568e-01,  3.4941e-01, -5.0592e-01,\n",
      "           5.2288e-01, -1.5472e-01,  6.6828e-01, -9.9948e-01, -7.6306e-01,\n",
      "           3.9175e-01,  5.2225e-01, -5.6732e-01, -8.5301e-02, -2.0100e-01,\n",
      "          -5.2807e-01,  1.3071e-01, -2.2853e+00, -4.6790e-01, -3.4336e-01,\n",
      "          -4.7688e-01,  1.7989e-01, -2.8195e-01, -9.1272e-01, -7.9047e-01,\n",
      "           1.8103e-01, -2.8280e-01, -7.2264e-02,  3.7008e-01, -4.7586e-01,\n",
      "          -1.0143e-01,  5.2711e-01, -8.9047e-01,  2.7937e-01,  3.5697e-01,\n",
      "          -8.6581e-01, -7.3133e-01,  3.3643e-01, -1.5580e-01, -4.9677e-01,\n",
      "           7.5342e-02, -8.0150e-02,  5.8341e-01,  7.9848e-01, -1.6788e-01,\n",
      "          -2.3470e-01,  4.2486e-01, -5.5933e-02, -7.1764e-01, -6.0341e-01,\n",
      "           1.5707e+00,  3.7311e-01, -9.1531e-01,  7.1706e-01],\n",
      "         [-5.1326e-01,  6.2342e-01,  7.5433e-01, -4.4802e-01,  9.4168e-01,\n",
      "          -9.1373e-01,  1.8464e-01, -1.0032e+00, -1.6929e-01, -1.1136e-01,\n",
      "           1.1449e+00, -6.0274e-01,  2.0232e-01,  2.5070e-01,  3.6415e-01,\n",
      "           8.9348e-02, -1.3361e-01,  1.0818e+00, -4.2225e-01, -1.2612e+00,\n",
      "          -8.2575e-01,  8.1113e-01,  5.5574e-01,  1.9363e-01,  7.9629e-01,\n",
      "           2.3604e-02,  1.3761e-01, -8.1735e-01, -1.4315e+00, -5.7464e-01,\n",
      "          -7.0656e-01, -1.7129e-01,  2.9346e-01, -1.3087e+00,  3.3153e-01,\n",
      "           1.6387e-01, -1.2695e-01,  3.4128e-01,  8.0472e-01,  7.0167e-02,\n",
      "          -4.6515e-02,  1.5244e-01, -1.6177e+00,  6.5571e-01,  1.0965e+00,\n",
      "          -1.4775e+00,  5.9698e-01,  8.4178e-02,  1.1054e-01,  4.0543e-01,\n",
      "          -1.4136e+00, -1.0663e-01, -9.0362e-01,  1.1108e+00,  3.4603e-01,\n",
      "          -3.5667e-01,  5.4746e-02, -7.2423e-01, -6.1021e-01, -2.0835e-01,\n",
      "           6.3490e-01, -1.2424e+00, -7.6893e-01,  1.3784e+00]],\n",
      "\n",
      "        [[ 7.5900e-01,  7.3852e-01, -1.5335e-01,  5.8930e-02,  1.9924e-01,\n",
      "          -1.8089e-01,  1.4352e-01, -7.3072e-01, -4.6352e-01,  6.3938e-01,\n",
      "          -5.0660e-02, -1.0192e-02, -3.7960e-01,  7.9804e-01,  6.3838e-01,\n",
      "          -1.1605e+00, -4.1870e-01, -1.7303e-01, -9.8845e-01, -1.2212e-01,\n",
      "           8.9683e-03, -2.4439e-01, -2.2407e-01, -2.4999e-01,  1.0304e-01,\n",
      "          -1.7272e+00,  4.1524e-01, -3.2353e-02, -6.3218e-01,  4.1447e-01,\n",
      "          -5.4769e-02,  5.2059e-01,  4.2821e-01,  3.1644e-01, -3.3352e-01,\n",
      "          -1.3193e+00,  6.0902e-01,  7.1920e-01, -3.3755e-01,  3.4663e-01,\n",
      "          -1.8587e-01, -4.6436e-01, -1.6581e-01,  6.8394e-01,  1.8213e-01,\n",
      "          -4.1040e-01,  1.3304e-01,  4.6584e-01, -9.0106e-01,  8.0236e-02,\n",
      "          -6.2370e-01,  3.5568e-01, -1.8874e+00, -3.8291e-01, -1.8786e-01,\n",
      "           4.4572e-01, -3.7907e-01,  3.4755e-01,  6.4170e-01, -5.5396e-02,\n",
      "           2.2389e-01,  1.6504e-01, -5.0909e-01,  4.7672e-01],\n",
      "         [ 1.7911e-01,  1.7310e-01, -5.2102e-01,  9.5716e-01,  2.5879e-01,\n",
      "          -3.6432e-01, -2.2162e-01, -5.4294e-01, -7.1580e-01,  5.9423e-01,\n",
      "          -6.4809e-02, -1.0601e-01,  6.6519e-01, -2.4188e-01, -2.0657e-01,\n",
      "          -1.6590e+00,  3.9510e-01,  2.5966e-01, -3.3733e-01, -2.1943e-01,\n",
      "          -7.2097e-02, -5.0395e-01,  8.0536e-02, -1.8383e-01,  6.5894e-01,\n",
      "          -1.4690e+00, -1.5066e-01, -4.5265e-01, -2.0158e-01, -5.5885e-01,\n",
      "          -2.9777e-01,  5.7347e-01,  4.6692e-01,  1.0435e+00, -5.0444e-01,\n",
      "          -1.1250e+00,  5.3360e-01,  1.5228e+00,  6.2406e-02,  6.6272e-01,\n",
      "          -1.1329e-01, -5.6006e-01,  1.1326e-01,  5.7117e-01,  7.5018e-01,\n",
      "          -8.1835e-02,  5.8200e-01,  3.8345e-01, -7.1146e-01,  6.1152e-01,\n",
      "          -3.9829e-01,  5.1509e-01, -1.8675e+00, -1.6394e-01,  3.3425e-01,\n",
      "           5.3087e-01,  6.1730e-03,  3.0340e-01,  8.9061e-01, -1.7659e-01,\n",
      "           3.3677e-01,  7.1735e-01, -5.2505e-01,  8.9598e-01]],\n",
      "\n",
      "        [[-6.7236e-01,  5.5916e-02, -8.5901e-01,  4.7124e-01, -4.8481e-01,\n",
      "          -1.2793e+00, -5.9819e-01, -9.6723e-01,  6.8913e-01, -1.4606e+00,\n",
      "           3.8184e-01,  3.1591e-02,  5.3512e-01, -1.4793e-01,  6.3469e-02,\n",
      "          -2.7662e-01,  2.9113e-01,  7.5000e-01,  9.9257e-01, -2.7509e-02,\n",
      "           4.7389e-01, -7.8918e-01,  2.3074e-02, -3.4546e-01,  9.1155e-01,\n",
      "           4.2012e-01,  1.1397e+00, -1.4963e-01, -3.2597e-02,  4.2724e-01,\n",
      "           4.4056e-01, -8.3157e-01, -4.8873e-01,  6.5416e-01, -1.3173e-01,\n",
      "           8.5318e-01, -5.5922e-01, -5.0975e-02, -3.6131e-01,  1.3112e+00,\n",
      "           4.3724e-01,  1.5363e-01, -1.0433e+00,  7.6403e-02, -1.1127e+00,\n",
      "           1.0407e+00,  5.5682e-02, -5.0859e-02,  6.8390e-01, -3.2481e-01,\n",
      "           5.3557e-01, -3.2934e-01,  3.8288e-01, -1.0261e+00,  1.2498e-01,\n",
      "           6.3805e-01,  4.9496e-01,  6.1351e-01,  4.7180e-01, -8.6992e-01,\n",
      "          -9.4141e-01,  5.0414e-01,  6.2279e-01,  7.5607e-01],\n",
      "         [-6.8574e-01,  8.8741e-02, -9.8351e-01,  5.7004e-01, -2.2907e-01,\n",
      "          -9.1311e-01, -5.0824e-01, -1.0065e+00,  7.1861e-01, -1.7778e+00,\n",
      "           3.6387e-01,  3.4878e-01,  3.7122e-01,  9.5473e-02,  5.1801e-01,\n",
      "          -4.3603e-01,  5.7054e-01,  6.1779e-01,  1.0209e+00, -1.4762e-01,\n",
      "           2.2129e-01, -7.9444e-01,  2.8188e-01, -5.9953e-01,  6.9239e-01,\n",
      "           3.8500e-01,  1.3507e+00, -4.7096e-01, -2.8849e-01,  4.4762e-01,\n",
      "           3.2145e-01, -1.0583e+00, -7.4874e-01,  6.5586e-01, -5.5241e-01,\n",
      "           1.0464e+00, -4.4536e-01,  1.5208e-01, -2.8458e-01,  9.6592e-01,\n",
      "           5.8658e-02,  2.1679e-01, -7.9027e-01, -2.3894e-02, -1.2196e+00,\n",
      "           9.6765e-01,  4.0638e-01,  2.7930e-02,  4.8438e-01, -4.8526e-01,\n",
      "           6.0093e-01, -1.3451e-01,  1.6635e-01, -1.0226e+00,  8.6025e-02,\n",
      "           5.5821e-01,  2.7074e-01,  6.7274e-01,  2.9279e-01, -6.8906e-01,\n",
      "          -9.5130e-01,  2.8391e-01,  5.8215e-01,  5.9498e-01]],\n",
      "\n",
      "        [[-4.3782e-01,  5.4436e-01,  7.2326e-01, -1.3089e+00,  2.2067e-02,\n",
      "          -5.1071e-01,  3.9104e-01, -1.4930e-01,  2.5778e-01, -2.4143e-01,\n",
      "          -6.0603e-01,  6.8815e-01, -4.9885e-02, -8.1047e-01, -1.8687e+00,\n",
      "           1.8475e-01,  6.2168e-01,  2.7918e-01, -8.1162e-01,  5.6056e-01,\n",
      "           6.7653e-01, -6.2403e-01,  8.7470e-03, -1.3286e-01,  1.9664e-01,\n",
      "           1.0472e+00, -5.6570e-01,  4.7362e-01,  1.3276e+00, -1.9191e+00,\n",
      "          -1.2788e+00,  9.4340e-01,  3.1530e-01, -1.0909e+00,  3.1089e-01,\n",
      "           1.0935e+00, -1.1064e-01,  4.0675e-01,  5.2856e-01, -3.3077e-02,\n",
      "           3.7926e-01,  7.3931e-01, -2.0997e-01,  9.6917e-01, -2.0955e-03,\n",
      "          -5.6487e-01, -3.4981e-01,  1.6896e-03, -3.4731e-02,  9.0893e-01,\n",
      "          -5.1302e-02, -3.5122e-01, -5.1843e-01,  6.7853e-01,  5.4660e-01,\n",
      "           3.5619e-01,  5.3045e-01,  1.0893e+00, -3.6175e-01, -1.2209e+00,\n",
      "           1.4021e+00, -7.8304e-01, -3.9241e-01, -3.0617e-01],\n",
      "         [-8.3828e-01,  1.4156e-01, -3.8542e-01,  1.1589e-01, -1.5165e+00,\n",
      "           1.1550e+00, -2.1972e-01, -4.3573e-01, -1.4012e-01,  2.2595e-01,\n",
      "           7.1537e-01, -3.4308e-01,  4.8249e-01, -5.5289e-01, -8.6313e-01,\n",
      "           2.8165e-01, -2.9377e-01,  6.4120e-01, -1.3757e-01, -1.2789e+00,\n",
      "           5.9082e-01,  4.6469e-01,  3.8658e-01, -9.3959e-01,  1.2563e+00,\n",
      "           2.5052e-01,  6.0901e-01,  1.8433e+00,  6.0621e-01,  1.1222e-01,\n",
      "          -4.4456e-01,  1.9178e-01,  1.1595e+00, -5.4237e-01,  2.2141e-01,\n",
      "          -4.3624e-01,  5.9385e-01, -3.2897e-01,  2.7115e-01, -2.0206e-01,\n",
      "           6.5911e-01, -1.4948e-01, -8.4680e-01,  1.1545e-01,  3.4769e-01,\n",
      "           4.3397e-01,  2.4728e-01,  4.7946e-01, -7.3289e-01,  2.7363e-01,\n",
      "          -8.9092e-01,  4.1609e-01, -1.1410e+00, -1.5084e+00,  5.3052e-01,\n",
      "           9.6544e-01, -1.6316e+00,  1.3932e+00, -6.1923e-01,  4.9028e-02,\n",
      "           6.3326e-01, -5.0503e-02, -5.3446e-01,  4.9406e-01]]])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    n_q, n_k, n_v = 2, 4, 4\n",
    "    d_q, d_k, d_v = 128, 128, 64\n",
    "    batch = 4\n",
    "    \n",
    "    q = torch.randn(batch, n_q, d_q)\n",
    "    k = torch.randn(batch, n_k, d_k)\n",
    "    v = torch.randn(batch, n_v, d_v)\n",
    "    mask = torch.zeros(batch, n_q, n_k).bool()\n",
    "    \n",
    "    attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))\n",
    "    attn, output = attention(q, k, v, mask=mask)\n",
    "    \n",
    "    print(attn.shape)\n",
    "    print(attn)\n",
    "    print(output.shape)\n",
    "    print(output)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多头注意力机制 (Multi Head Attention)\n",
    "\n",
    "<img src=\"./images/MultiHeadAttention.png\" alt=\"示例图片\" width=\"700\">\n",
    "\n",
    "$MultiHead(Q, K, V) = Concat(head_1, head_2, ... , head_h)W^O\\text{, where }head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$\n",
    "\n",
    "$W_i^Q \\in \\mathbb{R}^{d_{model} \\times d_k}, \\quad W_i^K \\in \\mathbb{R}^{d_{model} \\times d_K}, \\quad W_i^V \\in \\mathbb{R}^{d_{model} \\times d_V}, \\quad W_i^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_ori(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadAttention_ori, self).__init__()\n",
    "        \n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h \n",
    "        self.h = h\n",
    "        # linears: W_i^Q, W_i^K, W_i^V, W_i^O\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4) \n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        '''\n",
    "        Implements Figure\n",
    "        '''\n",
    "        if mask is not None:\n",
    "            # Some mask applied to all h heads\n",
    "            mask = mask.unsqueeze(1)\n",
    "        # K = W_k * x\n",
    "        # x: [batch_size, ..., features]\n",
    "        # Q, K: [batch_size, ... , features]\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguoous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        \n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        \n",
    "        return self.linears[-1](x) # W_i^O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-Head Attention \"\"\"\n",
    "    def __init__(self, n_head, d_k_, d_v_, d_k, d_v, d_o):\n",
    "        '''\n",
    "        d_k_, d_v_: 这是输入Q, V矩阵的维度。\n",
    "        d_k: 每个头的查询和键的维度。\n",
    "        d_q_ = d_k_ ?\n",
    "        d_q = d_k ?\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.fc_q = nn.Linear(d_k_, n_head * d_k)\n",
    "        self.fc_k = nn.Linear(d_k_, n_head * d_k)\n",
    "        self.fc_v = nn.Linear(d_v_, n_head * d_v)\n",
    "        # 这三个全连接层用于将输入的查询（Q）、键（K）、值（V）映射到多个注意力头的空间\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))\n",
    "        \n",
    "        self.fc_o = nn.Linear(n_head * d_v, d_o)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        n_head, d_q, d_k, d_v = self.n_head, self.d_k, self.d_k, self.d_v\n",
    "        batch, n_q, d_q_ = q.size()\n",
    "        batch, n_k, d_k_ = k.size()\n",
    "        batch, n_v, d_v_ = v.size()\n",
    "        \n",
    "        q = self.fc_q(q) # 1. 单头变多头\n",
    "        k = self.fc_k(k)\n",
    "        v = self.fc_v(v)\n",
    "        \n",
    "        # 重塑为多头的形状\n",
    "        # q, k, v: [b, l, h*d] -> [b, l, h, d] -> [h, b, l, d] -> [b*h, l, d]\n",
    "        q = q.view(batch, n_q, n_head, d_q).permute(2, 0, 1, 3).contiguous().view(-1, n_q, d_q)\n",
    "        k = k.view(batch, n_k, n_head, d_k).permute(2, 0, 1, 3).contiguous().view(-1, n_k, d_k)\n",
    "        v = v.view(batch, n_v, n_head, d_v).permute(2, 0, 1, 3).contiguous().view(-1, n_v, d_v)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1)\n",
    "        \n",
    "        attn, output = self.attention(q, k, v, mask=mask) # 2. 当成单头注意力求输出\n",
    "        # attn: [b*h, l_q, l_k]\n",
    "        \n",
    "        # output: [b*h, l_q, d_v] -> [b, h, l_q, d_v] -> [h, l_q, b, d_v] -> [b, l_q, h*d_v]\n",
    "        output = output.view(n_head, batch, n_q, d_v).permute(1, 2, 0, 3).contiguous().view(batch, n_q, -1) # 3. Concat\n",
    "        \n",
    "        # [..., h*d_v] -> [..., d_o]\n",
    "        output = self.fc_o(output) # 4. 仿射变换得到最终输出\n",
    "        \n",
    "        return attn, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2, 4])\n",
      "torch.Size([4, 2, 128])\n",
      "tensor([[[0.2009, 0.3082, 0.2627, 0.2283],\n",
      "         [0.3128, 0.3074, 0.2294, 0.1505]],\n",
      "\n",
      "        [[0.3394, 0.2519, 0.2555, 0.1533],\n",
      "         [0.2826, 0.1722, 0.3127, 0.2325]],\n",
      "\n",
      "        [[0.1779, 0.4567, 0.1665, 0.1989],\n",
      "         [0.2524, 0.2534, 0.3248, 0.1694]],\n",
      "\n",
      "        [[0.2794, 0.3056, 0.2340, 0.1810],\n",
      "         [0.2835, 0.1834, 0.2774, 0.2557]],\n",
      "\n",
      "        [[0.2392, 0.3608, 0.2344, 0.1657],\n",
      "         [0.2105, 0.3367, 0.2348, 0.2181]],\n",
      "\n",
      "        [[0.2490, 0.1739, 0.2463, 0.3308],\n",
      "         [0.2697, 0.3590, 0.2173, 0.1540]],\n",
      "\n",
      "        [[0.2851, 0.2632, 0.2858, 0.1658],\n",
      "         [0.2566, 0.3526, 0.2540, 0.1368]],\n",
      "\n",
      "        [[0.1978, 0.1414, 0.2378, 0.4230],\n",
      "         [0.3562, 0.2322, 0.1648, 0.2468]],\n",
      "\n",
      "        [[0.1987, 0.2720, 0.2849, 0.2444],\n",
      "         [0.1918, 0.1703, 0.3279, 0.3099]],\n",
      "\n",
      "        [[0.3124, 0.3550, 0.1565, 0.1760],\n",
      "         [0.2070, 0.2443, 0.3431, 0.2056]],\n",
      "\n",
      "        [[0.2701, 0.2096, 0.2996, 0.2207],\n",
      "         [0.2853, 0.2200, 0.3664, 0.1284]],\n",
      "\n",
      "        [[0.2146, 0.1276, 0.2126, 0.4453],\n",
      "         [0.2607, 0.3443, 0.2327, 0.1622]],\n",
      "\n",
      "        [[0.1977, 0.2016, 0.2146, 0.3861],\n",
      "         [0.3368, 0.1723, 0.2626, 0.2283]],\n",
      "\n",
      "        [[0.2120, 0.1572, 0.1679, 0.4629],\n",
      "         [0.1487, 0.3057, 0.2603, 0.2853]],\n",
      "\n",
      "        [[0.2508, 0.2133, 0.2628, 0.2731],\n",
      "         [0.2862, 0.2390, 0.1734, 0.3013]],\n",
      "\n",
      "        [[0.3134, 0.2391, 0.3127, 0.1348],\n",
      "         [0.4561, 0.2180, 0.1553, 0.1705]],\n",
      "\n",
      "        [[0.2135, 0.1758, 0.2308, 0.3799],\n",
      "         [0.2334, 0.3227, 0.3095, 0.1344]],\n",
      "\n",
      "        [[0.2804, 0.2283, 0.1684, 0.3230],\n",
      "         [0.2257, 0.3350, 0.2421, 0.1973]],\n",
      "\n",
      "        [[0.2436, 0.3195, 0.2161, 0.2208],\n",
      "         [0.2504, 0.3193, 0.2318, 0.1985]],\n",
      "\n",
      "        [[0.2477, 0.3147, 0.2791, 0.1585],\n",
      "         [0.2005, 0.2487, 0.3062, 0.2446]],\n",
      "\n",
      "        [[0.3077, 0.2565, 0.1708, 0.2650],\n",
      "         [0.2777, 0.3868, 0.2065, 0.1289]],\n",
      "\n",
      "        [[0.2563, 0.3696, 0.1865, 0.1876],\n",
      "         [0.1623, 0.3090, 0.3417, 0.1870]],\n",
      "\n",
      "        [[0.3247, 0.1906, 0.2140, 0.2707],\n",
      "         [0.2957, 0.3998, 0.1339, 0.1705]],\n",
      "\n",
      "        [[0.2170, 0.4442, 0.2122, 0.1266],\n",
      "         [0.2805, 0.2704, 0.2489, 0.2002]],\n",
      "\n",
      "        [[0.2055, 0.2943, 0.2356, 0.2647],\n",
      "         [0.3264, 0.2164, 0.2762, 0.1810]],\n",
      "\n",
      "        [[0.2413, 0.2134, 0.2408, 0.3045],\n",
      "         [0.2810, 0.2090, 0.3306, 0.1793]],\n",
      "\n",
      "        [[0.2929, 0.2336, 0.2366, 0.2369],\n",
      "         [0.2266, 0.2626, 0.2587, 0.2522]],\n",
      "\n",
      "        [[0.1725, 0.3463, 0.2046, 0.2765],\n",
      "         [0.2265, 0.3787, 0.2367, 0.1581]],\n",
      "\n",
      "        [[0.2922, 0.1840, 0.2081, 0.3158],\n",
      "         [0.2732, 0.1889, 0.3241, 0.2138]],\n",
      "\n",
      "        [[0.2925, 0.2315, 0.2488, 0.2273],\n",
      "         [0.2963, 0.2324, 0.2310, 0.2403]],\n",
      "\n",
      "        [[0.3013, 0.3631, 0.1444, 0.1912],\n",
      "         [0.3141, 0.2873, 0.1839, 0.2147]],\n",
      "\n",
      "        [[0.2342, 0.3049, 0.2473, 0.2136],\n",
      "         [0.1361, 0.2990, 0.3107, 0.2542]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[ 0.0351, -0.0879,  0.2382,  ...,  0.1855,  0.2268, -0.2367],\n",
      "         [-0.0116, -0.0502,  0.2317,  ...,  0.0787,  0.1302, -0.2969]],\n",
      "\n",
      "        [[-0.2037,  0.0704, -0.2191,  ..., -0.1085,  0.2326, -0.1142],\n",
      "         [-0.1454, -0.0062, -0.2202,  ..., -0.1580,  0.1815, -0.0971]],\n",
      "\n",
      "        [[-0.1486, -0.0955,  0.1855,  ...,  0.1970, -0.0726, -0.4176],\n",
      "         [-0.1030, -0.0987,  0.1370,  ...,  0.1478, -0.1028, -0.4048]],\n",
      "\n",
      "        [[-0.0762,  0.0258, -0.3240,  ..., -0.1953,  0.0532, -0.2073],\n",
      "         [-0.0949, -0.0322, -0.3647,  ..., -0.0446,  0.0800, -0.1855]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    n_q, n_k, n_v = 2, 4, 4\n",
    "    d_q_, d_k_, d_v_ = 128, 128, 64\n",
    "    batch = 4\n",
    "    \n",
    "    q = torch.randn(batch, n_q, d_q_)\n",
    "    k = torch.randn(batch, n_k, d_k_)\n",
    "    v = torch.randn(batch, n_v, d_v_)\n",
    "    mask = torch.zeros(batch, n_q, n_k).bool()\n",
    "    \n",
    "    mha = MultiHeadAttention(n_head=8, d_k_=128, d_v_=64, d_k=256, d_v=128, d_o=128)\n",
    "    attn, output = mha(q, k, v, mask=mask)\n",
    "    \n",
    "    print(attn.shape)\n",
    "    print(output.shape)\n",
    "    print(attn)\n",
    "    print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自注意力机制 (SelfAttention)\n",
    "\n",
    "<img src=\"./images/Decoder2.png\" alt=\"示例图片\" width=\"900\">\n",
    "\n",
    "Self-Attention, 和 Attention 类似, 他们都是一种注意力机制. 不同的是 Attention 是 source 对 target, 输入的 source 和输出的 target 内容不同. 例如英译中, 输入英文, 输出中文. 而 Self-Attention 是 source 对 source, 是 source 内部元素之间或者 target 内部元素之间发生的 Attention 机制, 也可以理解为 Target = Source 这种特殊情况下的注意力机制."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\" Self-Attention \"\"\"\n",
    "    def __init__(self, n_head, d_k, d_v, d_x, d_o):\n",
    "        super(SelfAttention, self).__init__()  # 调用父类的 __init__ 方法，必须放在参数初始化之前\n",
    "        self.wq = nn.Parameter(torch.Tensor(d_x, d_k))\n",
    "        self.wk = nn.Parameter(torch.Tensor(d_x, d_k))\n",
    "        self.wv = nn.Parameter(torch.Tensor(d_x, d_v))\n",
    "        \n",
    "        self.mha = MultiHeadAttention(n_head=n_head, d_k_=d_k, d_v_=d_v, d_k=d_k, d_v=d_v, d_o=d_o)\n",
    "        \n",
    "        self.init_parameters()\n",
    "        \n",
    "    def init_parameters(self):\n",
    "        # self.parameters() 返回的是模型中所有可训练参数的迭代器，其中每个参数（例如权重和偏置）都是一个 Tensor。\n",
    "        for param in self.parameters():\n",
    "            # 计算初始化标准差，通常使用参数的输入维度\n",
    "            # 单独一层的 param 的 shape 为: (out_features, in_features)\n",
    "            stdv = 1. / np.power(param.size(-1), 0.5)\n",
    "            # 使用均匀分布在 [-stdv, stdv] 范围内初始化参数\n",
    "            param.data.uniform_(-stdv, stdv)\n",
    "    def forward(self, x, mask=None):\n",
    "        q = torch.matmul(x, self.wq)\n",
    "        k = torch.matmul(x, self.wk)\n",
    "        v = torch.matmul(x, self.wv)\n",
    "        \n",
    "        attn, output = self.mha(q, k, v, mask=mask)\n",
    "        # attn: [b*h, l, l]\n",
    "        # output: [b, l, d_o]\n",
    "        \n",
    "        return attn, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 4])\n",
      "torch.Size([4, 4, 80])\n",
      "tensor([[[0.2377, 0.2362, 0.2598, 0.2663],\n",
      "         [0.2773, 0.2442, 0.2359, 0.2426],\n",
      "         [0.2324, 0.2502, 0.2457, 0.2717],\n",
      "         [0.2597, 0.2455, 0.2584, 0.2364]],\n",
      "\n",
      "        [[0.2499, 0.2387, 0.2506, 0.2608],\n",
      "         [0.2551, 0.2465, 0.2406, 0.2579],\n",
      "         [0.2815, 0.2339, 0.2320, 0.2527],\n",
      "         [0.2465, 0.2688, 0.2514, 0.2333]],\n",
      "\n",
      "        [[0.2485, 0.2578, 0.2538, 0.2398],\n",
      "         [0.2526, 0.2506, 0.2510, 0.2458],\n",
      "         [0.2360, 0.2568, 0.2537, 0.2534],\n",
      "         [0.2702, 0.2376, 0.2361, 0.2561]],\n",
      "\n",
      "        [[0.2391, 0.2506, 0.2865, 0.2238],\n",
      "         [0.2611, 0.2594, 0.2313, 0.2482],\n",
      "         [0.2439, 0.2736, 0.2417, 0.2408],\n",
      "         [0.2483, 0.2548, 0.2583, 0.2386]],\n",
      "\n",
      "        [[0.2126, 0.2802, 0.2462, 0.2610],\n",
      "         [0.2513, 0.2513, 0.2476, 0.2497],\n",
      "         [0.2218, 0.2782, 0.2158, 0.2842],\n",
      "         [0.2394, 0.2417, 0.2561, 0.2629]],\n",
      "\n",
      "        [[0.2739, 0.2509, 0.2433, 0.2319],\n",
      "         [0.2531, 0.2337, 0.2626, 0.2506],\n",
      "         [0.2532, 0.2464, 0.2657, 0.2346],\n",
      "         [0.2738, 0.2436, 0.2620, 0.2206]],\n",
      "\n",
      "        [[0.2400, 0.2663, 0.2596, 0.2341],\n",
      "         [0.2471, 0.2510, 0.2375, 0.2645],\n",
      "         [0.2412, 0.2557, 0.2667, 0.2365],\n",
      "         [0.2585, 0.2467, 0.2360, 0.2588]],\n",
      "\n",
      "        [[0.2994, 0.2221, 0.2363, 0.2422],\n",
      "         [0.2253, 0.2627, 0.2618, 0.2501],\n",
      "         [0.2364, 0.2498, 0.2770, 0.2369],\n",
      "         [0.2525, 0.2347, 0.2512, 0.2616]],\n",
      "\n",
      "        [[0.2437, 0.2644, 0.2588, 0.2331],\n",
      "         [0.2374, 0.2770, 0.2460, 0.2395],\n",
      "         [0.2531, 0.2437, 0.2657, 0.2375],\n",
      "         [0.2463, 0.2271, 0.2604, 0.2661]],\n",
      "\n",
      "        [[0.2261, 0.2740, 0.2474, 0.2524],\n",
      "         [0.2393, 0.2228, 0.2856, 0.2522],\n",
      "         [0.2583, 0.2552, 0.2361, 0.2504],\n",
      "         [0.2516, 0.2617, 0.2401, 0.2466]],\n",
      "\n",
      "        [[0.2289, 0.2410, 0.2878, 0.2423],\n",
      "         [0.2450, 0.2789, 0.2113, 0.2648],\n",
      "         [0.2648, 0.2345, 0.2609, 0.2398],\n",
      "         [0.2364, 0.2398, 0.2658, 0.2580]],\n",
      "\n",
      "        [[0.2284, 0.2507, 0.2687, 0.2522],\n",
      "         [0.2409, 0.2495, 0.2468, 0.2627],\n",
      "         [0.2353, 0.2535, 0.2412, 0.2699],\n",
      "         [0.2228, 0.2676, 0.2422, 0.2674]],\n",
      "\n",
      "        [[0.2592, 0.2478, 0.2445, 0.2485],\n",
      "         [0.2724, 0.2541, 0.2518, 0.2217],\n",
      "         [0.2330, 0.2524, 0.2625, 0.2522],\n",
      "         [0.2639, 0.2480, 0.2540, 0.2340]],\n",
      "\n",
      "        [[0.2651, 0.2270, 0.2695, 0.2385],\n",
      "         [0.2524, 0.2441, 0.2601, 0.2433],\n",
      "         [0.2411, 0.2402, 0.2338, 0.2849],\n",
      "         [0.2400, 0.2762, 0.2589, 0.2249]],\n",
      "\n",
      "        [[0.2527, 0.2585, 0.2285, 0.2603],\n",
      "         [0.2612, 0.2462, 0.2293, 0.2632],\n",
      "         [0.2426, 0.2592, 0.2505, 0.2477],\n",
      "         [0.2324, 0.2638, 0.2730, 0.2308]],\n",
      "\n",
      "        [[0.2273, 0.2600, 0.2775, 0.2352],\n",
      "         [0.2811, 0.2362, 0.2452, 0.2376],\n",
      "         [0.2824, 0.2436, 0.2421, 0.2319],\n",
      "         [0.2456, 0.2096, 0.2285, 0.3163]],\n",
      "\n",
      "        [[0.2563, 0.2860, 0.2503, 0.2074],\n",
      "         [0.2677, 0.2645, 0.2458, 0.2219],\n",
      "         [0.2360, 0.2625, 0.2650, 0.2365],\n",
      "         [0.2586, 0.2571, 0.2568, 0.2275]],\n",
      "\n",
      "        [[0.2532, 0.2347, 0.2677, 0.2444],\n",
      "         [0.2805, 0.2206, 0.2720, 0.2270],\n",
      "         [0.2369, 0.2518, 0.2445, 0.2668],\n",
      "         [0.2446, 0.2610, 0.2534, 0.2410]],\n",
      "\n",
      "        [[0.2435, 0.2724, 0.2479, 0.2361],\n",
      "         [0.2282, 0.2509, 0.2489, 0.2719],\n",
      "         [0.2564, 0.2505, 0.2571, 0.2360],\n",
      "         [0.2577, 0.2633, 0.2389, 0.2401]],\n",
      "\n",
      "        [[0.2551, 0.2453, 0.2734, 0.2262],\n",
      "         [0.2546, 0.2512, 0.2503, 0.2439],\n",
      "         [0.2499, 0.2499, 0.2487, 0.2515],\n",
      "         [0.2329, 0.2690, 0.2434, 0.2547]],\n",
      "\n",
      "        [[0.2350, 0.2559, 0.2416, 0.2674],\n",
      "         [0.2638, 0.2499, 0.2513, 0.2350],\n",
      "         [0.2288, 0.2564, 0.2294, 0.2853],\n",
      "         [0.2381, 0.2368, 0.2544, 0.2707]],\n",
      "\n",
      "        [[0.2738, 0.2326, 0.2579, 0.2357],\n",
      "         [0.2710, 0.2426, 0.2713, 0.2151],\n",
      "         [0.2521, 0.2597, 0.2419, 0.2463],\n",
      "         [0.2288, 0.2344, 0.2668, 0.2701]],\n",
      "\n",
      "        [[0.2569, 0.2665, 0.2434, 0.2332],\n",
      "         [0.2447, 0.2488, 0.2627, 0.2439],\n",
      "         [0.2533, 0.2359, 0.2739, 0.2369],\n",
      "         [0.2357, 0.2941, 0.2259, 0.2443]],\n",
      "\n",
      "        [[0.2555, 0.2400, 0.2786, 0.2259],\n",
      "         [0.2746, 0.2399, 0.2378, 0.2477],\n",
      "         [0.2564, 0.2663, 0.2522, 0.2251],\n",
      "         [0.2337, 0.2482, 0.2685, 0.2496]],\n",
      "\n",
      "        [[0.2777, 0.2446, 0.2366, 0.2411],\n",
      "         [0.2181, 0.2585, 0.2564, 0.2670],\n",
      "         [0.2394, 0.2501, 0.2578, 0.2526],\n",
      "         [0.2372, 0.2491, 0.2347, 0.2791]],\n",
      "\n",
      "        [[0.2348, 0.2771, 0.2472, 0.2409],\n",
      "         [0.2495, 0.2638, 0.2527, 0.2340],\n",
      "         [0.2448, 0.2545, 0.2542, 0.2465],\n",
      "         [0.2455, 0.2399, 0.2590, 0.2556]],\n",
      "\n",
      "        [[0.2534, 0.2393, 0.2655, 0.2417],\n",
      "         [0.2907, 0.2330, 0.2386, 0.2377],\n",
      "         [0.2429, 0.2570, 0.2617, 0.2384],\n",
      "         [0.2359, 0.2756, 0.2421, 0.2464]],\n",
      "\n",
      "        [[0.2333, 0.2410, 0.2652, 0.2604],\n",
      "         [0.2826, 0.2568, 0.2363, 0.2243],\n",
      "         [0.2576, 0.2430, 0.2408, 0.2586],\n",
      "         [0.2332, 0.2656, 0.2493, 0.2519]],\n",
      "\n",
      "        [[0.2825, 0.2400, 0.2516, 0.2259],\n",
      "         [0.2654, 0.2527, 0.2349, 0.2470],\n",
      "         [0.2525, 0.2535, 0.2631, 0.2310],\n",
      "         [0.2453, 0.2572, 0.2357, 0.2617]],\n",
      "\n",
      "        [[0.2481, 0.2423, 0.2438, 0.2658],\n",
      "         [0.2700, 0.2537, 0.2324, 0.2439],\n",
      "         [0.2645, 0.2726, 0.2285, 0.2344],\n",
      "         [0.2513, 0.2365, 0.2502, 0.2619]],\n",
      "\n",
      "        [[0.2553, 0.2260, 0.2635, 0.2552],\n",
      "         [0.2531, 0.2645, 0.2147, 0.2677],\n",
      "         [0.2528, 0.2607, 0.2432, 0.2432],\n",
      "         [0.2207, 0.2566, 0.2659, 0.2568]],\n",
      "\n",
      "        [[0.2300, 0.2633, 0.2483, 0.2584],\n",
      "         [0.2538, 0.2483, 0.2246, 0.2733],\n",
      "         [0.2506, 0.2551, 0.2182, 0.2761],\n",
      "         [0.2443, 0.2664, 0.2553, 0.2340]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[-1.1637e-01, -9.1149e-02, -1.2017e-01,  ...,  1.3139e-01,\n",
      "           1.8121e-01, -7.6232e-02],\n",
      "         [-1.1514e-01, -9.8148e-02, -1.0956e-01,  ...,  1.3173e-01,\n",
      "           1.8830e-01, -7.4475e-02],\n",
      "         [-1.1698e-01, -9.3333e-02, -1.1788e-01,  ...,  1.3657e-01,\n",
      "           1.8778e-01, -8.0077e-02],\n",
      "         [-1.1687e-01, -1.0232e-01, -1.0038e-01,  ...,  1.2846e-01,\n",
      "           1.9465e-01, -6.6438e-02]],\n",
      "\n",
      "        [[-8.2190e-02, -4.3746e-02, -2.0867e-01,  ...,  9.4334e-02,\n",
      "           1.9738e-01,  1.2694e-01],\n",
      "         [-7.7608e-02, -5.3325e-02, -2.0287e-01,  ...,  9.2061e-02,\n",
      "           1.9330e-01,  1.2494e-01],\n",
      "         [-8.5073e-02, -5.3494e-02, -1.9648e-01,  ...,  1.0273e-01,\n",
      "           2.0886e-01,  1.3210e-01],\n",
      "         [-8.4695e-02, -4.4681e-02, -2.0219e-01,  ...,  1.0301e-01,\n",
      "           1.9862e-01,  1.3289e-01]],\n",
      "\n",
      "        [[-3.3686e-01, -3.5920e-02, -5.0474e-02,  ..., -1.7035e-02,\n",
      "           7.8037e-02,  3.2323e-03],\n",
      "         [-3.3489e-01, -4.1231e-02, -3.1285e-02,  ..., -1.3879e-02,\n",
      "           8.8003e-02,  1.1729e-02],\n",
      "         [-3.3398e-01, -3.1641e-02, -4.3104e-02,  ..., -1.3259e-02,\n",
      "           8.6488e-02, -2.5015e-03],\n",
      "         [-3.2820e-01, -4.3230e-02, -6.5620e-02,  ..., -1.0373e-02,\n",
      "           8.5173e-02, -2.4042e-04]],\n",
      "\n",
      "        [[ 1.0342e-01, -1.1600e-01, -3.0584e-01,  ..., -1.0302e-01,\n",
      "           4.0957e-02,  2.7823e-01],\n",
      "         [ 1.0365e-01, -1.2341e-01, -3.0714e-01,  ..., -1.1049e-01,\n",
      "           5.8641e-02,  2.9032e-01],\n",
      "         [ 1.0498e-01, -1.2457e-01, -3.0686e-01,  ..., -1.1032e-01,\n",
      "           5.0129e-02,  2.8550e-01],\n",
      "         [ 1.2196e-01, -1.3098e-01, -3.2896e-01,  ..., -1.2297e-01,\n",
      "           4.2357e-02,  2.6327e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    n_x = 4\n",
    "    d_x = 80\n",
    "    batch = 4\n",
    "    x = torch.randn(batch, n_x, d_x)\n",
    "    mask = torch.zeros(batch, n_x, n_x).bool()\n",
    "    \n",
    "    selfattn = SelfAttention(n_head=8, d_k=128, d_v=64, d_x=80, d_o=80)\n",
    "    attn, output = selfattn(x, mask=mask)\n",
    "    print(attn.shape)\n",
    "    print(output.shape)\n",
    "    print(attn)\n",
    "    print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KV Cache\n",
    "\n",
    "<img src=\"./images/KVCache.png\" alt=\"示例图片\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPT2Attention(nn.Module):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_masl: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n",
    "        if encoder_hidden_states is not None:\n",
    "            if not hasattr(self, \"q_attn\"):\n",
    "                raise ValueError(\n",
    "                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n",
    "                    \"Plwase make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n",
    "                )\n",
    "            query = self.q_attn(hidden_states)\n",
    "            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n",
    "            attention_mask = encoder_attention_mask\n",
    "        \n",
    "        else:\n",
    "            query, key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n",
    "            \n",
    "            query = self._split_heads(query, self.num_heads, self.head_dim)\n",
    "            key = self._split_heads(key, self.num_heads, self.head_dim)\n",
    "            value = self._split_heads(value, self.num_heads, self.head_dim)\n",
    "            \n",
    "            # 过去所存的值\n",
    "            if layer_past is not None:\n",
    "                past_key, past_value = layer_past\n",
    "                key = torch.cat((past_key, key), dim=-2) # 把当前新的 key 加入\n",
    "                value = torch.cat((past_value. value), dim=-1) # 把当前新的 value 加入\n",
    "                \n",
    "            if use_cache is True:\n",
    "                present = (key, value) # 输出用于保存\n",
    "            else:\n",
    "                present = None\n",
    "            \n",
    "            if self.reorder_and_upcast_attn:\n",
    "                attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n",
    "            else:\n",
    "                attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
    "            \n",
    "            attn_output = self.merge_heads(attn_output, self.num_heads, self.head_dim)\n",
    "            attn_output = self.c_proj(attn_output)\n",
    "            attn_output = self.resid_dropout(attn_output)\n",
    "            \n",
    "            outputs = (attn_output, present)\n",
    "            if output_attentions:\n",
    "                outputs += (attn_weights, )\n",
    "            \n",
    "        return outputs # a, present, (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_KVCache(nn.Module):\n",
    "    \"\"\" Self-Attention \"\"\"\n",
    "    def __init__(self, n_head, d_k, d_v, d_x, d_o):\n",
    "        super(SelfAttention_KVCache, self).__init__()  # 调用父类的 __init__ 方法，必须放在参数初始化之前\n",
    "        self.wq = nn.Parameter(torch.Tensor(d_x, d_k))\n",
    "        self.wk = nn.Parameter(torch.Tensor(d_x, d_k))\n",
    "        self.wv = nn.Parameter(torch.Tensor(d_x, d_v))\n",
    "        \n",
    "        self.mha = MultiHeadAttention(n_head=n_head, d_k_=d_k, d_v_=d_v, d_k=d_k, d_v=d_v, d_o=d_o)\n",
    "        \n",
    "        self.init_parameters()\n",
    "        \n",
    "        # KV cache to store key and value during autoregressive generation\n",
    "        self.cache_k = None\n",
    "        self.cache_v = None\n",
    "        \n",
    "    def init_parameters(self):\n",
    "        # self.parameters() 返回的是模型中所有可训练参数的迭代器，其中每个参数（例如权重和偏置）都是一个 Tensor。\n",
    "        for param in self.parameters():\n",
    "            # 计算初始化标准差，通常使用参数的输入维度\n",
    "            # 单独一层的 param 的 shape 为: (out_features, in_features)\n",
    "            stdv = 1. / np.power(param.size(-1), 0.5)\n",
    "            # 使用均匀分布在 [-stdv, stdv] 范围内初始化参数\n",
    "            param.data.uniform_(-stdv, stdv)\n",
    "    def forward(self, x, mask=None):\n",
    "        \n",
    "        q = torch.matmul(x, self.wq)\n",
    "        k = torch.matmul(x, self.wk)\n",
    "        v = torch.matmul(x, self.wv)\n",
    "        \n",
    "        attn, output = self.mha(q, k, v, mask=mask)\n",
    "        # attn: [b*h, l, l]\n",
    "        # output: [b, l, d_o]\n",
    "        \n",
    "        return attn, output\n",
    "    def forward(self, x, mask=None, use_cache=False):\n",
    "        \n",
    "        if use_cache:\n",
    "            \n",
    "            # Step 1: Initialize q, k, v\n",
    "            q = torch.matmul(x[:, -1:, :], self.wq)  # Query tensor for the last token only [batch_size, 1, d_k]\n",
    "            k = torch.matmul(x[:, -1:, :], self.wk)  # Key tensor for all previous tokens [batch_size, seq_len-1, d_k]\n",
    "            v = torch.matmul(x[:, -1:, :], self.wv)  # Value tensor for all previous tokens [batch_size, seq_len-1, d_v]\n",
    "            \n",
    "            # Step 2: Caching mechanism for autoregressive generation\n",
    "            if self.cache_k is None:  # First call, initialize cache\n",
    "                self.cache_k = k\n",
    "                self.cache_v = v\n",
    "                \n",
    "            else:  # Subsequent calls, concatenate new keys and values to cache\n",
    "                self.cache_k = torch.cat([self.cache_k, k], dim=1)  # Append new keys\n",
    "                self.cache_v = torch.cat([self.cache_v, v], dim=1)  # Append new values\n",
    "            \n",
    "            k, v = self.cache_k, self.cache_v  # Use the cached keys and values\n",
    "        \n",
    "        else:\n",
    "            q = torch.matmul(x, self.wq)\n",
    "            k = torch.matmul(x, self.wk)\n",
    "            v = torch.matmul(x, self.wv)\n",
    "\n",
    "        # Step 3: Multi-Head Attention\n",
    "        attn, output = self.mha(q, k, v, mask=mask)  # Attention output\n",
    "        # attn: [b*h, l, l]\n",
    "        # output: [b, l, d_o]\n",
    "        \n",
    "        # Output: [batch_size, seq_len, d_o]\n",
    "        return attn, output, self.cache_k, self.cache_v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No KV Cache\n",
      "Attention Shape: torch.Size([4, 5, 5]), Output Shape: torch.Size([2, 5, 16])\n",
      "\n",
      "\n",
      "KV Cache\n",
      "Step 1: Attention Shape: torch.Size([4, 1, 1]), Output Shape: torch.Size([2, 1, 16])\n",
      "Step 2: Attention Shape: torch.Size([4, 1, 2]), Output Shape: torch.Size([2, 1, 16])\n",
      "Step 3: Attention Shape: torch.Size([4, 1, 3]), Output Shape: torch.Size([2, 1, 16])\n",
      "Step 4: Attention Shape: torch.Size([4, 1, 4]), Output Shape: torch.Size([2, 1, 16])\n",
      "Step 5: Attention Shape: torch.Size([4, 1, 5]), Output Shape: torch.Size([2, 1, 16])\n",
      "因为每次只查询一个 token, 所以 attention 和 output 的 dim=1 一直等于1\n",
      "\n",
      "Step 1: K Cache Shape: torch.Size([2, 1, 8]), V Cache Shape: torch.Size([2, 1, 8])\n",
      "Step 2: K Cache Shape: torch.Size([2, 2, 8]), V Cache Shape: torch.Size([2, 2, 8])\n",
      "Step 3: K Cache Shape: torch.Size([2, 3, 8]), V Cache Shape: torch.Size([2, 3, 8])\n",
      "Step 4: K Cache Shape: torch.Size([2, 4, 8]), V Cache Shape: torch.Size([2, 4, 8])\n",
      "Step 5: K Cache Shape: torch.Size([2, 5, 8]), V Cache Shape: torch.Size([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 创建一个简单的输入张量\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    d_x = 16  # 输入维度\n",
    "    d_k = 8   # Key 和 Query 维度\n",
    "    d_v = 8   # Value 维度\n",
    "    d_o = 16  # 输出维度\n",
    "    n_head = 2  # 头数\n",
    "\n",
    "    # 输入的随机张量 [batch_size, seq_len, d_x]\n",
    "    x = torch.randn(batch_size, seq_len, d_x)\n",
    "\n",
    "    # 定义模型\n",
    "    model = SelfAttention_KVCache(n_head=n_head, d_k=d_k, d_v=d_v, d_x=d_x, d_o=d_o)\n",
    "\n",
    "    # 测试是否能够正常计算输出\n",
    "    mask = None  # 可以在生成时定义一个mask\n",
    "    \n",
    "    print(\"No KV Cache\")\n",
    "    use_cache = False\n",
    "    attn, output, _, _ = model(x, mask=mask, use_cache=use_cache)\n",
    "    print(f\"Attention Shape: {attn.shape}, Output Shape: {output.shape}\")\n",
    "\n",
    "    print(\"\\n\\nKV Cache\")\n",
    "    # 测试自回归生成：接下来使用缓存进行推理\n",
    "    use_cache = True  # 启用缓存\n",
    "\n",
    "    # 模拟自回归生成过程\n",
    "    size = []\n",
    "    for i in range(1, seq_len + 1):  # 假设逐步生成\n",
    "        partial_x = x[:, :i, :]  # 只使用前i个token\n",
    "        attn, output, k_cache, v_cache = model(partial_x, mask=mask, use_cache=use_cache)\n",
    "        print(f\"Step {i}: Attention Shape: {attn.shape}, Output Shape: {output.shape}\")\n",
    "        size.append(f\"Step {i}: K Cache Shape: {k_cache.shape}, V Cache Shape: {v_cache.shape}\")\n",
    "    # 模拟自回归生成过程\n",
    "    print(\"因为每次只查询一个 token, 所以 attention 和 output 的 dim=1 一直等于1\\n\")\n",
    "    for i in range(1, seq_len + 1):  # 假设逐步生成\n",
    "        print(size[i-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多队列注意力机制 (MQA)\n",
    "\n",
    "<img src=\"./images/MQA.png\" alt=\"示例图片\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_multihead_dot_product_attention(query, key, value, n_heads, softmax_scale=None, attn_bias=None, key_padding_mask=None, is_causal=False, dropout_p=0.0, training=False, needs_weights=False, multiquery=False):\n",
    "    q = rearrange(query, 'b s (h d) -> b h s d', h=n_heads)\n",
    "    k = rearrange(key, 'b s (h d) -> b h d s', h=1 if multiquery else n_heads)\n",
    "    v = rearrange(value, 'b s (h d) -> b h s d', h=1 if multiquery else n_heads)\n",
    "    min_val = torch.finfo(q.dtype).min\n",
    "    (b, _, s_q, d) = q.shape\n",
    "    s_k = k.size(-1)\n",
    "    if softmax_scale is None:\n",
    "        softmax_scale = 1 / math.sqrt(d)\n",
    "    attn_weight = q.matmul(k) * softmax_scale\n",
    "    if attn_bias is not None:\n",
    "        if attn_bias.size(-1) != 1 and attn_bias.size(-1) != s_k or (attn_bias.size(-2) != 1 and attn_bias.size(-2) != s_q):\n",
    "            raise RuntimeError(f'attn_bias (shape: {attn_bias.shape}) is expected to broadcast to shape: {attn_weight.shape}.')\n",
    "        attn_weight = attn_weight + attn_bias\n",
    "    if key_padding_mask is not None:\n",
    "        if attn_bias is not None:\n",
    "            warnings.warn('Propogating key_padding_mask to the attention module ' + 'and applying it within the attention module can cause ' + 'unneccessary computation/memory usage. Consider integrating ' + 'into attn_bias once and passing that to each attention ' + 'module instead.')\n",
    "        attn_weight = attn_weight.masked_fill(~key_padding_mask.view((b, 1, 1, s_k)), min_val)\n",
    "    if is_causal:\n",
    "        s = max(s_q, s_k)\n",
    "        causal_mask = attn_weight.new_ones(s, s, dtype=torch.float16)\n",
    "        causal_mask = causal_mask.tril()\n",
    "        causal_mask = causal_mask.to(torch.bool)\n",
    "        causal_mask = ~causal_mask\n",
    "        causal_mask = causal_mask[-s_q:, -s_k:]\n",
    "        attn_weight = attn_weight.masked_fill(causal_mask.view(1, 1, s_q, s_k), min_val)\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    if dropout_p:\n",
    "        attn_weight = torch.nn.functional.dropout(attn_weight, p=dropout_p, training=training, inplace=True)\n",
    "    out = attn_weight.matmul(v)\n",
    "    out = rearrange(out, 'b h s d -> b s (h d)')\n",
    "    if needs_weights:\n",
    "        return (out, attn_weight)\n",
    "    return (out, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    '''\n",
    "    Multi-Query self attention\n",
    "    \n",
    "    Using torch or triton attention implementation enables user to also use additive bias.\n",
    "    使用 Torch 或 Triton 的注意力实现可以让用户同时使用加性偏置(additive bias)。\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        attn_impl: str = 'triton', # 注意力实现的方式，可以是 'triton' 或其他实现方式。这会决定使用哪个后端（比如 Triton 后端加速）。\n",
    "        clip_qkv: Optional[float] = None, # 可选的值，表示在计算 Q、K、V 时对结果进行裁剪。裁剪的目的是防止数值过大。\n",
    "        qk_ln: bool = False, # 一个布尔值，表示是否对查询（Q）和键（K）进行层归一化（LayerNorm）。\n",
    "        softmax_scale: Optional[float] = None,\n",
    "        attn_pdrop: float = 0.0,\n",
    "        low_precision_layernorm: bool = False, # 是否使用低精度的层归一化\n",
    "        verbose: int = 0, # 调试时是否输出详细信息。\n",
    "        device: Optional[str] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attn_impl = attn_impl\n",
    "        self.clip_qkv = clip_qkv\n",
    "        self.qk_ln = qk_ln\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.softmax_scale = softmax_scale\n",
    "        if self.softmax_scale is None:\n",
    "            self.softmax_scale = 1 / math.sqrt(self.head_dim)\n",
    "        self.attn_dropout_p = attn_pdrop\n",
    "        \n",
    "        # 这个线性层用于从输入 x 中计算出 Q、K 和 V。\n",
    "        # 它的输出维度是 d_model + 2 * head_dim，这意味着它一次性计算所有的 Q、K 和 V 值。\n",
    "        self.Wqkv = nn.Linear(\n",
    "            d_model,\n",
    "            d_model + 2 * self.head_dim,\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "        # 是用于在某些特定硬件（例如，使用加速库如 Triton 或 CUDA）上对模型的参数进行优化的一个技巧。\n",
    "        # 具体来说，这段代码的目的是 启用 Wqkv 线性层的权重融合，以加速计算。\n",
    "        # _fused 是一个非标准的属性，用于标记 Wqkv 权重的某些优化设置，指示如何在硬件层面进行 权重融合。\n",
    "        # (0, fuse_splits) 表示的是权重矩阵在某个维度上进行分割的方式。\n",
    "        # fuse_splits 会告诉系统如何将 Q、K、V 的权重矩阵分割成两个部分，这样可以利用硬件加速并行计算的能力。\n",
    "        # 使用 # type: ignore 来让类型检查器忽略对这一行代码的警告。\n",
    "        # # type: ignore 是一种 类型注释，用于告诉 类型检查器（如 mypy 或编辑器的静态类型检查器）忽略当前行的类型错误或警告。\n",
    "        # 它并不是传统意义上的注释，而是一种特定于类型检查的语法。\n",
    "        fuse_splits = (d_model, d_model + self.head_dim)\n",
    "        self.Wqkv._fused = (0, fuse_splits) # type: ignore\n",
    "        self.attn_fn = scaled_multihead_dot_product_attention\n",
    "        # 将注意力的结果映射到最终的输出空间。\n",
    "        self.out_proj = nn.Linear(self.d_model, self.d_model, device=device)\n",
    "        \n",
    "        # 实现残差连接的功能\n",
    "        self.out_proj._is_residual = True # type: ignore\n",
    "        \n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x, # [batch_size, seq_len, d_model]\n",
    "        past_key_value=None, # 上一次计算的 key 和 value，用于加速推理时的自回归过程（缓存之前的注意力结果）\n",
    "        attn_bias=None, # 可选的注意力偏置（用于掩蔽等）\n",
    "        attention_mask=None,\n",
    "        is_causal=True, # 是否使用因果掩蔽（自回归生成时使用），即防止当前位置关注到未来的信息。\n",
    "        needs_weoghts=False, # 是否需要返回注意力权重.\n",
    "    ):\n",
    "        \n",
    "        # Wqkv 将输入 x 映射到 Q、K 和 V 的合并空间，输出的形状是 [batch_size, seq_len, d_model + 2 * head_dim]\n",
    "        qkv = self.Wqkv(x) # (1, 512, 960)\n",
    "        \n",
    "        # 限制了 Q、K、V 的值范围，以避免数值过大。\n",
    "        if self.clip_qkv:\n",
    "            qkv.clamp_(min=-self.clip_qkv, max=self.clip_qkv)\n",
    "        \n",
    "        # 将 qkv 张量按列（维度2）分割为查询、键和值。这些分割出的张量分别对应不同的维度。\n",
    "        # q -> [1, 512, 768]\n",
    "        # k -> [1, 512, 96]\n",
    "        # v -> [1, 512, 96]\n",
    "        # 8 heads\n",
    "        query, key, value =qkv.split([self.d_model, self.head_dim, self.head_dim], dim=2)\n",
    "        \n",
    "        # 自注意力（Self-Attention）机制中用来指定哪些位置应被遮蔽（mask）的一个张量，通常用于 避免对填充位置的注意力计算。\n",
    "        # key_padding_mask 主要用于处理变长序列和批处理时的填充（padding）问题，尤其是在处理不同长度的序列时。\n",
    "        key_padding_mask = attention_mask\n",
    "        \n",
    "        # 如果启用了层归一化（qk_ln 为 True），则对查询和键分别进行层归一化。\n",
    "        if self.qk_ln:\n",
    "            # Applying layernorm to qk\n",
    "            dtype = query.dtype\n",
    "            query = self.q_ln(query).to(dtype)\n",
    "            key = self.k_ln(key).to(dtype)\n",
    "        \n",
    "        context, attn_weights, past_key_value = self.attn_fn(\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            self.n_heads,\n",
    "            past_key_value=past_key_value,\n",
    "            softmax_scale=self.softmax_scale,\n",
    "            attn_bias=attn_bias,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            is_causal=is_causal,\n",
    "            dropout_p=self.attn_dropout_p,\n",
    "            training=self.training,\n",
    "            needs_weights=needs_weoghts,\n",
    "            multiquery=True,\n",
    "        )\n",
    "        return self.out_proj(context, attn_weights, past_key_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiQueryAttention(nn.Module):\n",
    "    \"\"\" Multi-Head Attention \"\"\"\n",
    "    def __init__(self, n_head, d_k_, d_v_, d_k, d_v, d_o):\n",
    "        '''\n",
    "        d_k_, d_v_: 这是输入Q, V矩阵的维度。\n",
    "        d_k: 每个头的查询和键的维度。\n",
    "        d_q_ = d_k_ ?\n",
    "        d_q = d_k ?\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.fc_q = nn.Linear(d_k_, n_head * d_k)\n",
    "        # self.fc_k = nn.Linear(d_k_, n_head * d_k)\n",
    "        # self.fc_v = nn.Linear(d_v_, n_head * d_v)\n",
    "        self.fc_k = nn.Linear(d_k_, d_k)\n",
    "        self.fc_v = nn.Linear(d_v_, d_v) # 共享一个头\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))\n",
    "        \n",
    "        self.fc_o = nn.Linear(n_head * d_v, d_o)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        n_head, d_q, d_k, d_v = self.n_head, self.d_k, self.d_k, self.d_v\n",
    "        batch, n_q, d_q_ = q.size()\n",
    "        batch, n_k, d_k_ = k.size()\n",
    "        batch, n_v, d_v_ = v.size()\n",
    "        \n",
    "        q = self.fc_q(q) # 1. 单头变多头\n",
    "        k = self.fc_k(k)\n",
    "        v = self.fc_v(v)\n",
    "        \n",
    "        # 重塑为多头的形状\n",
    "        # q: [b, l, h*d] -> [b, l, h, d] -> [h, b, l, d] -> [b*h, l, d]\n",
    "        q = q.view(batch, n_q, n_head, d_q).permute(2, 0, 1, 3).contiguous().view(-1, n_q, d_q)\n",
    "        # k, v: [b, l, d] -> [b, h, l, d] -> [b*h, l, d]\n",
    "        k = k.view(batch, 1, n_k, d_k).repeat(1, n_head, 1, 1).contiguous().view(-1, n_k, d_k) # 8 个头共享一个 k, v\n",
    "        v = v.view(batch, 1, n_v, d_v).repeat(1, n_head, 1, 1).contiguous().view(-1, n_v, d_v)\n",
    "        # a = k.view(-1, n_head, n_k, d_k)\n",
    "        # assert a[0, 0, 0, 0] == a[0, 7, 0, 0]\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1)\n",
    "        \n",
    "        attn, output = self.attention(q, k, v, mask=mask) # 2. 当成单头注意力求输出\n",
    "        # attn: [b*h, l_q, l_k]\n",
    "        \n",
    "        # output: [b*h, l_q, d_v] -> [b, h, l_q, d_v] -> [h, l_q, b, d_v] -> [b, l_q, h*d_v]\n",
    "        output = output.view(n_head, batch, n_q, d_v).permute(1, 2, 0, 3).contiguous().view(batch, n_q, -1) # 3. Concat\n",
    "        \n",
    "        # [..., h*d_v] -> [..., d_o]\n",
    "        output = self.fc_o(output) # 4. 仿射变换得到最终输出\n",
    "        \n",
    "        return attn, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2, 4])\n",
      "torch.Size([4, 2, 128])\n",
      "tensor([[[0.2673, 0.1969, 0.1328, 0.4030],\n",
      "         [0.2837, 0.2374, 0.2473, 0.2316]],\n",
      "\n",
      "        [[0.1549, 0.2106, 0.3558, 0.2787],\n",
      "         [0.1554, 0.3733, 0.3000, 0.1713]],\n",
      "\n",
      "        [[0.1432, 0.4309, 0.2799, 0.1459],\n",
      "         [0.1951, 0.3352, 0.2062, 0.2635]],\n",
      "\n",
      "        [[0.2496, 0.1815, 0.3300, 0.2390],\n",
      "         [0.2416, 0.2436, 0.1758, 0.3390]],\n",
      "\n",
      "        [[0.2967, 0.2253, 0.2807, 0.1973],\n",
      "         [0.2050, 0.2747, 0.2827, 0.2376]],\n",
      "\n",
      "        [[0.2191, 0.2262, 0.1897, 0.3651],\n",
      "         [0.1095, 0.2302, 0.2499, 0.4104]],\n",
      "\n",
      "        [[0.1709, 0.2289, 0.2501, 0.3501],\n",
      "         [0.1842, 0.1808, 0.2313, 0.4037]],\n",
      "\n",
      "        [[0.2063, 0.3573, 0.2533, 0.1831],\n",
      "         [0.2624, 0.1467, 0.3159, 0.2750]],\n",
      "\n",
      "        [[0.2486, 0.1880, 0.3166, 0.2467],\n",
      "         [0.2750, 0.2199, 0.2350, 0.2701]],\n",
      "\n",
      "        [[0.3261, 0.2748, 0.2305, 0.1686],\n",
      "         [0.2672, 0.2331, 0.2529, 0.2469]],\n",
      "\n",
      "        [[0.2777, 0.0860, 0.3919, 0.2444],\n",
      "         [0.2631, 0.1802, 0.1633, 0.3934]],\n",
      "\n",
      "        [[0.2068, 0.2088, 0.2403, 0.3440],\n",
      "         [0.2063, 0.2095, 0.1876, 0.3965]],\n",
      "\n",
      "        [[0.2410, 0.3274, 0.2788, 0.1528],\n",
      "         [0.3313, 0.2753, 0.1808, 0.2126]],\n",
      "\n",
      "        [[0.2933, 0.2586, 0.1781, 0.2699],\n",
      "         [0.2670, 0.2173, 0.2585, 0.2572]],\n",
      "\n",
      "        [[0.2296, 0.2035, 0.4231, 0.1438],\n",
      "         [0.3371, 0.1937, 0.2528, 0.2163]],\n",
      "\n",
      "        [[0.2219, 0.2691, 0.2795, 0.2294],\n",
      "         [0.2978, 0.2237, 0.1132, 0.3653]],\n",
      "\n",
      "        [[0.2491, 0.3101, 0.2594, 0.1813],\n",
      "         [0.3432, 0.3122, 0.1084, 0.2361]],\n",
      "\n",
      "        [[0.3271, 0.1900, 0.1819, 0.3010],\n",
      "         [0.2748, 0.2533, 0.1581, 0.3139]],\n",
      "\n",
      "        [[0.1970, 0.3226, 0.1569, 0.3235],\n",
      "         [0.1862, 0.2682, 0.1877, 0.3579]],\n",
      "\n",
      "        [[0.1480, 0.1837, 0.2731, 0.3952],\n",
      "         [0.2571, 0.1993, 0.2255, 0.3180]],\n",
      "\n",
      "        [[0.1934, 0.3151, 0.2378, 0.2537],\n",
      "         [0.1999, 0.2319, 0.2872, 0.2811]],\n",
      "\n",
      "        [[0.2726, 0.2471, 0.1830, 0.2973],\n",
      "         [0.1988, 0.3400, 0.2893, 0.1719]],\n",
      "\n",
      "        [[0.2781, 0.1723, 0.1973, 0.3523],\n",
      "         [0.1761, 0.2598, 0.3802, 0.1839]],\n",
      "\n",
      "        [[0.2504, 0.2682, 0.1540, 0.3275],\n",
      "         [0.3186, 0.4055, 0.1016, 0.1742]],\n",
      "\n",
      "        [[0.2690, 0.2167, 0.2633, 0.2510],\n",
      "         [0.2012, 0.2944, 0.1878, 0.3166]],\n",
      "\n",
      "        [[0.4243, 0.2167, 0.1357, 0.2233],\n",
      "         [0.2102, 0.2650, 0.2642, 0.2606]],\n",
      "\n",
      "        [[0.2514, 0.2273, 0.1800, 0.3412],\n",
      "         [0.3058, 0.3525, 0.1383, 0.2034]],\n",
      "\n",
      "        [[0.2295, 0.4372, 0.1843, 0.1491],\n",
      "         [0.3522, 0.2469, 0.2395, 0.1614]],\n",
      "\n",
      "        [[0.3644, 0.2860, 0.1221, 0.2276],\n",
      "         [0.2612, 0.1846, 0.1994, 0.3548]],\n",
      "\n",
      "        [[0.3229, 0.3426, 0.2039, 0.1306],\n",
      "         [0.3499, 0.2664, 0.1647, 0.2189]],\n",
      "\n",
      "        [[0.2697, 0.1226, 0.3285, 0.2792],\n",
      "         [0.3296, 0.2021, 0.2357, 0.2326]],\n",
      "\n",
      "        [[0.2491, 0.1981, 0.2691, 0.2836],\n",
      "         [0.2199, 0.3171, 0.2414, 0.2216]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[-0.1286,  0.1770,  0.1622,  ...,  0.1626,  0.0506,  0.0542],\n",
      "         [-0.1381,  0.1922,  0.1885,  ...,  0.1706,  0.0924,  0.0450]],\n",
      "\n",
      "        [[-0.1298,  0.1699,  0.1560,  ...,  0.2212,  0.0705, -0.0232],\n",
      "         [-0.1377,  0.1865,  0.1622,  ...,  0.2228,  0.0528,  0.0034]],\n",
      "\n",
      "        [[-0.2153,  0.2010,  0.2491,  ...,  0.1886,  0.1238,  0.0221],\n",
      "         [-0.2109,  0.1936,  0.2860,  ...,  0.2426,  0.0561, -0.0215]],\n",
      "\n",
      "        [[-0.2664,  0.1320,  0.1586,  ...,  0.1502,  0.1229,  0.0636],\n",
      "         [-0.1199,  0.1523,  0.1124,  ...,  0.2008,  0.0500, -0.0063]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    n_q, n_k, n_v = 2, 4, 4\n",
    "    d_q_, d_k_, d_v_ = 128, 128, 64\n",
    "    batch = 4\n",
    "    \n",
    "    q = torch.randn(batch, n_q, d_q_)\n",
    "    k = torch.randn(batch, n_k, d_k_)\n",
    "    v = torch.randn(batch, n_v, d_v_)\n",
    "    mask = torch.zeros(batch, n_q, n_k).bool()\n",
    "    \n",
    "    mha = MyMultiQueryAttention(n_head=8, d_k_=128, d_v_=64, d_k=256, d_v=128, d_o=128)\n",
    "    attn, output = mha(q, k, v, mask=mask)\n",
    "    \n",
    "    print(attn.shape)\n",
    "    print(output.shape)\n",
    "    print(attn)\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
