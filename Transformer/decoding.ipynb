{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from einops import rearrange\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贪心搜索 greedy search\n",
    "\n",
    "$$\n",
    "\\hat{y_t} = \\arg\\max_y P(y | \\hat{Y}_{<t}, X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(model, tokenizer, input_ids, max_tokens=300):\n",
    "    # 使用torch.inference_mode()上下文管理器来禁用梯度计算，这在推理时可以提高性能并减少内存使用。\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(max_tokens):\n",
    "            # 将当前的input_ids输入到模型中，获取模型的输出:\n",
    "            # logits：模型的原始输出，是一个三维张量，形状为(batch_size, sequence_length, vocab_size)，表示每个token在词汇表中的预测分数。\n",
    "            # last_hidden_state：这是模型的最后一层隐藏状态，通常用于进一步的任务，如命名实体识别或问答系统。\n",
    "            # lhidden_states：这是一个元组，包含了模型每一层的隐藏状态。这可以用于分析模型内部的表示或进行更复杂的下游任务。\n",
    "            # lattentions：对于带有注意力机制的模型（如Transformer），这个属性包含了每一层的注意力权重。这可以用于可视化和分析模型的注意力分布。\n",
    "            # lcross_attentions：在序列到序列的模型中，这个属性包含了编码器和解码器之间的交叉注意力权重。\n",
    "            # lloss：如果在训练过程中提供了标签，模型输出可能包含一个损失值，表示模型预测与真实标签之间的差异。\n",
    "            # lpast_key_values：在某些模型（如GPT-2）中，这个属性包含了过去的键值对，用于加速生成任务中的解码过程。\n",
    "            outputs = model(input_ids)\n",
    "            # 从模型输出中提取最后一个token的logits（未归一化的预测概率）。[:, -1, :]表示取最后一个时间步的所有token的logits\n",
    "            # 第一个维度: 批次（batch）维度: 表示选择这个维度上的所有元素，即选择整个批次。这意味着对批次中的每个样本都执行相同的操作。\n",
    "            # 第二个维度: 序列（sequence）维度: -1 表示选择这个维度上的最后一个元素。\n",
    "            # 第三个维度: 词汇表（vocabulary）维度: 表示选择这个维度上的所有元素，即选择所有可能的token的logits。\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            # 使用argmax函数在logits上找到概率最高的token ID，即预测的下一个token。\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "            # 检查预测的下一个token是否是结束符（eos_token_id）。如果是，则停止生成\n",
    "            if next_token == tokenizer.eos_token_id:\n",
    "                break\n",
    "            # 将预测的下一个token添加到input_ids的末尾\n",
    "            input_ids = torch.cat([input_ids, rearrange(next_token, 'c -> 1 c')], dim=-1)\n",
    "            # rearrange(next_token, 'c -> 1 c')用于将next_token的形状从(batch_size,)变为(batch_size, 1)，以便与input_ids拼接。\n",
    "            # 拆分：可以将一个维度拆分成多个维度。例如，rearrange(data, '(a b) -> a b', a=2, b=5) 将一个维度拆分成两个维度\n",
    "            # 合并：可以将多个维度合并成一个维度。例如，rearrange(data, 'a b -> (a b)') 将两个维度合并成一个维度\n",
    "        # 使用分词器将最终的 input_ids 序列解码成文本。\n",
    "        generated_text = tokenizer.decode(input_ids[0])\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Once upon a time in a distant land, the sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was\n"
     ]
    }
   ],
   "source": [
    "# Load a pretrained model and tokenizer (e.g., GPT-2)\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Provide an initial prompt\n",
    "prompt = \"Once upon a time in a distant land,\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate text using greedy decoding\n",
    "generated_text = greedy_decoding(model, tokenizer, input_ids)\n",
    "\n",
    "# Print the result\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集束搜索 beam search\n",
    "\n",
    "beam search 在贪心搜索上进一步扩大了搜索范围，贪心搜索每下一步只考虑当前最优的 top-1 结果，beam search 考虑最优的 top-k 个结果。\n",
    "\n",
    "<img src=\"./images/beam.png\" alt=\"示例图片\" width=\"1100\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(LM_prob, beam_size=3):\n",
    "    batch, seqlen, vocab_size = LM_prob.shape\n",
    "    # 对 LM_prob 取对数\n",
    "    log_LM_prob = LM_prob.log() # torch.Tensor().log()\n",
    "    # 先选择第 0 个位置的最大 beam_size 个token, log_emb_prob 与 indices 的 shape 为 (batch, beam)\n",
    "    log_beam_prob, indices = log_LM_prob[:, 0, :].topk(beam_size, sorted = True)\n",
    "    # torch.Tensor([1, 2, 3, 4, 5]).topk(size, sorted) \n",
    "    # -> return torch.return_types.topk(values=tensor([5., 4., 3.]),indices=tensor([4, 3, 2]))\n",
    "    # unsqueeze(): 在多维数组（张量）中添加单维度\n",
    "    indices = indices.unsqueeze(-1) # tensor([4, 3, 2]) -> tensor([[4], [3], [2]])\n",
    "    # indices: [batch_size, beam_size, 1], 记录 k-top 的索引\n",
    "    # 对每个长度进行beam_search\n",
    "    for i in range(1, seqlen):\n",
    "        # log_beam_prob: [batch, beam] -> [batch, beam, 1]\n",
    "        # [:, i, :] 选择了第 i 个时间步的所有元素，结果是一个形状为 [batch_size, vocab_size] 的二维张量\n",
    "        # log_LM_prob[:, i, :].unsqueeze(1): [batch, 1, vocab_size]\n",
    "        # repeat(1, beam_size, 1): 沿着第二维度重复 beam_size 次 -> [batch, beam, vocab_size]\n",
    "        # [batch, beam, 1] + [batch, beam, vocab_size]: 前面的在第三个维度上会广播\n",
    "        # 因为取了对数，所以概率是相加\n",
    "        log_beam_prob = log_beam_prob.unsqueeze(-1) + log_LM_prob[:, i, :].unsqueeze(1).repeat(1, beam_size, 1)\n",
    "        # -> log_beam_prob: [batch_size, beam_size, vocab_size]: 每个beam的可能产生的概率\n",
    "        \n",
    "        # 选出当前步（所有分叉: view(batch, -1)）概率最高的 k 个 token\n",
    "        log_beam_prob, index = log_beam_prob.view(batch, -1).topk(beam_size, sorted=True)\n",
    "        # log_beam_prob, index: [batch, beam], 喂给下一步\n",
    "        \n",
    "        # 下面的计算: beam_id 选出新 beam 来源于之前的那个 beam; index 代表真实的 token_id\n",
    "        beam_id = index // vocab_size # 从第几个 beam 来的, (见图)一共有 beam_size * vocab_size 个概率，整除即可\n",
    "        index = index % vocab_size\n",
    "        # beam_id, index: [batch, beam]\n",
    "        mid = torch.Tensor([])\n",
    "        # 对 batch 内每个样本循环，选出 beam 的同时拼接上新生成的 token_id\n",
    "        for j, bid, idx in zip(range(batch), beam_id, index):\n",
    "            # indices: [batch_size, beam_size, 1], 记录 k-top 的索引\n",
    "            # x 记录索引 chain\n",
    "            # indices[j][bid]: [1]\n",
    "            # idx: [] --unsqueeze(-1)--> [1]\n",
    "            # 拼接成链\n",
    "            x = torch.cat([indices[j][bid], idx.unsqueeze(-1)], -1) # [1] -> [链长(步长)]\n",
    "            # mid: [链的数量, 链长(步长)]\n",
    "            # x: -> [1, 链长(步长)]\n",
    "            # 在第 1 个维度拼接\n",
    "            mid = torch.cat([mid, x.unsqueeze(0)], 0)\n",
    "        indices = mid # [batch_size, beam_size, 链长(步长)]\n",
    "        \n",
    "    return indices, log_beam_prob\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 77., 315.,  56.,  ..., 589., 811., 440.],\n",
      "         [ 77., 315.,  56.,  ..., 589., 811., 440.],\n",
      "         [ 77., 315.,  56.,  ..., 589., 811., 440.]],\n",
      "\n",
      "        [[605., 414.,  77.,  ..., 366., 440., 105.],\n",
      "         [605., 414.,  77.,  ..., 366., 440., 105.],\n",
      "         [605., 414.,  77.,  ..., 366., 440., 105.]],\n",
      "\n",
      "        [[924., 764., 667.,  ..., 368., 694.,  79.],\n",
      "         [924., 764., 667.,  ..., 368., 694.,  79.],\n",
      "         [924., 764., 667.,  ..., 368., 694., 877.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[506., 389.,  27.,  ..., 204., 820.,  55.],\n",
      "         [506., 389.,  27.,  ..., 204., 820.,  55.],\n",
      "         [506., 389.,  27.,  ..., 204., 820.,  55.]],\n",
      "\n",
      "        [[355., 221.,  55.,  ..., 222., 162., 215.],\n",
      "         [355., 221., 364.,  ..., 222., 162., 215.],\n",
      "         [355., 221.,  55.,  ..., 222., 162., 215.]],\n",
      "\n",
      "        [[471., 790., 712.,  ..., 891., 474., 873.],\n",
      "         [471., 790., 712.,  ..., 891., 474., 873.],\n",
      "         [471., 790., 712.,  ..., 891., 474., 873.]]])\n",
      "tensor([[-83.5129, -83.5279, -83.5399],\n",
      "        [-83.5688, -83.5810, -83.5819],\n",
      "        [-81.7680, -81.7764, -81.8013],\n",
      "        [-81.6422, -81.6557, -81.6703],\n",
      "        [-82.7229, -82.7240, -82.7267],\n",
      "        [-85.8737, -85.8772, -85.8942],\n",
      "        [-84.9733, -84.9831, -84.9900],\n",
      "        [-87.0642, -87.0839, -87.0842],\n",
      "        [-82.8796, -82.9057, -82.9110],\n",
      "        [-83.4216, -83.4251, -83.4301],\n",
      "        [-84.5580, -84.5711, -84.5871],\n",
      "        [-85.2114, -85.2123, -85.2521],\n",
      "        [-82.4836, -82.5148, -82.5339],\n",
      "        [-84.4538, -84.4557, -84.4559],\n",
      "        [-83.0369, -83.0400, -83.0692],\n",
      "        [-83.7725, -83.8066, -83.8222],\n",
      "        [-83.3809, -83.4005, -83.4023],\n",
      "        [-81.1988, -81.2190, -81.2330],\n",
      "        [-83.5746, -83.5874, -83.6044],\n",
      "        [-78.5475, -78.5509, -78.5635],\n",
      "        [-80.8936, -80.9273, -80.9444],\n",
      "        [-84.0237, -84.0322, -84.0342],\n",
      "        [-82.4433, -82.4507, -82.4763],\n",
      "        [-83.4729, -83.4730, -83.4861],\n",
      "        [-85.5762, -85.5916, -85.5936],\n",
      "        [-83.4556, -83.4745, -83.4772],\n",
      "        [-81.6334, -81.6451, -81.6745],\n",
      "        [-83.1837, -83.2006, -83.2213],\n",
      "        [-85.0492, -85.0567, -85.0871],\n",
      "        [-85.5322, -85.5396, -85.5514],\n",
      "        [-86.4284, -86.4365, -86.4594],\n",
      "        [-79.6717, -79.6934, -79.7338]])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 建立一个语言模型 LM_prob(batch, seqlen, vocab_size)\n",
    "    LM_prob = F.softmax(torch.randn([32, 20, 1000]), dim=-1)\n",
    "    # 最终返回每个候选，以及每个候选的 log_prob, shape 为 [batch, beam_size, seqlen]\n",
    "    indices, log_prob = beam_search(LM_prob, beam_size=3)\n",
    "    print(indices)\n",
    "    print(log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 加载 GPT-2 模型和分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# 2. 准备输入数据\n",
    "input_text = \"The quick brown fox\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# 3. 获取模型的输出概率\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    LM_prob = torch.nn.functional.softmax(outputs.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam 1: \n",
      "-iees\n",
      "Beam 2: \n",
      " answeriees\n",
      "Beam 3: \n",
      " andiees\n"
     ]
    }
   ],
   "source": [
    "# 5. 调用 beam_search 函数\n",
    "beam_size = 3\n",
    "indices, log_beam_prob = beam_search(LM_prob, beam_size)\n",
    "\n",
    "# 6. 解码输出\n",
    "for i in range(beam_size):\n",
    "    decoded_text = tokenizer.decode(indices[0, i].long(), skip_special_tokens=True)\n",
    "    print(f\"Beam {i+1}: {decoded_text}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 温度 Temperature\n",
    "\n",
    "<img src=\"./images/temp.png\" alt=\"示例图片\" width=\"1100\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_sampling(logits, temperature=1.0):\n",
    "    logits = logits / temperature\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    # torch.multinomial 函数从给定的概率分布中抽取样本。它确保每个样本的抽取是独立的，并且每个类别的概率由输入张量指定。\n",
    "    sampled_token = torch.multinomial(probabilities, 1)\n",
    "    return sampled_token, torch.max(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 加载 GPT-2 模型和分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# 2. 准备输入数据\n",
    "input_text = \"The quick brown fox\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# 3. 获取模型的 logits 输出\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled token:  killed\t\t Prob: 0.12967140972614288\n"
     ]
    }
   ],
   "source": [
    "# 5. 调用 temperature_sampling 函数\n",
    "sampled_token, prob = temperature_sampling(logits[:, -1, :], temperature=1)\n",
    "\n",
    "# 6. 解码输出\n",
    "sampled_token_id = sampled_token.item()\n",
    "decoded_text = tokenizer.decode([sampled_token_id], skip_special_tokens=True)\n",
    "print(f\"Sampled token: {decoded_text}\\t\\t Prob: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High temperature:\n",
      "Sampled token: 's\t\t Prob: 0.12967140972614288\n",
      "Sampled token: es\t\t Prob: 0.12967140972614288\n",
      "Sampled token:  that\t\t Prob: 0.12967140972614288\n",
      "Sampled token:  didn\t\t Prob: 0.12967140972614288\n",
      "Sampled token:  or\t\t Prob: 0.12967140972614288\n",
      "Sampled token:  gut\t\t Prob: 0.12967140972614288\n",
      "Sampled token:  jumped\t\t Prob: 0.12967140972614288\n",
      "Sampled token:  is\t\t Prob: 0.12967140972614288\n",
      "Sampled token: 's\t\t Prob: 0.12967140972614288\n",
      "Sampled token:  jumped\t\t Prob: 0.12967140972614288\n",
      "\n",
      "\n",
      "Low temperature:\n",
      "Sampled token: es\t\t Prob: 0.9999668598175049\n",
      "Sampled token: es\t\t Prob: 0.9999668598175049\n",
      "Sampled token: es\t\t Prob: 0.9999668598175049\n",
      "Sampled token: es\t\t Prob: 0.9999668598175049\n",
      "Sampled token: es\t\t Prob: 0.9999668598175049\n",
      "Sampled token: es\t\t Prob: 0.9999668598175049\n",
      "Sampled token: es\t\t Prob: 0.9999668598175049\n",
      "Sampled token: es\t\t Prob: 0.9999668598175049\n",
      "Sampled token: es\t\t Prob: 0.9999668598175049\n",
      "Sampled token: es\t\t Prob: 0.9999668598175049\n",
      "\n",
      "\n",
      "Normal temperature:\n",
      "Sampled token: es\t\t Prob: 0.6768402457237244\n",
      "Sampled token: es\t\t Prob: 0.6768402457237244\n",
      "Sampled token: y\t\t Prob: 0.6768402457237244\n",
      "Sampled token: es\t\t Prob: 0.6768402457237244\n",
      "Sampled token: es\t\t Prob: 0.6768402457237244\n",
      "Sampled token: es\t\t Prob: 0.6768402457237244\n",
      "Sampled token: 's\t\t Prob: 0.6768402457237244\n",
      "Sampled token:  down\t\t Prob: 0.6768402457237244\n",
      "Sampled token: es\t\t Prob: 0.6768402457237244\n",
      "Sampled token: es\t\t Prob: 0.6768402457237244\n"
     ]
    }
   ],
   "source": [
    "print(\"High temperature:\")\n",
    "for i in range(10):\n",
    "    # 5. 调用 temperature_sampling 函数\n",
    "    sampled_token, prob = temperature_sampling(logits[:, -1, :], temperature=1)\n",
    "\n",
    "    # 6. 解码输出\n",
    "    sampled_token_id = sampled_token.item()\n",
    "    decoded_text = tokenizer.decode([sampled_token_id], skip_special_tokens=True)\n",
    "    print(f\"Sampled token: {decoded_text}\\t\\t Prob: {prob}\")\n",
    "\n",
    "print(\"\\n\\nLow temperature:\")\n",
    "for i in range(10):\n",
    "    # 5. 调用 temperature_sampling 函数\n",
    "    sampled_token, prob = temperature_sampling(logits[:, -1, :], temperature=0.1)\n",
    "\n",
    "    # 6. 解码输出\n",
    "    sampled_token_id = sampled_token.item()\n",
    "    decoded_text = tokenizer.decode([sampled_token_id], skip_special_tokens=True)\n",
    "    print(f\"Sampled token: {decoded_text}\\t\\t Prob: {prob}\")\n",
    "\n",
    "print(\"\\n\\nNormal temperature:\")\n",
    "for i in range(10):\n",
    "    # 5. 调用 temperature_sampling 函数\n",
    "    sampled_token, prob = temperature_sampling(logits[:, -1, :], temperature=0.5)\n",
    "\n",
    "    # 6. 解码输出\n",
    "    sampled_token_id = sampled_token.item()\n",
    "    decoded_text = tokenizer.decode([sampled_token_id], skip_special_tokens=True)\n",
    "    print(f\"Sampled token: {decoded_text}\\t\\t Prob: {prob}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-K\n",
    "\n",
    "在每个时间步选择条件概率排名前 K 的词语，然后在这 K 个词语中进行随机采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_K_sampling(model, tokenizer, input_ids, max_tokens=100, top_k=50, temperature=1):\n",
    "    for _ in range(max_tokens):\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            top_k_probs = F.softmax(top_k_logits / temperature, dim=-1)\n",
    "            next_token_index = torch.multinomial(top_k_probs, num_samples=1)\n",
    "            # torch.gather() 函数从输入张量中按照指定的索引提取元素，并将这些元素放置在输出张量的相应位置\n",
    "            next_token = top_k_indices.gather(-1, next_token_index)\n",
    "            if next_token == tokenizer.eos_token_id:\n",
    "                break\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "    generated_text = tokenizer.decode(input_ids[0])\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Generated Text:\n",
      "The quick brown foxes are a great way to get a little bit of a kick out of your dog.\n",
      "\n",
      "The quick brown foxes are a great way to get a little bit of a kick out of your dog. The quick brown foxes are a great way to get a little bit of a kick out of your dog. The quick brown foxes are a great way to get a little bit of a kick out of your dog. The quick brown foxes are a great way to get a little bit\n",
      "\n",
      "\n",
      " Top-10 Generated Text:\n",
      "The quick brown foxes are so much faster than the brown foxes, it is amazing that they can survive under the cover of the forest.\"\n",
      "\n",
      "\"You mean, that's what you said?\"\n",
      "\n",
      "\"The only thing you could see was the trees and the grass,\" said the foxes. \"That was just the way I saw the forest. I was just going to go see it and it didn't matter where.\"\n",
      "\n",
      "\"Then I'll give them a ride back to the castle.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 加载 GPT-2 模型和分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# 2. 准备输入数据\n",
    "input_text = \"The quick brown fox\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# 4. 调用 top_K_sampling 函数\n",
    "generated_text = top_K_sampling(model, tokenizer, input_ids, max_tokens=100, top_k=1, temperature=1)\n",
    "\n",
    "# 5. 打印生成的文本\n",
    "print(\"Top-1 Generated Text:\")\n",
    "print(generated_text)\n",
    "\n",
    "# 4. 调用 top_K_sampling 函数\n",
    "generated_text = top_K_sampling(model, tokenizer, input_ids, max_tokens=100, top_k=10, temperature=1)\n",
    "\n",
    "# 5. 打印生成的文本\n",
    "print(\"\\n\\n Top-10 Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-P (Nucleus)\n",
    "\n",
    "在每个时间步根据模型输出的概率分布选择概率累计超过给定阈值 p 的词语集和，然后在这个词语集合中进行随机采样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(model, tokenizer, input_ids, max_tokens=100, top_p=0.95, temperature=1):\n",
    "    for _ in range(max_tokens):\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[:, -1, :] # [batch. vocab]\n",
    "            # torch.sort(): 用于对输入张量进行排序。返回排序后的张量以及排序后元素的原始索引。\n",
    "            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "            sorted_probabilities = F.softmax(sorted_logits, dim=-1)\n",
    "            # torch.cumsum 函数沿着指定的维度计算输入张量的累积和\n",
    "            # e.g. Input Tensor: tensor([1, 2, 3, 4]) -> Cumulative Sum Tensor: tensor([ 1,  3,  6, 10])\n",
    "            cumulative_probs = torch.cumsum(sorted_probabilities, dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            # [F, F, F, .., F, T, T, ..., T]\n",
    "            sorted_indices_to_remove[..., 0] = False # 保证至少采样一个\n",
    "            # ... 是一个 Ellipsis 对象，表示选择所有前面的维度。这在多维数组中用于选择所有前面的维度而不显式写出每个维度。\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            # scatter_ 是一个就地操作，用于根据索引将值更新到张量中。\n",
    "            # 第一个参数 dim=-1 指定了操作的维度，-1 表示最后一个维度（通常是词汇表的维度）。\n",
    "            # 第二个参数 indices_to_remove[None, :] 是一个索引张量，指定了要更新的索引位置。\n",
    "            #           None 用于增加一个新的维度，以匹配 next_token_logits 的维度。\n",
    "            # 第三个参数 float('-inf') 是要更新的值，这里使用负无穷大来表示这些位置的 logits 应该被设置为一个非常小的值，\n",
    "            #           这样在后续的 softmax 操作中，这些位置的概率接近于零。\n",
    "            next_token_logits.scatter_(-1, indices_to_remove[None, :], float('-inf'))\n",
    "            prob = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(prob, num_samples=1)\n",
    "            # torch.gather() 函数从输入张量中按照指定的索引提取元素，并将这些元素放置在输出张量的相应位置\n",
    "            if next_token == tokenizer.eos_token_id:\n",
    "                break\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "    generated_text = tokenizer.decode(input_ids[0])\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Generated Text/sample from all vocab:\n",
      "The quick brown foxes in hipster regalia bought carpet and interior paintings down the street to show off a saxophone necktie, white cap, velvet lapels and a female fleece skirt. Gonewood Street got high-PPP (resistive effective resin spray), a 25-gallon load of generic powder, Lauocha Powder, Vinegar Dioxide, weed salts and paprika with propane. The Rx Ecuador kit wasn't as powerful, but the plasma installer matched levels being used in local\n",
      "\n",
      "\n",
      "0.95 Generated Text:\n",
      "The quick brown fox happily stirred his ear in case his other two brother's feathers suddenly disappeared... By the time they heard his voice knocking, Ruby felt her heartbeat slowing as he struggled to try and get up. Her mind was going crazy. Running away would mean the worst, and he'd be no good living for him.\n",
      "\n",
      "Weiss decided she was going to have to get up and find shelter first. Ruby tried to find something, but she couldn't find a place. Her mind began to trick itself into thinking\n",
      "\n",
      "\n",
      "0 Generated Text/greedy:\n",
      "The quick brown foxes are a great way to get a little bit of a kick out of your dog.\n",
      "\n",
      "The quick brown foxes are a great way to get a little bit of a kick out of your dog. The quick brown foxes are a great way to get a little bit of a kick out of your dog. The quick brown foxes are a great way to get a little bit of a kick out of your dog. The quick brown foxes are a great way to get a little bit\n"
     ]
    }
   ],
   "source": [
    "# 1. 加载 GPT-2 模型和分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# 2. 准备输入数据\n",
    "input_text = \"The quick brown fox\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# 4. 调用 top_K_sampling 函数\n",
    "generated_text = top_p_sampling(model, tokenizer, input_ids, max_tokens=100, top_p=1, temperature=1)\n",
    "\n",
    "# 5. 打印生成的文本\n",
    "print(\"1 Generated Text/sample from all vocab:\")\n",
    "print(generated_text)\n",
    "\n",
    "# 4. 调用 top_K_sampling 函数\n",
    "generated_text = top_p_sampling(model, tokenizer, input_ids, max_tokens=100, top_p=0.95, temperature=1)\n",
    "\n",
    "# 5. 打印生成的文本\n",
    "print(\"\\n\\n0.95 Generated Text:\")\n",
    "print(generated_text)\n",
    "\n",
    "# 4. 调用 top_K_sampling 函数\n",
    "generated_text = top_p_sampling(model, tokenizer, input_ids, max_tokens=100, top_p=0, temperature=1)\n",
    "\n",
    "# 5. 打印生成的文本\n",
    "print(\"\\n\\n0 Generated Text/greedy:\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
