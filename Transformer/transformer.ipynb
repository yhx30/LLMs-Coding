{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import warnings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义功能函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES = True\n",
    "\n",
    "def is_interactive_notebook():\n",
    "    return __name__ == \"__main__\"\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "    def step(self):\n",
    "        None\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义深拷贝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    '''\n",
    "    Produce N identical layers.\n",
    "        copy.deepcopy() 是 Python 的 copy 模块中的一个函数，它用于创建一个对象的深拷贝。深拷贝意味着：\n",
    "            这个新拷贝的对象与原对象完全独立，它是原对象的一个递归副本，所有内部对象也会被拷贝。\n",
    "            修改新对象不会影响原对象，反之亦然。\n",
    "        nn.ModuleList 是 PyTorch 提供的一个特殊的列表，用于存放多个 nn.Module 实例。\n",
    "            与 Python 的普通 list 不同, ModuleList 被设计为能与 PyTorch 的其他模块(如优化器、损失函数等)很好地配合使用。\n",
    "            它的作用类似于一个容器，可以将多个子模块组织在一起并注册到 nn.Module 系统中。\n",
    "    '''\n",
    "    \n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder 架构\n",
    "\n",
    "<img src=\"./images/transformer.png\" alt=\"示例图片\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    '''\n",
    "    A standard Encoder-Decoder architecture. Base for this and many other models.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__() # 其作用是让当前类(EncoderDecoder)调用其父类的构造函数(__init__)，从而确保父类中的初始化逻辑得以执行。\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        '''\n",
    "        Take in and process masked src and target sequences.\n",
    "        '''\n",
    "        \n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    class Encoder(self, layer, N):\n",
    "        forward(self, x, mask):\n",
    "        \n",
    "    class EncoderLayer(self, size, self_attn, feed_forward, dropout):\n",
    "        forward(self, x, mask):\n",
    "    '''\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    class Decoder(self, layer, N):\n",
    "        forward(self, x, memory, src_mask, tgt_mask):\n",
    "    \n",
    "    class DecoderLayer(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        forward(self, x, memory, src_mask, tgt_mask):\n",
    "    '''\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成器 Generator\n",
    "\n",
    "<img src=\"./images/Generator.png\" alt=\"示例图片\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Define standard linear + softmax generator step\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim = -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNormalization\n",
    "\n",
    "$LN(x_i) = \\alpha * \\frac{x_i  - \\mu_L}{\\sqrt{\\sigma_L^2 + \\epsilon}} + \\beta , \\quad\\quad\\quad BN(x_i) = \\alpha * \\frac{x_i  - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\beta$\n",
    "\n",
    "\n",
    "<img src=\"./images/LayerNorm.png\" alt=\"示例图片\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # nn.Parameter()：这是 PyTorch 中的一种特殊张量，它被自动注册为模型的可学习参数。使用这种张量，PyTorch 的优化器会自动更新它的值。\n",
    "        # torch.ones(features)：生成一个大小为 features 的全为 1 的张量。a_2 作为缩放系数，将用于对归一化后的输出进行缩放。\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        # b_2 作为偏置项，将用于对归一化后的输出进行平移。\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x：这是输入张量。通常，它的形状为 [batch_size, ..., features]，其中 features 是最后一个维度的大小\n",
    "        # keepdim=True：确保均值计算后保留原始张量的维度，使得输出的形状与输入一致。这样在后续计算中，张量的广播机制可以正常工作。\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        # 这一行计算输入 x 的标准差（std），用法与计算均值类似。\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义子层框架 norm -> (any) sublayer -> dropout -> residual\n",
    "\n",
    "<img src=\"./images/Sublayer.png\" alt=\"示例图片\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    '''\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last\n",
    "    Each layer has two sub-layers. The first is a multi-head self-attention mechanism, \n",
    "    and the second is a simple, position-wise fully connected feed-forward network.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        # nn.Dropout(dropout) 是 PyTorch 中的 Dropout 层，用于在训练过程中对神经网络的部分神经元进行随机失活（dropout）\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        '''\n",
    "        Note for code simplicity the norm is first as opposed to last\n",
    "        注意，为了代码的简单性，规范是第一位的，而不是最后一位\n",
    "        norm -> (any) sublayer -> dropout -> residual\n",
    "        '''\n",
    "        return x + self.dropout(sublayer(self.norm))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "<img src=\"./images/attention.png\" alt=\"示例图片\" width=\"700\">\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = softmax(\\frac{QK^\\top}{\\sqrt{d_k}})V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    '''\n",
    "    Compute 'Scaled Dot Product Attention'.\n",
    "    The input consists of queries and keys of dimension d_k, and values of dimension d_v.\n",
    "    K = W_k * x\n",
    "    x: 这是输入张量。通常, 它的形状为 [batch_size, ..., features], 其中 features 是最后一个维度的大小\n",
    "    所以Q, K也是[batch_size, ... , features]\n",
    "    '''\n",
    "    \n",
    "    # 这通常是特征的维度，代表键和值的维度（d_k）。这个值用于缩放得分，以防止注意力得分过大。\n",
    "    d_k = query.size(-1)\n",
    "    # torch.matmul() 计算点积得分。它将两个相同维度的向量进行运算，产生一个标量（单一数字）.\n",
    "    # key.transpose(-2, -1) 将 key 的最后两个维度调换，使得它的形状变为 [batch_size, ..., d_k]，与 query 的形状匹配。\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        # 使用 masked_fill 函数将 scores 中对应于 mask 中为 0 的位置填充为 -1e9，以确保在 softmax 计算时这些位置的注意力分数变为 0。这样做的目的是防止模型关注这些被屏蔽的输入.\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    # 计算注意力权重（即对每个查询的注意力分配）。dim=-1 表示在最后一个维度上进行 softmax 操作。\n",
    "    alpha_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        alpha_attn = dropout(alpha_attn)\n",
    "    return torch.matmul(alpha_attn, value), alpha_attn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention\n",
    "\n",
    "<img src=\"./images/MultiHeadAttention.png\" alt=\"示例图片\" width=\"700\">\n",
    "\n",
    "$MultiHead(Q, K, V) = Concat(head_1, head_2, ... , head_h)W^O\\text{, where }head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$\n",
    "\n",
    "$W_i^Q \\in \\mathbb{R}^{d_{model} \\times d_k}, \\quad W_i^K \\in \\mathbb{R}^{d_{model} \\times d_K}, \\quad W_i^V \\in \\mathbb{R}^{d_{model} \\times d_V}, \\quad W_i^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # 如果 d_model 为 512，则输入向量的每个元素都由 512 维的特征组成。\n",
    "        # 在多头自注意力机制中，d_model 会被划分成多个头（如 h 个头），每个头处理(d_model / h)维的输入。因此, d_model 需要能够被头数整除，以便均匀划分。\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        # //: 这是整除运算符，确保结果为整数。也就是说，d_model 会被头数 h 整除，计算出每个头的特征维度。\n",
    "        self.d_k = d_model // h \n",
    "        self.h = h\n",
    "        # linears: W_i^Q, W_i^K, W_i^V, W_i^O\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4) \n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        '''\n",
    "        Implements Figure\n",
    "        '''\n",
    "        if mask is not None:\n",
    "            # Some mask applied to all h heads\n",
    "            # unsqueeze 是一个非常有用的张量操作，它用于在指定位置添加一个新的维度。\n",
    "            # mask.unsqueeze(1)：如果提供了 mask，则通过 unsqueeze 在第 1 维度添加一个维度，目的是为多个头共享相同的 mask。\n",
    "            mask = mask.unsqueeze(1)\n",
    "        # K = W_k * x\n",
    "        # x: 这是输入张量。通常, 它的形状为 [batch_size, ..., features], 其中 features 是最后一个维度的大小\n",
    "        # 所以Q, K也是[batch_size, ... , features]\n",
    "        # 获取批量大小（nbatches），通常是 query 的第一个维度，表示当前 batch 中有多少个样本。\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        '''\n",
    "        这段代码的作用是通过线性变换，将 query、key 和 value 映射到多头注意力机制的特征空间。\n",
    "        view(nbatches, -1, self.h, self.d_k):\n",
    "            .view(nbatches, -1, self.h, self.d_k)：将线性变换的结果重塑为 (nbatches, seq_len, num_heads, head_dim) 的形状，目的是分配给每个头。\n",
    "            view 函数用于改变张量的形状。这里的目的是将每个线性层的输出重塑为形状为 [nbatches, -1, self.h, self.d_k] 的张量。\n",
    "            nbatches 是批次大小，表示每个批次中的样本数。\n",
    "            -1 是自动推导的维度，它会根据原始张量的总元素数量和指定的其他维度自动计算,即 seq_len。\n",
    "            self.h 是注意力头的数量。\n",
    "            self.d_k 是每个头的特征维度。\n",
    "        .transpose(1, 2)：交换维度，方便后续对多个注意力头并行处理。 -> (nbatches, num_heads, seq_len, head_dim) \n",
    "        zip 函数将 self.linears 和 (query, key, value) 这三个张量打包在一起，使得在循环中可以同时迭代线性层和输入张量。\n",
    "            三个线性层, 分别以query, key, value作为参数跑一遍\n",
    "        '''\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        '''\n",
    "        Our pre-defined function attention:\n",
    "            attention(query, key, value, mask=None, dropout=None):\n",
    "            return torch.matmul(alpha_attn, value), alpha_attn\n",
    "        '''\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear\n",
    "        '''\n",
    "        x.transpose(1, 2)：将维度换回来，将注意力头的输出重新排列，使得每个样本的所有头的输出排列在一起。\n",
    "        .contiguous()：保证张量在内存中是连续的，以便后续的 .view() 操作。 (nbatches, num_heads, seq_len, head_dim)  -> (nbatches, seq_len, num_heads, head_dim)\n",
    "        .view(nbatches, -1, self.h * self.d_k)：将张量重塑为形状 (nbatches, seq_len, d_model)，将所有头的输出拼接到一起。\n",
    "        '''\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguoous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "        del 是一个用于删除对象的关键字。它可以删除变量、列表中的元素, 甚至删除对象的属性。\n",
    "        del 并不会直接释放内存，而是通过解除对象的引用，使得 Python 的垃圾回收机制可以在适当的时候回收那些不再被引用的对象。\n",
    "        '''\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        \n",
    "        return self.linears[-1](x) # W_i^O\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Attention in our Model\n",
    "\n",
    "1. 在“Encoder-Decoder Attention” Layer 中，查询来自于前一个 Decoder Layer，而内存的 Key 和 Value 则来自 Encoder 的输出。这使得 Decoder 中的每个位置都能够关注输入序列中的所有位置。这模仿了 Seq2Seq 模型中典型的 Encoder-Decoder 注意力机制。\n",
    "\n",
    "2. Encoder 包含 Self-Attention Layer 。在 Self-Attention Layer 中，所有的 Key、Value 和 Query 都来自同一个地方，在这种情况下，是 Encoder 中前一层的输出。Decoder 中的每个位置都可以关注编码器前一层中的所有位置。\n",
    "\n",
    "3. 同样，Decoder 中的 Self-Attention Layer 允许 Decoder 中的每个位置关注到该位置及之前的所有位置。我们需要防止 Decoder 中的左向信息流动，以保持 auto-regressive 特性。我们通过在缩放的点积注意力 (scaled dot-product attention) 中对输入 softmax 的所有非法连接对应的值进行 mask (设置为 -∞) 来实现这一点。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Networks\n",
    "\n",
    "$\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad FFN(x) = ReLU(xW_1 + b_1)W2 + b_2 = max(0, xW_1 + b_1)W_2 + b_2$\n",
    "\n",
    "The dimensionality of input and output is $d_{model} = 512$, and the inner-layer has dimensionality $d_{ff}$\n",
    "\n",
    "<img src=\"./images/FFN.png\" alt=\"示例图片\" width=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    '''\n",
    "    Implement FFN equation.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # return self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "        return self.w_2(self.dropout(max(0, self.w_1(x))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and Softmax\n",
    "\n",
    "<img src=\"./images/Embedding.png\" alt=\"示例图片\" width=\"200\">\n",
    "\n",
    "<img src=\"./images/Embedding2.png\" alt=\"示例图片\" width=\"172\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    '''\n",
    "    与其他序列转换模型类似, 我们使用学习的 embeddings 将输入 tokens 和输出 tokens 转换为维度向量。我们还使用常用的学习线性变换和 softmax 函数将 Decoder 的输出转换为预测的下一个令牌概率。\n",
    "    在我们的模型中, 我们在两个 Embedding Layers 和 pre-softmax 线性变换之间共享相同的权重矩阵\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        '''\n",
    "        lut 通常是 Look-Up Table(查找表）)的缩写, 用于将离散的词索引映射到连续的高维向量空间。\n",
    "        在机器学习和深度学习中, 查找表是用于存储和检索数据的一种数据结构。它通过一个键(key)来获取对应的值(value)。\n",
    "        对于词嵌入层，词索引(即每个词在词汇表中的位置)充当键，而嵌入向量则是与这些索引关联的值。\n",
    "        \n",
    "        d_model: 嵌入向量的维度。\n",
    "        vocab: 词汇表的大小，即可以表示的不同词的数量。\n",
    "        '''\n",
    "        self.lut = nn.Embedding(d_model, vocab)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 在 Embedding Layers 中，我们将这些权重乘以 根号(d_model)\n",
    "        # 这一步是为了对嵌入进行缩放，以增加数值稳定性。通常，乘以 sqrt(d_model) 有助于在后续的注意力机制中保持均匀的梯度分布，尤其是在使用 softmax 时，避免由于输入值过小而导致的梯度消失问题。\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "<img src=\"./images/Position.png\" alt=\"示例图片\" width=\"700\">\n",
    "\n",
    "<img src=\"./images/Position2.png\" alt=\"示例图片\" width=\"300\">\n",
    "\n",
    "$\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad PE_{(pos, 2i)} = \\sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$\n",
    "\n",
    "$\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad PE_{(pos, 2i+1)} = \\cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$\n",
    "\n",
    "$\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad$ 其中 2i 和 2i+1 是特征的维度第几维，分为第**奇**数维和第**偶**数维\n",
    "\n",
    "<img src=\"./images/Position3.png\" alt=\"示例图片\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    '''\n",
    "    Implement the PE function.\n",
    "    d_model: 输入序列中每个词嵌入的维度（与词向量的维度一致）\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # compute the positional encoding once in log space.\n",
    "        # 初始化一个大小为 [max_len, d_model] 的全零张量 pe，用于存储位置编码的值。\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # torch.arange 是 PyTorch 中用于生成等间隔数值序列的一种函数。其功能类似于 Python 中的 range 函数，但它生成的是张量（tensor）而不是普通的数字序列。\n",
    "        # 生成一个 position 张量，它的值是从 0 到 max_len-1 的序列（即每个位置的索引），形状为 [max_len, 1]。unsqueeze(1) 表示在第一维度增加一个维度。\n",
    "        position = torch.arange(0, max_len).unsqueeze(1) # [max_len, 1]\n",
    "        '''\n",
    "        计算位置编码中的分母项 div_term, 使用上述公式, 保存为 div_term = 1/分母。\n",
    "        torch.arange(0, d_model, 2) 生成从 0 开始到 d_model-1 步长为 2 的序列，这部分用来处理偶数位置的维度。\n",
    "        \n",
    "        为什么是公式: torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model) ?\n",
    "\n",
    "        '''\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # (0, 2, 4, ...)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # (1, 3, 5, ...)\n",
    "        \n",
    "        # 扩展维度并注册为 buffer\n",
    "        # 使用 unsqueeze(0) 为位置编码增加一个批次维度, 这样可以适配不同批次的数据。\n",
    "        pe = pe.unsqueeze(0) # [max_len, d_model] -> [1, max_len, d_model]\n",
    "        \n",
    "        '''\n",
    "        将 pe 作为 buffer 注册到模型中。Buffer 是模型的持久状态，但不会作为模型参数参与优化，也不会随着梯度更新而改变。\n",
    "        通常情况下，模型中的状态分为两类：\n",
    "            可训练参数：这些参数会在模型的前向传播和反向传播过程中参与梯度计算，并在优化器中更新。这些参数可以通过 nn.Parameter 来注册，它们通常是权重或偏置。通过优化器或手动调整更新\n",
    "            非可训练的持久状态：这些是模型需要的值，但它们不需要参与优化过程。只能通过代码逻辑手动修改。例如： \n",
    "                用于存储固定的常量，比如用于正则化的参数、批次统计数据（如 BatchNorm 中的均值和方差）。\n",
    "                用于模型中不需要梯度更新但要随模型一起保存的变量。\n",
    "        Buffer 就属于第二类状态。它的关键点是：\n",
    "            不会参与梯度计算，不会随着训练过程中的优化步骤被更新。\n",
    "            会随模型保存和加载，即使这些变量不会更新，仍然希望它们作为模型的一部分存储和恢复。\n",
    "        持久存储: pe 是一个位置编码矩阵，代表了每个位置的编码。这个矩阵是预先计算的，并且在整个模型中保持不变，因此它不需要被优化器更新。\n",
    "                但在保存模型时，我们希望位置编码 pe 和其他可训练的参数一样，被保存下来，并在加载模型时恢复。\n",
    "        不会更新：由于位置编码的矩阵 pe 是基于固定的公式计算的，它不需要参与反向传播和梯度更新。\n",
    "                因此，将其作为 buffer 来注册，这样在训练过程中它不会被优化器错误地修改。\n",
    "        '''\n",
    "         \n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, sequence_length, d_model]\n",
    "        # 这一步是将位置编码加到词嵌入上，使得模型能够感知位置信息。\n",
    "        # requires_grad_(False) 是为了防止位置编码参与梯度更新，因为它们是固定的。\n",
    "        x =  x + self.pe[:, x.size(1)].requires_grad_(False)\n",
    "        \n",
    "        '''\n",
    "        为什么对位置编码应用 Dropout ? \n",
    "            尽管 Dropout 最初是在全连接层中用于“随机失活”一部分神经元的，但它的应用范围实际上更广，既可以用于神经网络的各层、或输出层，也可以用于特征向量，如输入的词嵌入或位置编码。\n",
    "            增加随机性: 通过对位置编码的部分值进行随机“失活”，可以引入一些随机性，使得模型不会过度依赖特定的位置信息。这可以让模型在训练时更具鲁棒性，减少过拟合的可能。\n",
    "            正则化输入: Dropout 可以防止模型对特定的输入位置编码产生过度拟合。因为位置编码是添加到输入中的一部分，\n",
    "                      对其进行 dropout 可以有效地打破模型对绝对位置信息的依赖性，迫使模型更关注整体上下文，而不仅仅是某些特定位置的依赖。\n",
    "        '''\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 展示 Positional Encoding\n",
    "\n",
    "<img src=\"./images/Position_example.png\" alt=\"示例图片\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_positional():\n",
    "    pe = PositionalEncoding(20, 0)\n",
    "    y = pe.forward(torch.zeros(1, 100, 20))\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"embedding\": y[0, :, dim],\n",
    "                    \"dimension\": dim,\n",
    "                    \"position\": list(range(100)),\n",
    "                }\n",
    "            )\n",
    "            for dim in [4, 5, 6, 7]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        alt.Chart(data)\n",
    "        .mark_line()\n",
    "        .properties(width=800)\n",
    "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "# show_example(example_positional())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 框架\n",
    "\n",
    "The encoder is composed of a stack of N = 6 identical layers.\n",
    "\n",
    "<img src=\"./images/Encoder.png\" alt=\"示例图片\" width=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Core encoder is a stack of N layers\n",
    "    layer = class EncoderLayer(self, size, self_attn, feed_forward, dropout):\n",
    "                forward(self, x, mask):\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        '''\n",
    "        Pass the input (and mask) through each layer in turn\n",
    "        '''\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 中的每一层\n",
    "\n",
    "<img src=\"./images/Encoder.png\" alt=\"示例图片\" width=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    '''\n",
    "    Encoder is made up of self-attn and feed forward (defined below).\n",
    "    这个是 class Encoder 中的layer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self. dropout = dropout\n",
    "        '''\n",
    "        class SublayerConnection(self, size, dropout):\n",
    "            forward(self, x, sublayer):\n",
    "                return x + self.dropout(sublayer(self.norm))\n",
    "        '''\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2) # self-attention and feed forward\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        '''\n",
    "        Follow Figure for connections.\n",
    "        \n",
    "        sublayer[0] 代表一个可调用的子层, 并且该子层接受两个参数: x 和通过 Lambda 表达式计算的另一个函数调用的结果。\n",
    "        这一部分是一个 Lambda 表达式，它创建了一个匿名函数，并把这个匿名函数作为参数传递给 sublayer[0]\n",
    "        self.self_attn(x, x, x, mask)：调用当前类的 self_attn 函数, 它是自注意力机制(self-attention)的实现。\n",
    "            第一个 x: 是查询 Q(query)。\n",
    "            第二个 x: 是键 K(key)。\n",
    "            第三个 x: 是值 V(value)。\n",
    "            mask: 通常是用于屏蔽某些不需要关注的部分, 比如在解码器中的解码屏蔽机制, 或用于处理填充的部分。\n",
    "        '''\n",
    "        \n",
    "        x = self.sublayer[0](x, lambda x : self.self_attn(x, x, x, mask)) # 在 self_attn 内部实现了 W_Q, W_K, W_V 的相乘和 Q, K, V 的计算，所以这里只需传 x, x, x.\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 框架\n",
    "\n",
    "The decoder is also composed of a stack of N = 6 identical layers.\n",
    "\n",
    "<img src=\"./images/Decoder.png\" alt=\"示例图片\" width=\"250\">\n",
    "\n",
    "<img src=\"./images/Decoder2.png\" alt=\"示例图片\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Generic N layer decoder with masking\n",
    "    class LayerNorm(self, features, eps=1e-6)\n",
    "        forward(self, x):\n",
    "    layer = class DecoderLayer(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "                forward(self, x, memory, src_mask, tgt_mask):\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = self.layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 中的每一层\n",
    "\n",
    "<img src=\"./images/Decoder.png\" alt=\"示例图片\" width=\"250\">\n",
    "\n",
    "<img src=\"./images/Decoder2.png\" alt=\"示例图片\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Decoder is made of self-attn, src-attn and feed forward (defined below)\n",
    "    除了每个Encoder层中的两个子层之外, 解码器还插入第三子层, 该第三子层对Encoder栈的输出执行多头注意。类似于Encoder, 我们在每个子层周围使用残差连接, 然后进行层归一化。\n",
    "    \n",
    "    这个是 class Decoder 中的layer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        '''\n",
    "        class SublayerConnection(self, size, dropout):\n",
    "            forward(self, x, sublayer):\n",
    "                return x + self.dropout(sublayer(self.norm))\n",
    "        '''\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3) # self-attention, src-attention and feed forward\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        '''\n",
    "        Follow Figure for connections.\n",
    "        \n",
    "        self_attn 是 Decoder 内部的自注意力, 用于当前 Decoder 状态的自我计算。\n",
    "        src_attn 是 Decoder 对 Encoder 输出的注意力, 帮助 Decoder 利用编码器的上下文信息。\n",
    "        src_mask 用于过滤掉 Encoder 的输出 memory 的填充位置。\n",
    "        tgt_mask 用于对目标序列进行掩码，防止模型关注无效的填充部分，以及阻止 Decoder 在训练时看到未来的词。\n",
    "        '''\n",
    "        m = memory\n",
    "        # self_attn 和 src_attn 实现是一样的, 只是传入的参数不同\n",
    "        x = self.sublayer[0](x, lambda x : self.self_attn(x, x, x, tgt_mask)) # 在 self_attn 内部实现了 W_Q, W_K, W_V 的相乘和 Q, K, V 的计算，所以这里只需传 x, x, x.\n",
    "        x = self.sublayer[1](x, lambda x : self.src_attn(x, m, m, src_mask)) # 在 self_attn 内部实现了 W_Q, W_K, W_V 的相乘和 Q, K, V 的计算，所以这里只需传 x, m, m.\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "        \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 展示 Mask\n",
    "\n",
    "<img src=\"./images/mask.png\" alt=\"示例图片\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    '''\n",
    "    Mask out subsequent positions.\n",
    "    这个函数用于生成一个 \"上三角掩码\"，防止解码器在自注意力机制中关注未来的时间步。掩码矩阵的对角线及其以下部分为 True(可以关注), 对角线上方的部分为 False(不能关注)。\n",
    "    '''\n",
    "    \n",
    "    # 1：第一个维度通常是批次大小，这里设为 1 是因为该掩码在所有批次上共享。 size：第二个和第三个维度表示要处理的序列长度。\n",
    "    attn_shape = (1, size, size)\n",
    "    '''\n",
    "    torch.ones(attn_shape)：首先生成一个形状为 (1, size, size) 的全 1 张量。这是一个全 1 的矩阵，表示初始的注意力权重矩阵。\n",
    "    torch.triu(..., diagonal=1): \n",
    "        torch.triu 是一个函数，用于获取上三角矩阵。它会将张量中所有对角线以下的元素置为 0。\n",
    "        参数 diagonal=1 表示生成一个上三角矩阵，但对角线及其以下的部分为 0。对角线上的元素和其下方的元素被遮蔽, 只有对角线右上方的元素是保留的 1。\n",
    "        这样能保证在注意力机制中，每个时间步只能关注自己和之前的时间步，而不能关注未来的时间步\n",
    "    .type(torch.uint8)：将张量的类型转换为 torch.uint8。这个数据类型通常用于表示掩码, 因为掩码通常是布尔值或 0/1 值。\n",
    "    '''\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    \n",
    "    '''\n",
    "    这里返回一个布尔张量，它将上一步中的 subsequent_mask 与 0 进行比较，生成一个布尔掩码。\n",
    "    对于那些 subsequent_mask 中值为 0 的位置，比较操作 subsequent_mask == 0 将返回 True, 这些位置将不会被掩盖。\n",
    "    对于值为 1 的位置，比较操作返回 False, 这些位置会被掩盖。\n",
    "    最终返回的张量是一个大小为 (1, size, size) 的布尔掩码，表示哪些位置可以被注意力关注，哪些位置需要被遮蔽。\n",
    "    '''\n",
    "    return subsequent_mask == 0\n",
    "\n",
    "def example_mask():\n",
    "    LS_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Subsequent Mask\": subsequent_mask(20)[0][x, y].flatten(), # (1, size, size)\n",
    "                    \"Window\": y,\n",
    "                    \"Masking\": x,\n",
    "                }\n",
    "                for y in range(20)\n",
    "                for x in range(20)\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        alt.Chart(LS_data)\n",
    "        .mark_rect()\n",
    "        .properties(height=250, width=250)\n",
    "        .encode(\n",
    "            alt.X(\"Window:O\"),\n",
    "            alt.Y(\"Masking:O\"),\n",
    "            alt.Color(\"Subsequent Mask:Q\", scale=alt.Scale(scheme=\"viridis\")),\n",
    "        )\n",
    "        .interactive()\n",
    "    )\n",
    "    \n",
    "# show_example(example_mask())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Model\n",
    "\n",
    "<img src=\"./images/transformer.png\" alt=\"示例图片\" width=\"600\">\n",
    "\n",
    "### Xavier 初始化\n",
    "\n",
    "它通过均匀分布对权重矩阵进行初始化，权重的取值范围为[−a,a]，假设 in_features 和 out_features 分别是输入和输出的维度。其中：\n",
    "\n",
    "$\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad a = \\sqrt{\\frac{6}{in\\_features \\quad+\\quad out\\_features}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2024, h=8, dropout=0.1\n",
    "):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    '''\n",
    "    class MultiHeadAttention(self, h, d_model, dropout=0.1):\n",
    "        forward(self, query, key, value, mask=None):\n",
    "    '''\n",
    "    attn = MultiHeadAttention(h, d_model)\n",
    "    '''\n",
    "    class PositionwiseFeedForward(self, d_model, d_ff, dropout=0.1):\n",
    "        forward(self, x):\n",
    "    '''\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    '''\n",
    "    class PositionalEncoding(self, d_model, dropout, max_len=5000):\n",
    "        forward(self, x):\n",
    "    '''\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    '''\n",
    "    class EncoderDecoder(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \n",
    "        def encode(self, src, src_mask):\n",
    "            return self.encoder(self.src_embed(src), src_mask)\n",
    "            \n",
    "        def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "            return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "            \n",
    "    class Encoder(self, layer, N):\n",
    "        forward(self, x, mask):\n",
    "    class EncoderLayer(self, size, self_attn, feed_forward, dropout):\n",
    "        forward(self, x, mask):\n",
    "            \n",
    "    class Decoder(self, layer, N):\n",
    "        forward(self, x, memory, src_mask, tgt_mask):\n",
    "    class DecoderLayer(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        forward(self, x, memory, src_mask, tgt_mask):\n",
    "    '''\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N), # self_attn 和 src_attn 实现是一样的, 只是传入的参数不同\n",
    "        # nn.Sequential() 可以将多个 nn.Module 对象按顺序组合为一个新的模块。\n",
    "        # 当输入通过该组合模块时，会按照定义的顺序依次通过每个子模块。\n",
    "        # 这对构建简单的神经网络结构非常有用，特别是那些具有线性层堆叠的模型。\n",
    "        # class Embeddings(self, d_model, vocab):\n",
    "        #     forward(self, x):\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings((d_model, tgt_vocab), c(position))),\n",
    "        # class Generator(self, d_model, vocab):\n",
    "        #     forward(self, x):\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "    \n",
    "    '''\n",
    "    它对模型中所有维度大于1的参数(通常是权重矩阵)使用 Xavier 初始化(或称为 Glorot 初始化)，以保证网络的训练效果更好。\n",
    "    model.parameters()：返回模型中所有可训练参数的迭代器。这些参数通常包括模型中的权重矩阵和偏置向量。\n",
    "    '''\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1: # 检查参数的维度，如果维度大于 1。p.dim()：返回参数张量的维度。\n",
    "            '''\n",
    "            nn.init.xavier_uniform_()：是 PyTorch 中的一种初始化方法，也称为 Xavier 初始化。\n",
    "            详情见 Markdown 公式\n",
    "            Xavier 初始化的作用: Xavier 初始化的目的是让每一层的输入和输出方差保持一致，防止信号在前向传播时快速衰减或爆炸。\n",
    "            这种初始化在神经网络的早期层和深度网络的训练中非常有效，特别是在没有使用 ReLU 激活函数的情况下。\n",
    "            nn.init.xavier_uniform_ 是一个 就地(in-place) 操作，意味着它直接修改传入的参数张量(例如，权重矩阵)。\n",
    "                在这个函数名的末尾有一个下划线(_), 这表示这个函数会修改传入的对象, 而不是返回一个新的对象。\n",
    "            nn.init.xavier_uniform 是一个 函数，用于返回一个 \"新的张量\"，其元素使用 Xavier 均匀分布进行初始化。\n",
    "                这个函数没有下划线，意味着它不会修改传入的参数，而是返回一个新的张量。\n",
    "            '''\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "在这里，我们生成模型的预测。我们尝试使用我们的Transformer来记忆输入。由于模型尚未经过训练，因此输出是随机生成的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test():\n",
    "    \n",
    "    # make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2024, h=8, dropout=0.1)\n",
    "    test_model = make_model(11, 11, 2)\n",
    "    # model.eval() 用于将模型设置为评估模式（evaluation mode）\n",
    "    test_model.eval()\n",
    "    # 创建一个包含输入序列的张量 src，表示源序列的 tokens，维度为 (1, 10)，表示 batch size 为 1，序列长度为 10。\n",
    "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "    # 创建一个全 1 的掩码 src_mask，用于表示源序列中所有位置都可以被注意到。\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "    '''\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    '''\n",
    "    memory = test_model.encode(src, src_mask)\n",
    "    # ys 通常表示模型在推断（inference）阶段生成的输出\n",
    "    # 初始化输出序列 ys，开始时只有一个 token（通常是一个开始符号），形状为 (1, 1)，数据类型与 src 相同。\n",
    "    ys = torch.zeros(1, 1).type_as(src) # [batches, 输出答案长度]\n",
    "    \n",
    "    # range(9) 生成总共 10 个 tokens 的输出序列。\n",
    "    for i in range(9):\n",
    "        # 生成第i个输出序列\n",
    "        '''\n",
    "        def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "            return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "        '''\n",
    "        out = test_model.decode(\n",
    "            # subsequent_mask(size) 生成一个 \"上三角掩码\"\n",
    "            # 输入为 memory（编码器的输出），src_mask（源序列的掩码），ys（当前输出序列）\n",
    "            # 以及通过 subsequent_mask(ys.size(1)).type_as(src.data) 生成的后续掩码。\n",
    "            # 后续掩码用于确保解码器在生成输出时不会访问到未来的 tokens。\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        '''\n",
    "        class Generator(self, d_model, vocab):\n",
    "            forward(self, x):\n",
    "        '''\n",
    "        # 通过模型的生成器（通常是一个线性层加 softmax）将解码器输出的最后一个 token out[:, -1] 转换为概率分布 prob，\n",
    "        # 表示每个可能的下一步 token 的概率。\n",
    "        prob = test_model.generator(out[:, -1])\n",
    "        #  从概率分布中找到概率最高的 token，得到其索引 next_word，这就是模型在当前步骤预测的下一个 token。\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        # .data 返回的是 next_word 张量的内容\n",
    "        next_word = next_word.data[0]\n",
    "        '''\n",
    "        ys = torch.cat([...], dim=1): 将预测的 next_word 添加到 ys 中，扩展输出序列\n",
    "        \n",
    "        torch.empty(1, 1).type_as(src.data).fill(next_word):\n",
    "            创建一个新张量来存放下一个 token, 然后与当前的 ys 进行拼接。\n",
    "            torch.empty(size) 创建一个未初始化的张量，其形状为 size。在这里, size 是 (1, 1)，所以创建的是一个包含单个元素的二维张量。\n",
    "            src.data 是一个张量, src 是在前面的代码段中定义的源输入\n",
    "            .fill(value) 方法用于将张量中的所有元素填充为指定的值。在这里, next_word 是之前预测的下一个词的索引。\n",
    "            所以，调用 fill(next_word) 会将这个未初始化的张量中的唯一元素设置为 next_word 的值。\n",
    "            这整行代码的目的就是创建一个形状为 (1, 1) 的张量，初始化为 next_word 的值，并确保这个张量的数据类型和设备与 src 张量保持一致。\n",
    "            最终的结果是一个包含单个元素(即预测的下一个词的索引)的张量, 用于在后续的步骤中与其他张量进行连接(concatenate)。\n",
    "        '''\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.empty(1, 1).type_as(src.data).fill(next_word)], dim=1\n",
    "        )\n",
    "        \n",
    "        print(\"Example Untrained Model Prediction: \", ys)\n",
    "        \n",
    "def run_tests():\n",
    "    for _ in range(10):\n",
    "        inference_test()\n",
    "        \n",
    "# show_example(run_tests)\n",
    "    \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/inference_test.png\" alt=\"示例图片\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
